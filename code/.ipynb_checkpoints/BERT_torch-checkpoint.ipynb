{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "한국어 데이터 - 채만식 : 탁류"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "with open('/Users/hapkim/stream_story.txt', 'r') as f:\n",
    "    lines = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['한 작가의 생애와 작품을 연구 정리한다는 것은 매우 중요하고도 보람있는 일이다. 그 생애가 고난과 시련에 차고, 그 작품이 시대를 뛰어넘어 더욱 빛을 발하는 작가일수록  보람은 더하다.',\n",
       " '채만식은 일제 식민지시대와 해방공간을 고스란히 살다 간 작가다. 그의 전 생애는 그러니까, 조국의 독립과 근대화라는 두 가지 당면문제가 걸쳐 있는 시기였다. 이 시기를 그는 가난한 지식인으로 의(義)롭게 살다 갔다.',\n",
       " '채만식은 전 생애를 통해 10편에 가까운 장편소설과 100편 이상의 중.단편소설을 발표하였다. 그것들은 모두 탁월한 비판력으로 역사와 현실을 주목한 결과였으며, 특히 [탁류]와 [태평천하]는 리얼리즘문학과 풍자문학의 승리로 기록되고 있다.',\n",
       " '그러나, 이만한 업적에도 불구하고 그는 당대에 문단의 따뜻한 애정을 받지도, 물질적인 풍요로움을 누리지도 못하였다. 그는 항상 현실의 어두운 구석을 찾아 홀로 헤매었고, 건전한 역사 발전을 위해 정신의 칼날을 갈고 있었기 때문이다. 그는 세속적인 명예를 위해 불의와 타협한 적도 없거니와, 개인의 행복을 구하고자 현실에 안주한 적도 없었다. 시대가 바뀔수록 그의 소설이 더욱 소중하게 여겨지는 까닭은 바로 그 때문이다.',\n",
       " '직장이 없고 거처가 없어서 그는 평생동안 떠돌아 다녔지만, 그래도 소설쓰기를 중단한 적은 없었다. 일제의 강압에 못이기어 그는 잠시 치욕적인 삶의 구렁텅이로 휩쓸린 적도 있다. 그러나 그는 곧 [민족의 죄인]과 같은 작품을 써서 회개하고, 뼈아픈 재기의 터전을 마련하였다. 일제 암흑기부터 6.25 직전까지, 그는 자신의 생애 가운데 마지막 10년 간을 전라북도 이리시에서 칩거하는데, 이 기간에 그는 참으로 빛나는 작품들을 남겼다. 에 수록된 단편들이 그 대표작들이다.']"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lines.split('\\n')[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import re\n",
    "from random import *\n",
    "import numpy as npz\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### BERT Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAXLEN = 512 #dimension of the token embeddings.\n",
    "BATCH_SIZE = 4 # used example of each iteration ref: https://blog.lunit.io/2018/08/03/batch-size-in-deep-learning/\n",
    "MAX_PRED = 5 # MAX_PRED = 20 # max tokens of prediction # 즉, 이거 이상 구멍 뚫지마라.\n",
    "N_LAYERS = 12\n",
    "N_HEADS = 8\n",
    "D_MODEL = 768 # (12*8*8) = (N_LAYERS * N_HEAD * N_HEAD) #TODO: check it later.\n",
    "D_FF = 768*4 # 4*d_model, FeedForward dimension\n",
    "D_K = D_V = 64  # dimension of K(=Q), V\n",
    "N_SEGMENTS = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test text format : korean paraphrase data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Option1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "file_list = os.listdir('/Users/hapkim')\n",
    "[i for i in file_list if 'txt' in i]\n",
    "\n",
    "f = open('/Users/hapkim/sae_v1.txt')\n",
    "lines = f.read()\n",
    "\n",
    "import pandas as pd\n",
    "df = pd.DataFrame(lines.split('\\n'))\n",
    "df2 = df[0].str.split('\\t', expand = True )\n",
    "\n",
    "df3 = df2.drop(columns=[2])\n",
    "df3.head()\n",
    "\" \".join(df3.apply(lambda x: \" \".join(x), axis=1))\n",
    "\n",
    "\" \".join(df3.apply(lambda x: \" \".join(x), axis=1))\n",
    "\n",
    "word_list=\" \".join(df3.apply(lambda x: \" \".join(x), axis=1)).split(); word_list[:10]\n",
    "\n",
    "\n",
    "token_list_0 = list()\n",
    "for sentence in df3[0]:\n",
    "    arr = [word_dict[s] for s in str(sentence).split()]\n",
    "    token_list.append(arr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### option2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['how', 'name', 'i', 'to', 'today', 'juliet', 'team', 'am', 'nice', 'great']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_eng = (\n",
    "    'Hello, how are you? I am Romeo.\\n'\n",
    "    'Hello, Romeo My name is Juliet. Nice to meet you.\\n'\n",
    "    'Nice meet you too. How are you today?\\n'\n",
    "    'Great. My baseball team won the competition.\\n'\n",
    "    'Oh Congratulations, Juliet\\n'\n",
    "    'Thanks you Romeo'\n",
    ")\n",
    "\n",
    "sentences = re.sub(\"[.,!?\\\\-]\", '', text_eng.lower()).split('\\n') # filter '.', ',', '?', '!'\n",
    "word_list  = list(set(\" \".join(sentences).split())); word_list[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_dict = {'[PAD]' : 0, '[CLS]' : 1, '[SEP]' : 2, '[MASK]' : 3}\n",
    "for idx, word in enumerate(word_list):\n",
    "    word_dict[word] = idx + 4\n",
    "number_dict = {i: w for i, w in enumerate(word_dict)}\n",
    "VOCAB_SIZE = len(word_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_list = list()\n",
    "for sentence in sentences:\n",
    "    arr = [word_dict[s] for s in sentence.split()] # get the number of the token\n",
    "    token_list.append(arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gelu(x):\n",
    "    \"Implementation of the gelu activation function by Hugging Face\"\n",
    "    return x * 0.5 * (1.0 + torch.erf(x / math.sqrt(2.0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Embedding(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Embedding, self).__init__()\n",
    "        self.tok_embed = nn.Embedding(VOCAB_SIZE, D_MODEL)  # token embedding\n",
    "        self.pos_embed = nn.Embedding(MAXLEN, D_MODEL)  # position embedding\n",
    "        self.seg_embed = nn.Embedding(N_SEGMENTS, D_MODEL)  # segment(token type) embedding\n",
    "        self.norm = nn.LayerNorm(D_MODEL)\n",
    "        self.drop = nn.Dropout(0.1)\n",
    "\n",
    "    def forward(self, x, seg):\n",
    "        seq_len = x.size(1)\n",
    "        pos = torch.arange(seq_len, dtype=torch.long)\n",
    "        pos = pos.unsqueeze(0).expand_as(x)  # (seq_len,) -> (batch_size, seq_len)\n",
    "        embedding = self.tok_embed(x) + self.pos_embed(pos) + self.seg_embed(seg)\n",
    "        return self.drop(self.norm(embedding))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "ex_embedding = Embedding()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ScaledDotProductAttention(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ScaledDotProductAttention, self).__init__()\n",
    "\n",
    "    def forward(self, Q, K, V, attn_mask=None):\n",
    "        scores = torch.matmul(Q, K.transpose(-1, -2)) / np.sqrt(d_k) # scores : [batch_size x n_heads x len_q(=len_k) x len_k(=len_q)]\n",
    "        attn = nn.Softmax(dim=-1)(scores)\n",
    "        context = torch.matmul(attn, V)\n",
    "        return context, attn\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        self.W_Q = nn.Linear(d_model, d_k * n_heads)\n",
    "        self.W_K = nn.Linear(d_model, d_k * n_heads)\n",
    "        self.W_V = nn.Linear(d_model, d_v * n_heads)\n",
    "\n",
    "    def forward(self, Q, K, V, attn_mask=None):\n",
    "        # q: [batch_size x len_q x d_model], k: [batch_size x len_k x d_model], v: [batch_size x len_k x d_model]\n",
    "        residual, batch_size = Q, Q.size(0)\n",
    "        # (B, S, D) -proj-> (B, S, D) -split-> (B, S, H, W) -trans-> (B, H, S, W)\n",
    "        q_s = self.W_Q(Q).view(batch_size, -1, n_heads, d_k).transpose(1,2)  # q_s: [batch_size x n_heads x len_q x d_k]\n",
    "        k_s = self.W_K(K).view(batch_size, -1, n_heads, d_k).transpose(1,2)  # k_s: [batch_size x n_heads x len_k x d_k]\n",
    "        v_s = self.W_V(V).view(batch_size, -1, n_heads, d_v).transpose(1,2)  # v_s: [batch_size x n_heads x len_k x d_v]\n",
    "\n",
    "        if attn_mask is not None: # attn_mask : [batch_size x len_q x len_k]\n",
    "            attn_mask = attn_mask.unsqueeze(1).repeat(1, n_heads, 1, 1) # attn_mask : [batch_size x n_heads x len_q x len_k]\n",
    "        # context: [batch_size x n_heads x len_q x d_v], attn: [batch_size x n_heads x len_q(=len_k) x len_k(=len_q)]\n",
    "        context, attn = ScaledDotProductAttention()(q_s, k_s, v_s, attn_mask=attn_mask)\n",
    "        context = context.transpose(1, 2).contiguous().view(batch_size, -1, n_heads * d_v) # context: [batch_size x len_q x n_heads * d_v]\n",
    "        output = nn.Linear(n_heads * d_v, d_model)(context)\n",
    "        return nn.LayerNorm(d_model)(output + residual), attn # output: [batch_size x len_q x d_model]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PoswiseFeedForwardNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(PoswiseFeedForwardNet, self).__init__()\n",
    "        self.fc1 = nn.Linear(d_model, d_ff)\n",
    "        self.fc2 = nn.Linear(d_ff, d_model)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # (batch_size, len_seq, d_model) -> (batch_size, len_seq, d_ff) -> (batch_size, len_seq, d_model)\n",
    "        return self.fc2(gelu(self.fc1(x)))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BERT(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(BERT, self).__init__()\n",
    "        self.embedding = Embedding()\n",
    "        self.layers = nn.ModuleList([EncoderLayer() for _ in range(n_layers)])\n",
    "        self.fc = nn.Linear(d_model, d_model)\n",
    "        self.activ1 = nn.Tanh()\n",
    "        self.linear = nn.Linear(d_model, d_model)\n",
    "        self.activ2 = gelu\n",
    "        self.norm = nn.LayerNorm(d_model)\n",
    "        self.classifier = nn.Linear(d_model, 2)\n",
    "        # decoder is shared with embedding layer\n",
    "        embed_weight = self.embedding.tok_embed.weight\n",
    "        n_vocab, n_dim = embed_weight.size()\n",
    "        self.decoder = nn.Linear(n_dim, n_vocab, bias=False)\n",
    "        self.decoder.weight = embed_weight\n",
    "        self.decoder_bias = nn.Parameter(torch.zeros(n_vocab))\n",
    "\n",
    "    def forward(self, input_ids, segment_ids, masked_pos):\n",
    "        output = self.embedding(input_ids, segment_ids)\n",
    "        for layer in self.layers:\n",
    "            output, enc_self_attn = layer(output)\n",
    "        # output : [batch_size, len, d_model], attn : [batch_size, n_heads, d_mode, d_model]\n",
    "        h_pooled = self.activ1(self.fc(output[:, 0])) # [batch_size, d_model]\n",
    "        logits_clsf = self.classifier(h_pooled) # [batch_size, 2]\n",
    "\n",
    "        masked_pos = masked_pos[:, :, None].expand(-1, -1, output.size(-1)) # [batch_size, maxlen, d_model]\n",
    "        h_masked = torch.gather(output, 1, masked_pos) # masking position [batch_size, len, d_model]\n",
    "        h_masked = self.norm(self.activ2(self.linear(h_masked)))\n",
    "        logits_lm = self.decoder(h_masked) + self.decoder_bias # [batch_size, maxlen, n_vocab]\n",
    "\n",
    "        return logits_lm, logits_clsf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample IsNext and NotNext to be same in small batch size\n",
    "def make_batch():\n",
    "    batch = []\n",
    "    positive = negative = 0\n",
    "    while positive != BATCH_SIZE/2 or negative != BATCH_SIZE/2:\n",
    "        # random extract one from left, the other from right\n",
    "        tokens_a_index, tokens_b_index= randrange(len(sentences)), randrange(len(sentences))\n",
    "        tokens_a, tokens_b= token_list[tokens_a_index], token_list[tokens_b_index]\n",
    "        # concatenation of two word. CLS - classification problem. SEP - concatenated.\n",
    "        # ex) [1, 5, 22, 6, 9, 23, 27, 15, 25, 4, 17, 2, 15, 4, 17, 28, 24, 7, 17, 16, 2]\n",
    "        input_ids = [word_dict['[CLS]']] + tokens_a + [word_dict['[SEP]']] + tokens_b + [word_dict['[SEP]']]\n",
    "        # first setence : CLS + SEP, second sentence : SEP\n",
    "        # ex) [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
    "        segment_ids = [0] * (1 + len(tokens_a) + 1) + [1] * (len(tokens_b) + 1)\n",
    "\n",
    "        # MASK LM\n",
    "        n_pred =  min(MAX_PRED, max(1, int(round(len(input_ids) * 0.15)))) # 15 % of tokens in one sentence\n",
    "#         cand_maked_pos = [i for i, token in enumerate(input_ids)]\n",
    "        cand_maked_pos = [i for i in range(len(input_ids))]\n",
    "        shuffle(cand_maked_pos)\n",
    "        masked_tokens, masked_pos = [], []\n",
    "        for pos in cand_maked_pos[:n_pred]:\n",
    "            masked_pos.append(pos)\n",
    "            masked_tokens.append(input_ids[pos])\n",
    "            if random() < 0.8:  # 80%\n",
    "                input_ids[pos] = word_dict['[MASK]'] # make mask\n",
    "            if random() < 0.5:  # 10%\n",
    "                index = randint(0, vocab_size - 1) # random index in vocabulary\n",
    "                input_ids[pos] = word_dict[number_dict[index]] # replace\n",
    "\n",
    "        # Zero Paddings\n",
    "        n_pad = MAXLEN - len(input_ids)\n",
    "        input_ids.extend([0] * n_pad)\n",
    "        segment_ids.extend([0] * n_pad)\n",
    "\n",
    "        # Zero Padding (100% - 15%) tokens\n",
    "        if MAX_PRED > n_pred:\n",
    "            n_pad = MAX_PRED - n_pred\n",
    "            masked_tokens.extend([0] * n_pad)\n",
    "            masked_pos.extend([0] * n_pad)\n",
    "\n",
    "        if tokens_a_index + 1 == tokens_b_index and positive < BATCH_SIZE/2:\n",
    "            batch.append([input_ids, segment_ids, masked_tokens, masked_pos, True]) # IsNext\n",
    "            positive += 1\n",
    "        elif tokens_a_index + 1 != tokens_b_index and negative < BATCH_SIZE/2:\n",
    "            batch.append([input_ids, segment_ids, masked_tokens, masked_pos, False]) # NotNext\n",
    "            negative += 1\n",
    "    return batch\n",
    "# Proprecessing Finished"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = make_batch()\n",
    "input_ids, segment_ids, masked_tokens, masked_pos, isNext = zip(*batch)\n",
    "model = BERT()\n",
    "criterion1 = nn.CrossEntropyLoss(reduction='none')\n",
    "criterion2 = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
    "\n",
    "batch = make_batch()\n",
    "input_ids, segment_ids, masked_tokens, masked_pos, isNext = zip(*batch)\n",
    "input_ids = Variable(torch.LongTensor(input_ids))\n",
    "segment_ids = Variable(torch.LongTensor(segment_ids))\n",
    "masked_pos = Variable(torch.LongTensor(masked_pos))\n",
    "masked_tokens = Variable(torch.LongTensor(masked_tokens))\n",
    "isNext = Variable(torch.LongTensor(isNext))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(25):\n",
    "    optimizer.zero_grad()\n",
    "    logits_lm, logits_clsf = model(input_ids, segment_ids, masked_pos)\n",
    "    loss_lm = criterion1(logits_lm.transpose(1, 2), masked_tokens) # for masked LM\n",
    "    loss_lm = (loss_lm.float()).mean()\n",
    "    loss_clsf = criterion2(logits_clsf, isNext) # for sentence classification\n",
    "    loss = loss_lm + loss_clsf\n",
    "    print('Epoch:', '%04d' % (epoch + 1), 'cost =', '{:.6f}'.format(loss))\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "# Predict mask tokens ans isNext\n",
    "input_ids, segment_ids, masked_tokens, masked_pos, isNext = make_batch()[0]\n",
    "print(text)\n",
    "print([number_dict[w] for w in input_ids if number_dict[w] != '[PAD]'])\n",
    "\n",
    "logits_lm, logits_clsf = model(Variable(torch.LongTensor([input_ids])), Variable(torch.LongTensor([segment_ids])), Variable(torch.LongTensor([masked_pos])))\n",
    "logits_lm = logits_lm.data.max(2)[1][0].data.numpy()\n",
    "print('masked tokens list : ',[pos for pos in masked_tokens if pos != 0])\n",
    "print('predict masked tokens list : ',[pos for pos in logits_lm if pos != 0])\n",
    "\n",
    "logits_clsf = logits_clsf.data.max(1)[1].data.numpy()[0]\n",
    "print('isNext : ', True if isNext else False)\n",
    "print('predict isNext : ',True if logits_clsf else False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "reference"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "https://github.com/jadore801120/attention-is-all-you-need-pytorch\n",
    "https://github.com/JayParks/transformer, https://github.com/dhlee347/pytorchic-bert"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
