{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "pooling.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Speech Emotion Recognition with `CNN + Transformer`\n",
        "\n",
        "- The major architecture of this model is motivated by this paper [Self-attention for Speech Emotion Recognition](https://publications.idiap.ch/attachments/papers/2019/Tarantino_INTERSPEECH_2019.pdf)\n",
        "\n",
        "- This is my individual notebook to understand transformer and speech emotion recognition task."
      ],
      "metadata": {
        "id": "7kPMSMvg_d-B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/gdrive')\n",
        "from pathlib import Path\n",
        "drive_path = Path('/gdrive/Shareddrives/Dion-Account/2122WS/4-dl4slp/coding-project/ser/')"
      ],
      "metadata": {
        "id": "4bMB_Fg6l4ub",
        "outputId": "8bf6ceca-05ba-4848-8ddb-43b165cfb7b5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /gdrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from exp.nb_08 import *"
      ],
      "metadata": {
        "id": "dK23Yp19bAT_"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "drive_data_path = drive_path/'data/v1'\n",
        "(drive_data_path).ls()"
      ],
      "metadata": {
        "id": "xw7rlEPZpuNv",
        "outputId": "26bb6105-7720-418f-ca22-fa50a58d8f24",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[PosixPath('/gdrive/Shareddrives/Dion-Account/2122WS/4-dl4slp/coding-project/ser/data/v1/train'),\n",
              " PosixPath('/gdrive/Shareddrives/Dion-Account/2122WS/4-dl4slp/coding-project/ser/data/v1/dev'),\n",
              " PosixPath('/gdrive/Shareddrives/Dion-Account/2122WS/4-dl4slp/coding-project/ser/data/v1/ser.tar-v1.gz')]"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class ItemList(ListContainer):\n",
        "    def __init__(self, items, path='.', tfms=None):\n",
        "        super().__init__(items)\n",
        "        self.path,self.tfms = Path(path),tfms\n",
        "\n",
        "    def __repr__(self): return f'{super().__repr__()}\\nPath: {self.path}'\n",
        "\n",
        "    def new(self, items, cls=None):\n",
        "        if cls is None: cls=self.__class__\n",
        "        return cls(items, self.path, tfms=self.tfms)\n",
        "\n",
        "    def  get(self, i): return i\n",
        "    def _get(self, i): return compose(self.get(i), self.tfms)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        res = super().__getitem__(idx)\n",
        "        if isinstance(res,list): return [self._get(o) for o in res]\n",
        "        return self._get(res)\n"
      ],
      "metadata": {
        "id": "G7UTFPBDeSRS"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class AudioList(ItemList):\n",
        "    @classmethod\n",
        "    def from_files(cls, path, extensions = None, recurse=True, include=None, **kwargs):\n",
        "        return cls(get_files(path, extensions, recurse=recurse, include=include), path, **kwargs)\n",
        "    \n",
        "    def get(self, fn):\n",
        "        return torch.load(fn)\n",
        "\n",
        "class Reshape():\n",
        "    \"transpose to [n_features, n_frames]\"\n",
        "    _order=12\n",
        "    def __call__(self, item):\n",
        "        w, h = item.shape\n",
        "        return item.view(h, w)\n",
        "\n",
        "class DummyChannel():\n",
        "    \"insert pseudo axis in height [n_features, 1, n_frames]\"\n",
        "    _order = 30\n",
        "    def __call__(self, item):\n",
        "        return item.unsqueeze(1)\n",
        "\n",
        "def re_labeler(fn, pat, subcl='act'):\n",
        "    assert subcl in ['act', 'val', 'all']\n",
        "    if subcl=='all': return tuple(int(i) for i in re.findall(pat, str(fn)))\n",
        "    else:\n",
        "        return re.findall(pat, str(fn))[0] if pat == 'act' else re.findall(pat, str(fn))[1]\n"
      ],
      "metadata": {
        "id": "PHB8bfNz-KiH"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "def random_splitter(fn, p_valid): return random.random() < p_valid"
      ],
      "metadata": {
        "id": "s72NONpGdFf0"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CategoryProcessor(Processor):\n",
        "    \"convert string to float, which was retrieved from the file name\"\n",
        "    def __init__(self): self.vocab=None\n",
        "\n",
        "    def __call__(self, items):\n",
        "        #The vocab is defined on the first use.\n",
        "        if self.vocab is None:\n",
        "            # set_trace()\n",
        "            self.vocab = uniqueify(items)\n",
        "            # self.otoi  = {v:k for k,v in enumerate(self.vocab)}\n",
        "        return [torch.tensor(o).float() for o in items]\n",
        "    def proc1(self, item):  return self.otoi[item]\n",
        "\n",
        "    def deprocess(self, idxs):\n",
        "        assert self.vocab is not None\n",
        "        return [self.deproc1(idx) for idx in idxs]\n",
        "    def deproc1(self, idx): return self.vocab[idx]\n"
      ],
      "metadata": {
        "id": "Lo9Q2I_gfl-_"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def _get_files(p, fs, extensions=None):\n",
        "    p = Path(p)\n",
        "    res = [p/f for f in fs if not f.startswith('.')\n",
        "        #    and '_0_0' in f\n",
        "           and ((not extensions) or f'.{f.split(\".\")[-1].lower()}' in extensions)]\n",
        "    return res\n",
        "\n",
        "def get_files(path, extensions=None, recurse=False, include=None):\n",
        "    path = Path(path)\n",
        "    extensions = setify(extensions)\n",
        "    extensions = {e.lower() for e in extensions}\n",
        "    if recurse:\n",
        "        res = []\n",
        "        for i,(p,d,f) in enumerate(os.walk(path)): # returns (dirpath, dirnames, filenames)\n",
        "            if include is not None and i==0: d[:] = [o for o in d if o in include]\n",
        "            else:                            d[:] = [o for o in d if not o.startswith('.')]\n",
        "            res += _get_files(p, f, extensions)\n",
        "        return res\n",
        "    else:\n",
        "        f = [o.name for o in os.scandir(path) if o.is_file()]\n",
        "        return _get_files(path, f, extensions)"
      ],
      "metadata": {
        "id": "Jb_86tKdlyYU"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_dls(train_ds, valid_ds, bs, **kwargs):\n",
        "    return (DataLoader(train_ds, batch_size=bs, shuffle=True, **kwargs),\n",
        "            DataLoader(valid_ds, batch_size=bs, **kwargs))"
      ],
      "metadata": {
        "id": "npt7ZXfs59Yo"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_path = drive_data_path/'train'\n",
        "tfms = [Reshape(), DummyChannel()]\n",
        "al=AudioList.from_files(train_path, tfms=tfms)"
      ],
      "metadata": {
        "id": "V1-WeGWbceda"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "al[0].shape, al.items.__len__()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BzgHtq8lcpwG",
        "outputId": "357aa511-4d49-4d15-a397-216439c5a126"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(torch.Size([26, 1, 614]), 7800)"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def databunchify(sd, bs, c_in=None, c_out=None, **kwargs):\n",
        "    dls = get_dls(sd.train, sd.valid, bs, **kwargs)\n",
        "    return DataBunch(*dls, c_in=c_in, c_out=c_out)\n",
        "\n",
        "SplitData.to_databunch = databunchify"
      ],
      "metadata": {
        "id": "ucopmUtS6Wet"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "label_pat = r'_(\\d+)'\n",
        "emotion_labeler = partial(re_labeler, pat=label_pat, subcl='all')\n",
        "sd = SplitData.split_by_func(al, partial(random_splitter, p_valid=0.0001))\n",
        "ll = label_by_func(sd, emotion_labeler, proc_y=CategoryProcessor())"
      ],
      "metadata": {
        "id": "7t-IrdLscir_"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ll.y"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QbBLDyZkc4Rt",
        "outputId": "ddfd883e-4eaf-409d-ba5a-c8c1c2655536"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ItemList (7800 items)\n",
              "[tensor([1., 1.]), tensor([0., 0.]), tensor([0., 0.]), tensor([1., 0.]), tensor([1., 1.]), tensor([1., 0.]), tensor([1., 1.]), tensor([1., 0.]), tensor([1., 0.]), tensor([1., 1.])...]\n",
              "Path: /gdrive/Shareddrives/Dion-Account/2122WS/4-dl4slp/coding-project/ser/data/v1/train"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "bs=1"
      ],
      "metadata": {
        "id": "5oN7RbWZCeb6"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "c_in = ll.train[0][0].shape[0]\n",
        "c_out = 2\n",
        "data = ll.to_databunch(bs,c_in=c_in,c_out=c_out)"
      ],
      "metadata": {
        "id": "DAO3200FesZw"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data.train_dl.batch_size, data.valid_dl.batch_size"
      ],
      "metadata": {
        "id": "UDN3bG_o5Zh_",
        "outputId": "7d665c1d-c945-4ea0-a84e-47f2989554b0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1, 1)"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data.c_in, data.c_out"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SJ5YJvE6e5KW",
        "outputId": "0283d352-5748-4d63-c822-e967caf23897"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(26, 2)"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "xb, yb = next(iter(data.train_dl))"
      ],
      "metadata": {
        "id": "9DmN6vGxe8_o"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "xb.shape"
      ],
      "metadata": {
        "id": "SCtBT8F5892Y",
        "outputId": "799261e4-249f-4940-8e78-f789930c673f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([1, 26, 1, 184])"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "yb"
      ],
      "metadata": {
        "id": "Y4bcsqEw8-pV",
        "outputId": "9fb7aed5-5ce0-4384-84d0-66eefeca1379",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[1., 1.]])"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model"
      ],
      "metadata": {
        "id": "EsHcnnmQlGo7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title\n",
        "class RunningBatchNorm(nn.Module):\n",
        "    def __init__(self, nf, mom=0.1, eps=1e-5):\n",
        "        super().__init__()\n",
        "        self.mom, self.eps = mom, eps\n",
        "        self.mults = nn.Parameter(torch.ones (nf,1,1))\n",
        "        self.adds  = nn.Parameter(torch.zeros(nf,1,1))\n",
        "        self.register_buffer('sums', torch.zeros(1,nf,1,1))\n",
        "        self.register_buffer('sqrs', torch.zeros(1,nf,1,1))\n",
        "        self.register_buffer('count', tensor(0.))\n",
        "        self.register_buffer('factor', tensor(0.))\n",
        "        self.register_buffer('offset', tensor(0.))\n",
        "        self.batch = 0\n",
        "\n",
        "    def update_stats(self, x):\n",
        "        bs,nc,*_ = x.shape\n",
        "        self.sums.detach_()\n",
        "        self.sqrs.detach_()\n",
        "        dims = (0,2,3)\n",
        "        s    = x    .sum(dims, keepdim=True)\n",
        "        ss   = (x*x).sum(dims, keepdim=True)\n",
        "        c    = s.new_tensor(x.numel()/nc)\n",
        "        mom1 = s.new_tensor(1 - (1-self.mom)/math.sqrt(bs-1))\n",
        "        self.sums .lerp_(s , mom1)\n",
        "        self.sqrs .lerp_(ss, mom1)\n",
        "        self.count.lerp_(c , mom1)\n",
        "        self.batch += bs\n",
        "        means = self.sums/self.count\n",
        "        varns = (self.sqrs/self.count).sub_(means*means)\n",
        "        if bool(self.batch < 20): varns.clamp_min_(0.01)\n",
        "        self.factor = self.mults / (varns+self.eps).sqrt()\n",
        "        self.offset = self.adds - means*self.factor\n",
        "\n",
        "    def forward(self, x):\n",
        "        if self.training: self.update_stats(x)\n",
        "        return x*self.factor + self.offset"
      ],
      "metadata": {
        "id": "WwIGJW5fs54s",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Optimizer():\n",
        "    def __init__(self, params, steppers, **defaults):\n",
        "        self.steppers = listify(steppers)\n",
        "        maybe_update(self.steppers, defaults, get_defaults)\n",
        "        # might be a generator\n",
        "        self.param_groups = list(params)\n",
        "        # ensure params is a list of lists\n",
        "        if not isinstance(self.param_groups[0], list): self.param_groups = [self.param_groups]\n",
        "        self.hypers = [{**defaults} for p in self.param_groups]\n",
        "\n",
        "    def grad_params(self):\n",
        "        return [(p,hyper) for pg,hyper in zip(self.param_groups,self.hypers)\n",
        "            for p in pg if p.grad is not None]\n",
        "\n",
        "    def zero_grad(self):\n",
        "        for p,hyper in self.grad_params():\n",
        "            p.grad.detach_()\n",
        "            p.grad.zero_()\n",
        "\n",
        "    def step(self):\n",
        "        for p,hyper in self.grad_params(): compose(p, self.steppers, **hyper)\n",
        "def maybe_update(os, dest, f):\n",
        "    for o in os:\n",
        "        for k,v in f(o).items():\n",
        "            if k not in dest: dest[k] = v\n",
        "\n",
        "class Stat():\n",
        "    _defaults = {}\n",
        "    def init_state(self, p): raise NotImplementedError\n",
        "    def update(self, p, state, **kwargs): raise NotImplementedError\n",
        "\n",
        "class AverageGrad(Stat):\n",
        "    _defaults = dict(mom=0.9)\n",
        "\n",
        "    def __init__(self, dampening:bool=False): self.dampening=dampening\n",
        "    def init_state(self, p): return {'grad_avg': torch.zeros_like(p.grad.data)}\n",
        "    def update(self, p, state, mom, **kwargs):\n",
        "        state['mom_damp'] = 1-mom if self.dampening else 1.\n",
        "        state['grad_avg'].mul_(mom).add_(state['mom_damp'], p.grad.data)\n",
        "        return state\n",
        "\n",
        "class AverageSqrGrad(Stat):\n",
        "    _defaults = dict(sqr_mom=0.99)\n",
        "\n",
        "    def __init__(self, dampening:bool=True): self.dampening=dampening\n",
        "    def init_state(self, p): return {'sqr_avg': torch.zeros_like(p.grad.data)}\n",
        "    def update(self, p, state, sqr_mom, **kwargs):\n",
        "        state['sqr_damp'] = 1-sqr_mom if self.dampening else 1.\n",
        "        state['sqr_avg'].mul_(sqr_mom).addcmul_(state['sqr_damp'], p.grad.data, p.grad.data)\n",
        "        return state\n",
        "\n",
        "class StepCount(Stat):\n",
        "    def init_state(self, p): return {'step': 0}\n",
        "    def update(self, p, state, **kwargs):\n",
        "        state['step'] += 1\n",
        "        return state\n",
        "\n",
        "def debias(mom, damp, step): return damp * (1 - mom**step) / (1-mom)\n",
        "\n",
        "class StatefulOptimizer(Optimizer):\n",
        "    def __init__(self, params, steppers, stats=None, **defaults):\n",
        "        self.stats = listify(stats)\n",
        "        maybe_update(self.stats, defaults, get_defaults)\n",
        "        super().__init__(params, steppers, **defaults)\n",
        "        self.state = {}\n",
        "\n",
        "    def step(self):\n",
        "        for p,hyper in self.grad_params():\n",
        "            if p not in self.state:\n",
        "                #Create a state for p and call all the statistics to initialize it.\n",
        "                self.state[p] = {}\n",
        "                maybe_update(self.stats, self.state[p], lambda o: o.init_state(p))\n",
        "            state = self.state[p]\n",
        "            for stat in self.stats: state = stat.update(p, state, **hyper)\n",
        "            compose(p, self.steppers, **state, **hyper)\n",
        "            self.state[p] = state\n",
        "\n",
        "def adam_step(p, lr, mom, mom_damp, step, sqr_mom, sqr_damp, grad_avg, sqr_avg, eps, **kwargs):\n",
        "    debias1 = debias(mom,     mom_damp, step)\n",
        "    debias2 = debias(sqr_mom, sqr_damp, step)\n",
        "    p.data.addcdiv_(-lr / debias1, grad_avg, (sqr_avg/debias2).sqrt() + eps)\n",
        "    return p\n",
        "adam_step._defaults = dict(eps=1e-5)\n",
        "\n",
        "def weight_decay(p, lr, wd, **kwargs):\n",
        "    p.data.mul_(1 - lr*wd)\n",
        "    return p\n",
        "weight_decay._defaults = dict(wd=0.)\n",
        "\n",
        "def adam_opt(xtra_step=None, **kwargs):\n",
        "    return partial(StatefulOptimizer, steppers=[adam_step,weight_decay]+listify(xtra_step),\n",
        "                   stats=[AverageGrad(dampening=True), AverageSqrGrad(), StepCount()], **kwargs)\n",
        "    \n",
        "opt_func = adam_opt(mom=0.9, mom_sqr=0.99, eps=1e-6, wd=1e-1, )    "
      ],
      "metadata": {
        "id": "M1yJTG9lji3X"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_defaults(d): return getattr(d,'_defaults',{})"
      ],
      "metadata": {
        "id": "bzPopnue3D0t"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Callback():\n",
        "    _order=0\n",
        "    def set_runner(self, run): self.run=run\n",
        "    def __getattr__(self, k): return getattr(self.run, k)\n",
        "\n",
        "    @property\n",
        "    def name(self):\n",
        "        name = re.sub(r'Callback$', '', self.__class__.__name__)\n",
        "        return camel2snake(name or 'callback')\n",
        "\n",
        "    def __call__(self, cb_name):\n",
        "        f = getattr(self, cb_name, None)\n",
        "        if f and f(): return True\n",
        "        return False\n",
        "\n",
        "class TrainEvalCallback(Callback):\n",
        "    def begin_fit(self):\n",
        "        self.run.n_epochs=0.\n",
        "        self.run.n_iter=0\n",
        "\n",
        "    def after_batch(self):\n",
        "        if not self.in_train: return\n",
        "        self.run.n_epochs += 1./self.iters\n",
        "        self.run.n_iter   += 1\n",
        "\n",
        "    def begin_epoch(self):\n",
        "        self.run.n_epochs=self.epoch\n",
        "        self.model.train()\n",
        "        self.run.in_train=True\n",
        "\n",
        "    def begin_validate(self):\n",
        "        self.model.eval()\n",
        "        self.run.in_train=False\n",
        "\n",
        "class CancelTrainException(Exception): pass\n",
        "class CancelEpochException(Exception): pass\n",
        "class CancelBatchException(Exception): pass\n",
        "\n",
        "class Runner():\n",
        "    def __init__(self, cbs=None, cb_funcs=None):\n",
        "        self.in_train = False\n",
        "        cbs = listify(cbs)\n",
        "        for cbf in listify(cb_funcs):\n",
        "            cb = cbf()\n",
        "            setattr(self, cb.name, cb)\n",
        "            cbs.append(cb)\n",
        "        self.stop,self.cbs = False,[TrainEvalCallback()]+cbs\n",
        "\n",
        "    @property\n",
        "    def opt(self):       return self.learn.opt\n",
        "    @property\n",
        "    def model(self):     return self.learn.model\n",
        "    @property\n",
        "    def loss_func(self): return self.learn.loss_func\n",
        "    @property\n",
        "    def data(self):      return self.learn.data\n",
        "\n",
        "    def one_batch(self, xb, yb):\n",
        "        try:\n",
        "            self.xb,self.yb = xb,yb\n",
        "            self('begin_batch')\n",
        "            print(self.xb.shape)\n",
        "            self.pred = self.model(self.xb)\n",
        "            self('after_pred')\n",
        "            print(self.pred.shape, self.yb.shape)\n",
        "            self.loss = self.loss_func(self.pred, self.yb)\n",
        "            self('after_loss')\n",
        "            if not self.in_train: return\n",
        "            self.loss.backward()\n",
        "            self('after_backward')\n",
        "            self.opt.step()\n",
        "            self('after_step')\n",
        "            self.opt.zero_grad()\n",
        "        except CancelBatchException: self('after_cancel_batch')\n",
        "        finally: self('after_batch')\n",
        "\n",
        "    def all_batches(self, dl):\n",
        "        self.iters = len(dl)\n",
        "        try:\n",
        "            for xb,yb in dl: self.one_batch(xb, yb)\n",
        "        except CancelEpochException: self('after_cancel_epoch')\n",
        "\n",
        "    def fit(self, epochs, learn):\n",
        "        self.epochs,self.learn,self.loss = epochs,learn,tensor(0.)\n",
        "\n",
        "        try:\n",
        "            for cb in self.cbs: cb.set_runner(self)\n",
        "            self('begin_fit')\n",
        "            for epoch in range(epochs):\n",
        "                self.epoch = epoch\n",
        "                if not self('begin_epoch'): self.all_batches(self.data.train_dl)\n",
        "\n",
        "                with torch.no_grad():\n",
        "                    if not self('begin_validate'): self.all_batches(self.data.valid_dl)\n",
        "                self('after_epoch')\n",
        "\n",
        "        except CancelTrainException: self('after_cancel_train')\n",
        "        finally:\n",
        "            self('after_fit')\n",
        "            self.learn = None\n",
        "\n",
        "    def __call__(self, cb_name):\n",
        "        res = False\n",
        "        for cb in sorted(self.cbs, key=lambda x: x._order):\n",
        "            print(cb_name, cb)\n",
        "            res = cb(cb_name) and res\n",
        "        return res\n",
        "\n",
        "class AvgStatsCallback(Callback):\n",
        "    def __init__(self, metrics):\n",
        "        self.train_stats,self.valid_stats = AvgStats(metrics,True),AvgStats(metrics,False)\n",
        "\n",
        "    def begin_epoch(self):\n",
        "        self.train_stats.reset()\n",
        "        self.valid_stats.reset()\n",
        "\n",
        "    def after_loss(self):\n",
        "        stats = self.train_stats if self.in_train else self.valid_stats\n",
        "        with torch.no_grad(): stats.accumulate(self.run)\n",
        "\n",
        "    def after_epoch(self):\n",
        "        print(self.train_stats)\n",
        "        print(self.valid_stats)\n",
        "\n",
        "class Recorder(Callback):\n",
        "    def begin_fit(self): self.lrs,self.losses = [],[]\n",
        "\n",
        "    def after_batch(self):\n",
        "        if not self.in_train: return\n",
        "        self.lrs.append(self.opt.hypers[-1]['lr'])\n",
        "        self.losses.append(self.loss.detach().cpu())\n",
        "\n",
        "    def plot_lr  (self): plt.plot(self.lrs)\n",
        "    def plot_loss(self): plt.plot(self.losses)\n",
        "\n",
        "    def plot(self, skip_last=0):\n",
        "        losses = [o.item() for o in self.losses]\n",
        "        n = len(losses)-skip_last\n",
        "        plt.xscale('log')\n",
        "        plt.plot(self.lrs[:n], losses[:n])\n",
        "\n",
        "class ParamScheduler(Callback):\n",
        "    _order=1\n",
        "    def __init__(self, pname, sched_funcs):\n",
        "        self.pname,self.sched_funcs = pname,listify(sched_funcs)\n",
        "\n",
        "    def begin_batch(self):\n",
        "        if not self.in_train: return\n",
        "        fs = self.sched_funcs\n",
        "        if len(fs)==1: fs = fs*len(self.opt.param_groups)\n",
        "        pos = self.n_epochs/self.epochs\n",
        "        for f,h in zip(fs,self.opt.hypers): h[self.pname] = f(pos)\n",
        "\n",
        "class LR_Find(Callback):\n",
        "    _order=1\n",
        "    def __init__(self, max_iter=100, min_lr=1e-6, max_lr=10):\n",
        "        self.max_iter,self.min_lr,self.max_lr = max_iter,min_lr,max_lr\n",
        "        self.best_loss = 1e9\n",
        "\n",
        "    def begin_batch(self):\n",
        "        if not self.in_train: return\n",
        "        pos = self.n_iter/self.max_iter\n",
        "        lr = self.min_lr * (self.max_lr/self.min_lr) ** pos\n",
        "        for pg in self.opt.hypers: pg['lr'] = lr\n",
        "\n",
        "    def after_step(self):\n",
        "        if self.n_iter>=self.max_iter or self.loss>self.best_loss*10:\n",
        "            raise CancelTrainException()\n",
        "        if self.loss < self.best_loss: self.best_loss = self.loss\n",
        "\n",
        "\n",
        "def get_runner(model, data, lr=0.6, cbs=None, opt_func=None, loss_func = F.cross_entropy):\n",
        "    if opt_func is None: opt_func = optim.SGD\n",
        "    opt = opt_func(model.parameters(), lr=lr)\n",
        "    learn = Learner(model, opt, loss_func, data)\n",
        "    return learn, Runner(cb_funcs=listify(cbs))"
      ],
      "metadata": {
        "id": "Ds8HJcRt4Xv6"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Flatten(nn.Module):\n",
        "    \"remove last (pooled) dimension and reshape tensor tp (seq_len x d_model)\"\n",
        "    def __init__(self): super().__init__()\n",
        "    def forward(self, x):\n",
        "        return x.squeeze(-1).permute(1,0)"
      ],
      "metadata": {
        "id": "OxniMMCvy9qh"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# class StatMean(nn.Module):\n",
        "#     def __init__(self):\n",
        "#         super().__init__()\n",
        "#         self.f = nn.Sigmoid()\n",
        "#     def forward(self, x):\n",
        "#         return self.f(x.mean(0, keepdim=True))"
      ],
      "metadata": {
        "id": "nHhYyG3kBtai"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Here, the Author mentioned that they use '6-block' for raw data. but in our case, as it is log mel, it's not 100% raw data."
      ],
      "metadata": {
        "id": "TvHhUxCmOrl8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# TO Fix the length, cnn 1d"
      ],
      "metadata": {
        "id": "PUY5uRaEmxxG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class CNN1d(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.module = nn.Sequential(\n",
        "            nn.Conv1d(1,8, kernel_size=10, stride=1), GeneralRelu(),\n",
        "            nn.Conv1d(8,16, kernel_size=13), GeneralRelu(),\n",
        "            nn.Conv1d(16,32, kernel_size=10), GeneralRelu(),\n",
        "            nn.Conv1d(32,64, kernel_size=7), GeneralRelu(),\n",
        "            nn.Conv1d(64,128, kernel_size=5), GeneralRelu(),\n",
        "            nn.Conv1d(128,128, kernel_size=3), GeneralRelu(), #26, 128, 28\n",
        "            nn.AdaptiveMaxPool1d(1), #26, 128, 1\n",
        "            Flatten()) #128, 26\n",
        "    def forward(self, x):\n",
        "        return self.module(x)"
      ],
      "metadata": {
        "id": "1EwucPjayOua"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# time_frame = 10000\n",
        "# for xb, _ in iter(data.train_dl):\n",
        "#     crt_time_frame = xb.squeeze(0).shape[-1]\n",
        "#     if time_frame > crt_time_frame:\n",
        "#         print(crt_time_frame)\n",
        "#         time_frame = crt_time_frame\n",
        "#     # print(CNN1d()(xb.squeeze(0)).shape)"
      ],
      "metadata": {
        "id": "QKpKV0EYp6bp"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "xb.shape, CNN1d()(xb.squeeze(0)).shape"
      ],
      "metadata": {
        "id": "Wve9XSpGrvjR",
        "outputId": "721bdf34-a07d-459b-eec1-d482b082ec13",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(torch.Size([1, 26, 1, 199]), torch.Size([128, 26]))"
            ]
          },
          "metadata": {},
          "execution_count": 59
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- 128 : Time frame, which corresponds to `seq_len`\n",
        "- 26 : d_model, which corresponds to `d_model` "
      ],
      "metadata": {
        "id": "lc6Fm508utl_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Let us introduce transformer\n",
        "- embedding is composed of two parts\n",
        "    - positional encoding - equal to original one\n",
        "    - embedding - learn from CNN 1d model"
      ],
      "metadata": {
        "id": "oExrKQXOwKG_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch import Tensor"
      ],
      "metadata": {
        "id": "frEqAixDXr7Q"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -qq ipdb\n",
        "from ipdb import set_trace"
      ],
      "metadata": {
        "id": "2C7E9Ge0b7Pw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class PositionalEncoding(nn.Module):\n",
        "    \"Encode the position with a sinusoid.\"\n",
        "    def __init__(self, d:int):\n",
        "        super().__init__()\n",
        "        self.register_buffer('freq', 1 / (10000 ** (torch.arange(0., d, 2.)/d)))\n",
        "\n",
        "    def forward(self, pos:Tensor):\n",
        "        inp = torch.ger(pos, self.freq)\n",
        "        enc = torch.cat([inp.sin(), inp.cos()], dim=-1)\n",
        "        return enc\n",
        "\n",
        "class TransformerEmbedding(nn.Module):\n",
        "    \"Embedding from CNN + positional encoding + dropout\"\n",
        "    def __init__(self, emb_sz:int, inp_p:float=0.):\n",
        "        super().__init__()\n",
        "        self.emb_sz = emb_sz\n",
        "        # (seq_len x d_model)\n",
        "        self.embed = CNN1d()\n",
        "        self.pos_enc = PositionalEncoding(emb_sz)\n",
        "        self.drop = nn.Dropout(inp_p)\n",
        "\n",
        "    def forward(self, inp):\n",
        "        # Need to insert the batch dimension as you removed it when it is used for conv1d\n",
        "        # 1,       26,       1,       75 -> 128    , 26\n",
        "        # bs, d_model, seq_len, n_frames -> seq_len, d_model\n",
        "        inp = self.embed(inp)\n",
        "        pos = torch.arange(0, inp.size(0), device=inp.device).float()     \n",
        "        # reconstruct pseudo batch dimension (1 x 128 x 26)\n",
        "        return self.drop(inp * math.sqrt(self.emb_sz) + self.pos_enc(pos)).unsqueeze(0)\n"
      ],
      "metadata": {
        "id": "2YUXXcXKwLxe"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dummy_inp_emb = torch.randn(128, 26)\n",
        "dummy_pos_emb = PositionalEncoding(26)\n",
        "dummy_pos = torch.arange(0, dummy_inp.size(0)).float()\n",
        "dummy_pos_emb(dummy_pos)"
      ],
      "metadata": {
        "id": "XIqFVq4Vc5u7",
        "outputId": "c08c0513-058d-4d80-e2e5-28f697d98aaf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 0.0000,  0.0000,  0.0000,  ...,  1.0000,  1.0000,  1.0000],\n",
              "        [ 0.8415,  0.4727,  0.2401,  ...,  1.0000,  1.0000,  1.0000],\n",
              "        [ 0.9093,  0.8331,  0.4661,  ...,  1.0000,  1.0000,  1.0000],\n",
              "        ...,\n",
              "        [-0.6160, -0.9590, -0.8958,  ...,  0.9945,  0.9987,  0.9997],\n",
              "        [ 0.3300, -0.7110, -0.7628,  ...,  0.9944,  0.9986,  0.9997],\n",
              "        [ 0.9726, -0.2941, -0.5853,  ...,  0.9943,  0.9986,  0.9997]])"
            ]
          },
          "metadata": {},
          "execution_count": 66
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- `emb_sz`: 26\n",
        "- Transformer-embedding: 1 x 128 x 26 (bs, seq_len, d_model)\n",
        "\n"
      ],
      "metadata": {
        "id": "gcDh8QESsOAw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def feed_forward(d_model:int, d_ff:int, ff_p:float=0., double_drop:bool=True):\n",
        "    layers = [nn.Linear(d_model, d_ff), nn.ReLU()]\n",
        "    if double_drop: layers.append(nn.Dropout(ff_p))\n",
        "    return SequentialEx(*layers, nn.Linear(d_ff, d_model), nn.Dropout(ff_p), MergeLayer(), nn.LayerNorm(d_model))\n",
        "\n",
        "class MergeLayer(nn.Module):\n",
        "    \"Merge a shortcut with the result of the module by adding them or concatenating them if `dense=True`.\"\n",
        "    def __init__(self, dense:bool=False):\n",
        "        super().__init__()\n",
        "        self.dense=dense\n",
        "    def forward(self, x):\n",
        "        return torch.cat([x,x.orig], dim=1) if self.dense else (x+x.orig)\n",
        "\n",
        "class SequentialEx(nn.Module):\n",
        "    \"Like `nn.Sequential`, but with ModuleList semantics, and can access module input\"\n",
        "    def __init__(self, *layers):\n",
        "        super().__init__()\n",
        "        self.layers = nn.ModuleList(layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        res = x\n",
        "        for l in self.layers:\n",
        "            res.orig = x\n",
        "            nres = l(res)\n",
        "            # We have to remove res.orig to avoid hanging refs and therefore memory leaks\n",
        "            res.orig, nres.orig = None, None\n",
        "            res = nres\n",
        "        return res\n",
        "\n",
        "    def __getitem__(self,i): return self.layers[i]\n",
        "    def append(self,l):      return self.layers.append(l)\n",
        "    def extend(self,l):      return self.layers.extend(l)\n",
        "    def insert(self,i,l):    return self.layers.insert(i,l)\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    \"MutiHeadAttention.\"\n",
        "\n",
        "    def __init__(self, n_heads:int, d_model:int, d_head:int=None, resid_p:float=0., attn_p:float=0., bias:bool=True,\n",
        "                 scale:bool=True):\n",
        "        super().__init__()\n",
        "        # d_head = ifnone(d_head, d_model//n_heads)\n",
        "        self.n_heads,self.d_head,self.scale = n_heads,d_head,scale\n",
        "        self.q_wgt = nn.Linear(d_model, n_heads * d_head, bias=bias)\n",
        "        self.k_wgt = nn.Linear(d_model, n_heads * d_head, bias=bias)\n",
        "        self.v_wgt = nn.Linear(d_model, n_heads * d_head, bias=bias)\n",
        "        self.out = nn.Linear(n_heads * d_head, d_model, bias=bias)\n",
        "        self.drop_att,self.drop_res = nn.Dropout(attn_p),nn.Dropout(resid_p)\n",
        "        self.ln = nn.LayerNorm(d_model)\n",
        "\n",
        "    def forward(self, q:Tensor, k:Tensor, v:Tensor, mask:Tensor=None):\n",
        "        return self.ln(q + self.drop_res(self.out(self._apply_attention(q, k, v, mask=mask))))\n",
        "\n",
        "    def _apply_attention(self, q:Tensor, k:Tensor, v:Tensor, mask:Tensor=None):\n",
        "        bs,seq_len = q.size(0),q.size(1)\n",
        "        wq,wk,wv = self.q_wgt(q),self.k_wgt(k),self.v_wgt(v)\n",
        "        wq,wk,wv = map(lambda x:x.view(bs, x.size(1), self.n_heads, self.d_head), (wq,wk,wv))\n",
        "        wq,wk,wv = wq.permute(0, 2, 1, 3),wk.permute(0, 2, 3, 1),wv.permute(0, 2, 1, 3)\n",
        "        attn_score = torch.matmul(wq, wk)\n",
        "        if self.scale: attn_score = attn_score.div_(self.d_head ** 0.5)\n",
        "        if mask is not None:\n",
        "            attn_score = attn_score.float().masked_fill(mask, -float('inf')).type_as(attn_score)\n",
        "        attn_prob = self.drop_att(F.softmax(attn_score, dim=-1))\n",
        "        attn_vec = torch.matmul(attn_prob, wv)\n",
        "        return attn_vec.permute(0, 2, 1, 3).contiguous().contiguous().view(bs, seq_len, -1)\n",
        "\n",
        "class EncoderBlock(nn.Module):\n",
        "    \"Encoder block of a Transformer model.\"\n",
        "    #Can't use Sequential directly cause more than one input...\n",
        "    def __init__(self, n_heads:int, d_model:int, d_head:int, d_inner:int, resid_p:float=0., attn_p:float=0., ff_p:float=0.,\n",
        "                 bias:bool=True, scale:bool=True, double_drop:bool=True):\n",
        "        super().__init__()\n",
        "        self.mha = MultiHeadAttention(n_heads, d_model, d_head, resid_p=resid_p, attn_p=attn_p, bias=bias, scale=scale)\n",
        "        self.ff  = feed_forward(d_model, d_inner, ff_p=ff_p, double_drop=double_drop)\n",
        "\n",
        "    def forward(self, x:Tensor, mask:Tensor=None): return self.ff(self.mha(x, x, x, mask=mask))"
      ],
      "metadata": {
        "id": "t6DdCZWvb1MB"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CNNTransformer(nn.Module):\n",
        "    \"CNN Transformer model\"\n",
        "    \n",
        "    def __init__(self, out_vsz:int=4, n_layers:int=1, n_heads:int=3, d_model:int=26, d_head:int=9, \n",
        "                 d_inner:int=3 * 9,\n",
        "                 inp_p:float=0.1, resid_p:float=0.1, attn_p:float=0.1,\n",
        "                 ff_p:float=0.1,\n",
        "                 bias:bool=True, \n",
        "                 scale:bool=True, double_drop:bool=True):\n",
        "        super().__init__()\n",
        "        self.enc_emb = TransformerEmbedding(d_model, inp_p)\n",
        "        self.encoder = nn.ModuleList([EncoderBlock(n_heads, d_model, d_head, d_inner, resid_p, attn_p, ff_p, bias, scale, double_drop) for _ in range(0, n_layers)])\n",
        "        # TODO: Tarantino (2019) applied 2d-convolution here.\n",
        "        # self.out = nn.Linear(d_model, out_vsz)\n",
        "        \n",
        "    def forward(self, inp):\n",
        "        # torch.Size([1, 26, 1, 75]) => torch.Size([1, 128, 26])\n",
        "        enc = self.enc_emb(inp.squeeze(0))\n",
        "        # torch.Size([1, 128, 26])\n",
        "        for enc_block in self.encoder: enc = enc_block(enc)\n",
        "        \n",
        "        return enc"
      ],
      "metadata": {
        "id": "tXXWpd88wq4i"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y1= CNNTransformer()(xb).unsqueeze(1); y1.shape"
      ],
      "metadata": {
        "id": "XAB1IMSAeugu",
        "outputId": "42e8d79e-1d81-489d-aff2-8677c2c7f534",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([1, 1, 128, 26])"
            ]
          },
          "metadata": {},
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "conv_out = nn.Conv2d(1, 5, 20)"
      ],
      "metadata": {
        "id": "dVampgJyp7T1"
      },
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "conv_out(y1).shape"
      ],
      "metadata": {
        "id": "KvWtRC3pqPXI",
        "outputId": "68536fe6-95ab-4749-b9ff-fa5f4cffffd0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([1, 5, 109, 7])"
            ]
          },
          "metadata": {},
          "execution_count": 60
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pool_out = nn.AdaptiveMaxPool1d(1)"
      ],
      "metadata": {
        "id": "5SATGmNRr_Qa"
      },
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y2 = pool_out(conv_out(y1).squeeze(0)); y2.shape"
      ],
      "metadata": {
        "id": "oBVCY7Rfp6Zf",
        "outputId": "960a4b00-eeae-4791-a6cb-51483e23cad8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([5, 109, 1])"
            ]
          },
          "metadata": {},
          "execution_count": 62
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y3 = y2.squeeze(-1).unsqueeze(0); y3.shape"
      ],
      "metadata": {
        "id": "y4qXAaz_tKUP",
        "outputId": "3763ee17-5420-4d43-fe74-2817b86991f1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([1, 5, 109])"
            ]
          },
          "metadata": {},
          "execution_count": 64
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "lin_out = nn.Linear(5*109, 2)"
      ],
      "metadata": {
        "id": "QiVdcTtZtpVT"
      },
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y4 = lin_out(y3.view(1, -1)); y4"
      ],
      "metadata": {
        "id": "ZMVb-6YUtKOy",
        "outputId": "430d9178-ae16-471c-c625-b7119236ba84",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0.1562, 0.5040]], grad_fn=<AddmmBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 71
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "loss = FocalLoss()"
      ],
      "metadata": {
        "id": "0oJq47VGvLPd"
      },
      "execution_count": 72,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "yb"
      ],
      "metadata": {
        "id": "JH7PUi0GvQ91",
        "outputId": "60b4fe7c-b4f7-41fb-d266-713a3d10726f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0., 1.]])"
            ]
          },
          "metadata": {},
          "execution_count": 73
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "loss(y4, yb)"
      ],
      "metadata": {
        "id": "X9lwiPbFvM-M",
        "outputId": "affbc1de-d03f-41aa-aa89-97bf92907205",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(0.0915, grad_fn=<MeanBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 74
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class FocalLoss(nn.modules.loss._WeightedLoss):\n",
        "    def __init__(self, weight=None, gamma=2,reduction='mean'):\n",
        "        super(FocalLoss, self).__init__(weight,reduction=reduction)\n",
        "        self.gamma = gamma\n",
        "        self.weight = weight #weight parameter will act as the alpha parameter to balance class weights\n",
        "\n",
        "    def forward(self, input, target):\n",
        "\n",
        "        ce_loss = F.cross_entropy(input, target,reduction=self.reduction,weight=self.weight) \n",
        "        pt = torch.exp(-ce_loss)\n",
        "        focal_loss = ((1 - pt) ** self.gamma * ce_loss).mean()\n",
        "        return focal_loss"
      ],
      "metadata": {
        "id": "030LU6V7tKJB"
      },
      "execution_count": 69,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---"
      ],
      "metadata": {
        "id": "gANUWJmIhiWp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "h9duXvlWtJpC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cbfs = [partial(AvgStatsCallback,accuracy),\n",
        "        CudaCallback]"
      ],
      "metadata": {
        "id": "NMqTlV_2kvsn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def normalize_chan(x, mean, std):\n",
        "    return (x-mean[...,None,None]) / std[...,None,None]\n",
        "\n",
        "_m = tensor([4.6458, 5.4130, 6.5964, 6.8884, 6.8241, 7.1415, 7.2152, 7.1492, 6.6828,\n",
        "        6.7434, 6.8610, 6.9456, 7.2149, 7.3144, 7.3993, 7.1714, 7.3913, 7.5860,\n",
        "        7.3430, 7.3854, 7.4977, 7.4650, 7.3808, 7.0497, 6.7768, 6.4319])\n",
        "_s = tensor([1.6839, 2.3302, 2.6372, 2.6552, 2.7861, 2.7495, 2.7446, 2.6085, 2.5018,\n",
        "        2.3586, 2.3367, 2.3888, 2.4452, 2.4944, 2.4172, 2.4300, 2.3737, 2.4037,\n",
        "        2.4891, 2.4774, 2.4399, 2.3689, 2.2110, 2.2310, 2.2802, 2.2686])\n",
        "norm_ser = partial(normalize_chan, mean=_m.cuda(), std=_s.cuda())\n",
        "\n",
        "class BatchTransformXCallback(Callback):\n",
        "    _order=2\n",
        "    def __init__(self, tfm): self.tfm = tfm\n",
        "    def begin_batch(self):\n",
        "        self.run.xb = self.tfm(self.xb).squeeze(0)\n",
        "        # self.run.yb = self.yb.squeeze(0)"
      ],
      "metadata": {
        "id": "eAce3l3fkefR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cbfs.append(partial(BatchTransformXCallback, norm_ser))"
      ],
      "metadata": {
        "id": "40BHfbmWsfBE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sched = combine_scheds([0.3,0.7], cos_1cycle_anneal(0.1,0.3,0.05))"
      ],
      "metadata": {
        "id": "VDx9rJ1S7evB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_learn_run(model, data, lr, cbs=None, opt_func=None, **kwargs):\n",
        "    init_cnn(model)\n",
        "    return get_runner(model, data, lr=lr, cbs=cbs, opt_func=opt_func)"
      ],
      "metadata": {
        "id": "YRndPhvA10xj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "learn,run = get_learn_run(model, data, 0.2, cbs=cbfs+[\n",
        "    partial(ParamScheduler, 'lr', sched)], opt_func=opt_func, loss_func = nn.BCELoss()\n",
        ")"
      ],
      "metadata": {
        "id": "D_CtaxjgIxIg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "learn.opt"
      ],
      "metadata": {
        "id": "9opjsOubIx-S",
        "outputId": "57f61727-3744-46ff-bc7e-c046f1270900",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<__main__.StatefulOptimizer at 0x7f4cc380c090>"
            ]
          },
          "metadata": {},
          "execution_count": 165
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def model_summary(run, learn, data, find_all=False):\n",
        "    xb,yb = get_batch(data.valid_dl, run)\n",
        "    device = next(learn.model.parameters()).device#Model may not be on the GPU yet\n",
        "    xb,yb = xb.to(device),yb.to(device)\n",
        "    mods = learn.model.children()\n",
        "    f = lambda hook,mod,inp,out: print(f\"{mod}\\n{out.shape}\\n\")\n",
        "    with Hooks(mods, f) as hooks: learn.model(xb)"
      ],
      "metadata": {
        "id": "ZhGMUtaJ3JP1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "run.cbs"
      ],
      "metadata": {
        "id": "Ly5Uu4pSJLDk",
        "outputId": "7072ba33-013d-4efd-9bb7-5499874e9441",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<__main__.TrainEvalCallback at 0x7f4cc4c13f90>,\n",
              " <__main__.AvgStatsCallback at 0x7f4cc4c13450>,\n",
              " <exp.nb_06.CudaCallback at 0x7f4cc7ae7a90>,\n",
              " <__main__.BatchTransformXCallback at 0x7f4cc7ae7210>,\n",
              " <__main__.ParamScheduler at 0x7f4cc4c15c90>]"
            ]
          },
          "metadata": {},
          "execution_count": 167
        }
      ]
    }
  ]
}