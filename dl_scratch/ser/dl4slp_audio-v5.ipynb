{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "dl4slp-audio-v5.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "##  Let me try [Parrelle is all you want](https://github.com/IliaZenkov/transformer-cnn-emotion-recognition/blob/main/notebooks/Parallel_is_All_You_Want.ipynb), but only adaopting its model.\n",
        " 21 Oct 2020"
      ],
      "metadata": {
        "id": "bYMZuk-QNKGl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/fastai/course-v3/ /content/course-v3\n",
        "%cd /content/course-v3/nbs/dl2"
      ],
      "metadata": {
        "id": "-p-B9N_qsun4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- NOTE: if you do not have installed [nvidia/apex](https://github.com/nvidia/apex),  change `from exp.nb_10c import *` to `from exp.nb_10b import *` in `exp.nb_11`"
      ],
      "metadata": {
        "id": "nb6S4CPyqS_F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from exp.nb_12a import *"
      ],
      "metadata": {
        "id": "77V-bw0pqR2J"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Local data (which is divided into separate dataset) is saved to `Path('/gdrive/Shareddrives/Dion-Account/2122WS/8-dl4slp/coding-project/data/v1/ser'`\n"
      ],
      "metadata": {
        "id": "_U7CK3KXqnr5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/gdrive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VL3HjY7zp5jZ",
        "outputId": "511cc67e-a759-4ca1-de7d-5ff9352b1f66"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /gdrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "root_path = Path('/gdrive/Shareddrives/Dion-Account/2122WS/8-dl4slp/coding-project/data/ser/v1'); root_path.ls()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V93zJPFhqnpC",
        "outputId": "d92bc110-f658-4220-86ca-5e2fbad35cdf"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[PosixPath('/gdrive/Shareddrives/Dion-Account/2122WS/8-dl4slp/coding-project/data/ser/v1/train'),\n",
              " PosixPath('/gdrive/Shareddrives/Dion-Account/2122WS/8-dl4slp/coding-project/data/ser/v1/dev')]"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D1FVouZFT7-5"
      },
      "source": [
        "features-entry contains a list of a list with 26 items.\n",
        "- length of inner list: 26 (float numbers - represent one preprocessed speech frame (logMel))\n",
        "\n",
        "- length of outer list: number of frames per data-point, e.g. 10 or 15, ..."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_path = root_path/'train'"
      ],
      "metadata": {
        "id": "XxRfMtm7UTb_"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "audios = get_files(train_path)"
      ],
      "metadata": {
        "id": "vR7U0z1HmgoR"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# restart-run\n",
        "class AudioList(ItemList):\n",
        "    @classmethod\n",
        "    def from_files(cls, path, extensions = None, recurse=True, include=None, **kwargs):\n",
        "        return cls(get_files(path, extensions, recurse=recurse, include=include), path, **kwargs)\n",
        "    \n",
        "    def get(self, fn):\n",
        "        return torch.load(fn)"
      ],
      "metadata": {
        "id": "fPJ2emMUmglw"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "al=AudioList.from_files(train_path); al"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PSfrsOPbmgil",
        "outputId": "d978e5bf-6ced-4738-edd0-6c7b05aa2ba7"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AudioList (7800 items)\n",
              "[PosixPath('/gdrive/Shareddrives/Dion-Account/2122WS/8-dl4slp/coding-project/data/ser/v1/train/7760_1_1.pt'), PosixPath('/gdrive/Shareddrives/Dion-Account/2122WS/8-dl4slp/coding-project/data/ser/v1/train/7761_0_0.pt'), PosixPath('/gdrive/Shareddrives/Dion-Account/2122WS/8-dl4slp/coding-project/data/ser/v1/train/7762_0_0.pt'), PosixPath('/gdrive/Shareddrives/Dion-Account/2122WS/8-dl4slp/coding-project/data/ser/v1/train/7763_1_0.pt'), PosixPath('/gdrive/Shareddrives/Dion-Account/2122WS/8-dl4slp/coding-project/data/ser/v1/train/7764_1_1.pt'), PosixPath('/gdrive/Shareddrives/Dion-Account/2122WS/8-dl4slp/coding-project/data/ser/v1/train/7765_1_0.pt'), PosixPath('/gdrive/Shareddrives/Dion-Account/2122WS/8-dl4slp/coding-project/data/ser/v1/train/7766_1_1.pt'), PosixPath('/gdrive/Shareddrives/Dion-Account/2122WS/8-dl4slp/coding-project/data/ser/v1/train/7767_1_0.pt'), PosixPath('/gdrive/Shareddrives/Dion-Account/2122WS/8-dl4slp/coding-project/data/ser/v1/train/7768_1_0.pt'), PosixPath('/gdrive/Shareddrives/Dion-Account/2122WS/8-dl4slp/coding-project/data/ser/v1/train/7769_1_1.pt')...]\n",
              "Path: /gdrive/Shareddrives/Dion-Account/2122WS/8-dl4slp/coding-project/data/ser/v1/train"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Use adaptive pooling to fit the frames"
      ],
      "metadata": {
        "id": "4lnGePmPwoUL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## input transform\n",
        "\n",
        "1. load tensor\n",
        "2. tocuda\n",
        "3. transpose, add dimension to first axis\n",
        "4. AdaptiveAvgPooling to fit width(time frames) 250\n"
      ],
      "metadata": {
        "id": "-VsPUxouyOcu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ToCuda():\n",
        "    _order=20\n",
        "    def __call__(self, ad):\n",
        "        return ad.cuda()"
      ],
      "metadata": {
        "id": "w-bI6BPNzmde"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Reshape():\n",
        "    _order=30\n",
        "    def __call__(self, item):\n",
        "        w, h = item.shape\n",
        "        return item.view(h, w).unsqueeze(0)"
      ],
      "metadata": {
        "id": "4PjKjkXtzoQQ"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class FrameAvgPooling():\n",
        "    _order=40\n",
        "    def __init__(self, n_tf):\n",
        "        # n_tf: numner of time frames\n",
        "        self.p = nn.AdaptiveAvgPool1d(n_tf)\n",
        "    def __call__(self, item):\n",
        "        return self.p(item)"
      ],
      "metadata": {
        "id": "OtJ4_itDz_s8"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class FramePooling():\n",
        "    _order=40\n",
        "    def __init__(self, f, n_tf):\n",
        "        # n_tf: numner of time frames\n",
        "        self.p = f(n_tf)\n",
        "    def __call__(self, item):\n",
        "        return self.p(item)"
      ],
      "metadata": {
        "id": "1EalvurFcVfU"
      },
      "execution_count": 224,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tfms = [ToCuda(), Reshape(), FramePooling(f= nn.AdaptiveMaxPool1d, n_tf = 150)]\n",
        "il = AudioList.from_files(train_path, tfms = tfms)"
      ],
      "metadata": {
        "id": "cVNZY7nn0uOM"
      },
      "execution_count": 226,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "il[0].shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FCw0tHmT1wPq",
        "outputId": "4fe8a7dd-e6d6-4853-8294-4941039fc75e"
      },
      "execution_count": 227,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([1, 26, 150])"
            ]
          },
          "metadata": {},
          "execution_count": 227
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def re_labeler(fn, pat, subcl='act'):\n",
        "    assert subcl in ['act', 'val', 'all']\n",
        "    if subcl=='all': return ''.join(re.findall(pat, str(fn)))\n",
        "    else:\n",
        "        return re.findall(pat, str(fn))[0] if pat == 'act' else re.findall(pat, str(fn))[1]\n",
        "\n",
        "label_pat = r'_(\\d+)'\n",
        "emotion_labeler = partial(re_labeler, pat=label_pat, subcl='all')"
      ],
      "metadata": {
        "id": "kL9IVX4uzeOT"
      },
      "execution_count": 228,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sd = SplitData.split_by_func(il, partial(random_splitter, p_valid=0.2))\n",
        "ll = label_by_func(sd, emotion_labeler, proc_y=CategoryProcessor())"
      ],
      "metadata": {
        "id": "IqzVzlce1vOO"
      },
      "execution_count": 229,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bs=64\n",
        "c_in = ll.train[0][0].shape[0]\n",
        "c_out = len(uniqueify(ll.train.y))\n",
        "data = ll.to_databunch(bs,c_in=c_in,c_out=c_out)"
      ],
      "metadata": {
        "id": "XPMz25enABaj"
      },
      "execution_count": 240,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "c_in, c_out"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jjkO31kEAHeJ",
        "outputId": "0348081e-d201-4ad7-d75e-9aa86099a186"
      },
      "execution_count": 231,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1, 4)"
            ]
          },
          "metadata": {},
          "execution_count": 231
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x, y= ll.train[0]"
      ],
      "metadata": {
        "id": "3D7EF4Mg2GpI"
      },
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x.shape, y"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uxARykjF2Izl",
        "outputId": "f4e2c458-5859-4be2-c1f3-63d5106379bf"
      },
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(torch.Size([1, 26, 250]), 0)"
            ]
          },
          "metadata": {},
          "execution_count": 57
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model"
      ],
      "metadata": {
        "id": "37S5tn75yN6j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title\n",
        "model_demo = nn.Sequential(\n",
        "    \n",
        " nn.Sequential(   nn.Conv2d(\n",
        "        in_channels=1, # input volume depth == input channel dim == 1\n",
        "        out_channels=16, # expand output feature map volume's depth to 16\n",
        "        kernel_size=3, # typical 3*3 stride 1 kernel\n",
        "        stride=1,\n",
        "        padding=1\n",
        "                ),\n",
        "    nn.BatchNorm2d(16), # batch normalize the output feature map before activation\n",
        "    nn.ReLU(), # feature map --> activation map\n",
        "    nn.MaxPool2d(kernel_size=2, stride=2), #typical maxpool kernel size\n",
        "    nn.Dropout(p=0.3), #randomly zero 30% of 1st layer's output feature map in training\n",
        "    ),\n",
        "    # 2nd 2D convolution layer identical to last except output dim, maxpool kernel\n",
        "nn.Sequential(    nn.Conv2d(\n",
        "        in_channels=16, \n",
        "        out_channels=32, # expand output feature map volume's depth to 32\n",
        "        kernel_size=3,\n",
        "        stride=1,\n",
        "        padding=1\n",
        "                ),\n",
        "    nn.BatchNorm2d(32),\n",
        "    nn.ReLU(),\n",
        "    nn.MaxPool2d(kernel_size=4, stride=4), # increase maxpool kernel for subsequent filters\n",
        "    nn.Dropout(p=0.3)),\n",
        "    \n",
        "    # 3rd 2D convolution layer identical to last except output dim\n",
        "nn.Sequential(    nn.Conv2d(\n",
        "        in_channels=32,\n",
        "        out_channels=64, # expand output feature map volume's depth to 64\n",
        "        kernel_size=3,\n",
        "        stride=1,\n",
        "        padding=1\n",
        "                ),\n",
        "    nn.BatchNorm2d(64),\n",
        "    nn.ReLU(),\n",
        "    nn.AdaptiveMaxPool2d((1,4)),\n",
        "    nn.Dropout(p=0.3),)\n",
        "    )"
      ],
      "metadata": {
        "cellView": "form",
        "id": "tobyTP6O4W2b"
      },
      "execution_count": 96,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Model(nn.Module):\n",
        "    # Define all layers present in the network\n",
        "    def __init__(self,num_emotions):\n",
        "        super().__init__() \n",
        "        \n",
        "        ################ TRANSFORMER BLOCK #############################\n",
        "        # maxpool the input feature map/tensor to the transformer \n",
        "        # a rectangular kernel worked better here for the rectangular input spectrogram feature map/tensor\n",
        "        self.transformer_maxpool = nn.MaxPool2d(kernel_size=[1,4], stride=[1,4])\n",
        "        \n",
        "        # define single transformer encoder layer\n",
        "        # self-attention + feedforward network from \"Attention is All You Need\" paper\n",
        "        # 4 multi-head self-attention layers each with 26-->512--->26 feedforward network\n",
        "        transformer_layer = nn.TransformerEncoderLayer(\n",
        "            d_model=26, # input feature (frequency) dim after maxpooling 26*250 -> 26*62 (MFC*time)\n",
        "            nhead=2, # 4 self-attention layers in each multi-head self-attention layer in each encoder block\n",
        "            dim_feedforward=256, # 2 linear layers in each encoder block's feedforward network: dim 26-->512--->26\n",
        "            dropout=0.4, \n",
        "            activation='gelu' # ReLU: avoid saturation/tame gradient/reduce compute time\n",
        "        )\n",
        "        \n",
        "        # I'm using 4 instead of the 6 identical stacked encoder layrs used in Attention is All You Need paper\n",
        "        # Complete transformer block contains 4 full transformer encoder layers (each w/ multihead self-attention+feedforward)\n",
        "        self.transformer_encoder = nn.TransformerEncoder(transformer_layer, num_layers=4)\n",
        "        \n",
        "        ############### 1ST PARALLEL 2D CONVOLUTION BLOCK ############\n",
        "        # 3 sequential conv2D layers: (1,26,250) --> (16, 13, 125) -> (32, 3, 31) -> (64, 1, 4)\n",
        "        self.conv2Dblock1 = nn.Sequential(\n",
        "            \n",
        "            # 1st 2D convolution layer\n",
        "            nn.Conv2d(\n",
        "                in_channels=1, # input volume depth == input channel dim == 1\n",
        "                out_channels=16, # expand output feature map volume's depth to 16\n",
        "                kernel_size=3, # typical 3*3 stride 1 kernel\n",
        "                stride=1,\n",
        "                padding=1\n",
        "                      ),\n",
        "            nn.BatchNorm2d(16), # batch normalize the output feature map before activation\n",
        "            nn.ReLU(), # feature map --> activation map\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2), #typical maxpool kernel size\n",
        "            nn.Dropout(p=0.3), #randomly zero 30% of 1st layer's output feature map in training\n",
        "            \n",
        "            # 2nd 2D convolution layer identical to last except output dim, maxpool kernel\n",
        "            nn.Conv2d(\n",
        "                in_channels=16, \n",
        "                out_channels=32, # expand output feature map volume's depth to 32\n",
        "                kernel_size=3,\n",
        "                stride=1,\n",
        "                padding=1\n",
        "                      ),\n",
        "            nn.BatchNorm2d(32),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=4, stride=4), # increase maxpool kernel for subsequent filters\n",
        "            nn.Dropout(p=0.3), \n",
        "            \n",
        "            # 3rd 2D convolution layer identical to last except output dim\n",
        "            nn.Conv2d(\n",
        "                in_channels=32,\n",
        "                out_channels=64, # expand output feature map volume's depth to 64\n",
        "                kernel_size=3,\n",
        "                stride=1,\n",
        "                padding=1\n",
        "                      ),\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.ReLU(),\n",
        "            nn.AdaptiveMaxPool2d((1,4)),\n",
        "            nn.Dropout(p=0.3),\n",
        "        )\n",
        "        ############### 2ND PARALLEL 2D CONVOLUTION BLOCK ############\n",
        "        # 3 sequential conv2D layers: (1,40,282) --> (16, 20, 141) -> (32, 5, 35) -> (64, 1, 4)\n",
        "        self.conv2Dblock2 = nn.Sequential(\n",
        "            \n",
        "            # 1st 2D convolution layer\n",
        "            nn.Conv2d(\n",
        "                in_channels=1, # input volume depth == input channel dim == 1\n",
        "                out_channels=16, # expand output feature map volume's depth to 16\n",
        "                kernel_size=3, # typical 3*3 stride 1 kernel\n",
        "                stride=1,\n",
        "                padding=1\n",
        "                      ),\n",
        "            nn.BatchNorm2d(16), # batch normalize the output feature map before activation\n",
        "            nn.ReLU(), # feature map --> activation map\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2), #typical maxpool kernel size\n",
        "            nn.Dropout(p=0.3), #randomly zero 30% of 1st layer's output feature map in training\n",
        "            \n",
        "            # 2nd 2D convolution layer identical to last except output dim, maxpool kernel\n",
        "            nn.Conv2d(\n",
        "                in_channels=16, \n",
        "                out_channels=32, # expand output feature map volume's depth to 32\n",
        "                kernel_size=3,\n",
        "                stride=1,\n",
        "                padding=1\n",
        "                      ),\n",
        "            nn.BatchNorm2d(32),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=4, stride=4), # increase maxpool kernel for subsequent filters\n",
        "            nn.Dropout(p=0.3), \n",
        "            \n",
        "            # 3rd 2D convolution layer identical to last except output dim\n",
        "            nn.Conv2d(\n",
        "                in_channels=32,\n",
        "                out_channels=64, # expand output feature map volume's depth to 64\n",
        "                kernel_size=3,\n",
        "                stride=1,\n",
        "                padding=1\n",
        "                      ),\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.ReLU(),\n",
        "            nn.AdaptiveMaxPool2d((1,4)),\n",
        "            nn.Dropout(p=0.3),\n",
        "        )\n",
        "\n",
        "        ################# FINAL LINEAR BLOCK ####################\n",
        "        # Linear softmax layer to take final concatenated embedding tensor \n",
        "        #    from parallel 2D convolutional and transformer blocks, output 8 logits \n",
        "        # Each full convolution block outputs (64*1*8) embedding flattened to dim 512 1D array \n",
        "        # Full transformer block outputs 40*70 feature map, which we time-avg to dim 40 1D array\n",
        "        # 512*2+40 == 1064 input features --> 8 output emotions \n",
        "        self.fc1_linear = nn.Linear(256*2+26,num_emotions)\n",
        "        \n",
        "        ### Softmax layer for the 8 output logits from final FC linear layer \n",
        "        self.softmax_out = nn.Softmax(dim=1) # dim==1 is the freq embedding\n",
        "        \n",
        "    # define one complete parallel fwd pass of input feature tensor thru 2*conv+1*transformer blocks\n",
        "    def forward(self,x):\n",
        "        \n",
        "        ############ 1st parallel Conv2D block: 4 Convolutional layers ############################\n",
        "        # create final feature embedding from 1st convolutional layer \n",
        "        # input features pased through 4 sequential 2D convolutional layers\n",
        "        conv2d_embedding1 = self.conv2Dblock1(x) # x == N/batch * channel * freq * time\n",
        "        \n",
        "        # flatten final 64*1*8 feature map from convolutional layers to length 512 1D array \n",
        "        # skip the 1st (N/batch) dimension when flattening\n",
        "        conv2d_embedding1 = torch.flatten(conv2d_embedding1, start_dim=1) \n",
        "        \n",
        "        ############ 2nd parallel Conv2D block: 4 Convolutional layers #############################\n",
        "        # create final feature embedding from 2nd convolutional layer \n",
        "        # input features pased through 4 sequential 2D convolutional layers\n",
        "        conv2d_embedding2 = self.conv2Dblock2(x) # x == N/batch * channel * freq * time\n",
        "        \n",
        "        # flatten final 64*1*8 feature map from convolutional layers to length 512 1D array \n",
        "        # skip the 1st (N/batch) dimension when flattening\n",
        "        conv2d_embedding2 = torch.flatten(conv2d_embedding2, start_dim=1) \n",
        "        \n",
        "         \n",
        "        ########## 4-encoder-layer Transformer block w/ 26-->512-->26 feedfwd network ##############\n",
        "        # maxpool input feature map: 1*40*282 w/ 1*4 kernel --> 1*40*70\n",
        "        x_maxpool = self.transformer_maxpool(x)\n",
        "\n",
        "        # remove channel dim: 1*40*70 --> 40*70\n",
        "        x_maxpool_reduced = torch.squeeze(x_maxpool,1)\n",
        "        \n",
        "        # convert maxpooled feature map format: batch * freq * time ---> time * batch * freq format\n",
        "        # because transformer encoder layer requires tensor in format: time * batch * embedding (freq)\n",
        "        x = x_maxpool_reduced.permute(2,0,1) \n",
        "        \n",
        "        # finally, pass reduced input feature map x into transformer encoder layers\n",
        "        transformer_output = self.transformer_encoder(x)\n",
        "        \n",
        "        # create final feature emedding from transformer layer by taking mean in the time dimension (now the 0th dim)\n",
        "        # transformer outputs 2x40 (MFCC embedding*time) feature map, take mean of columns i.e. take time average\n",
        "        transformer_embedding = torch.mean(transformer_output, dim=0) # dim 40x70 --> 40\n",
        "        \n",
        "        ############# concatenate freq embeddings from convolutional and transformer blocks ######\n",
        "        # concatenate embedding tensors output by parallel 2*conv and 1*transformer blocks\n",
        "        complete_embedding = torch.cat([conv2d_embedding1, conv2d_embedding2,transformer_embedding], dim=1)  \n",
        "\n",
        "        ######### final FC linear layer, need logits for loss #########################\n",
        "        output_logits = self.fc1_linear(complete_embedding)\n",
        "        \n",
        "        # need output logits to compute cross entropy loss, need softmax probabilities to predict class\n",
        "        return output_logits"
      ],
      "metadata": {
        "id": "7Wd4KI6Url1J"
      },
      "execution_count": 239,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -qq torchmetrics\n",
        "from torchmetrics import F1\n",
        "f1 = F1(num_classes=4)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ao_l1zIz_sMt",
        "outputId": "eb80c48d-d36a-4a1e-a43b-f4cc6367f95c"
      },
      "execution_count": 124,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l\r\u001b[K     |█                               | 10 kB 34.0 MB/s eta 0:00:01\r\u001b[K     |██                              | 20 kB 24.7 MB/s eta 0:00:01\r\u001b[K     |███                             | 30 kB 16.8 MB/s eta 0:00:01\r\u001b[K     |████                            | 40 kB 14.3 MB/s eta 0:00:01\r\u001b[K     |█████                           | 51 kB 10.5 MB/s eta 0:00:01\r\u001b[K     |██████                          | 61 kB 9.9 MB/s eta 0:00:01\r\u001b[K     |███████                         | 71 kB 9.7 MB/s eta 0:00:01\r\u001b[K     |███████▉                        | 81 kB 10.8 MB/s eta 0:00:01\r\u001b[K     |████████▉                       | 92 kB 11.2 MB/s eta 0:00:01\r\u001b[K     |█████████▉                      | 102 kB 9.8 MB/s eta 0:00:01\r\u001b[K     |██████████▉                     | 112 kB 9.8 MB/s eta 0:00:01\r\u001b[K     |███████████▉                    | 122 kB 9.8 MB/s eta 0:00:01\r\u001b[K     |████████████▉                   | 133 kB 9.8 MB/s eta 0:00:01\r\u001b[K     |█████████████▉                  | 143 kB 9.8 MB/s eta 0:00:01\r\u001b[K     |██████████████▉                 | 153 kB 9.8 MB/s eta 0:00:01\r\u001b[K     |███████████████▊                | 163 kB 9.8 MB/s eta 0:00:01\r\u001b[K     |████████████████▊               | 174 kB 9.8 MB/s eta 0:00:01\r\u001b[K     |█████████████████▊              | 184 kB 9.8 MB/s eta 0:00:01\r\u001b[K     |██████████████████▊             | 194 kB 9.8 MB/s eta 0:00:01\r\u001b[K     |███████████████████▊            | 204 kB 9.8 MB/s eta 0:00:01\r\u001b[K     |████████████████████▊           | 215 kB 9.8 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▊          | 225 kB 9.8 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▋         | 235 kB 9.8 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▋        | 245 kB 9.8 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▋       | 256 kB 9.8 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▋      | 266 kB 9.8 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▋     | 276 kB 9.8 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▋    | 286 kB 9.8 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▋   | 296 kB 9.8 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▋  | 307 kB 9.8 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▌ | 317 kB 9.8 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▌| 327 kB 9.8 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 332 kB 9.8 MB/s \n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "hdaoeOcvi-N1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# opt_func = adam_opt(mom=0.9, mom_sqr=0.99, eps=1e-6, wd=1e-2)\n",
        "loss_func = LabelSmoothingCrossEntropy()\n",
        "lr = 1e-2\n",
        "pct_start = 0.5\n",
        "phases = create_phases(pct_start)\n",
        "sched_lr  = combine_scheds(phases, cos_1cycle_anneal(lr/10., lr, lr/1e5))\n",
        "sched_mom = combine_scheds(phases, cos_1cycle_anneal(0.95,0.85, 0.95))\n",
        "cbscheds = [ParamScheduler('lr', sched_lr), \n",
        "            ParamScheduler('mom', sched_mom)]"
      ],
      "metadata": {
        "id": "sDKXz6KZ9MNR"
      },
      "execution_count": 246,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def learner(arch, data, loss_func, opt_func, c_in=None, c_out=None,\n",
        "                lr=1e-2, cuda=True, norm=None, progress=True, mixup=0, xtra_cb=None, **kwargs):\n",
        "    cbfs = [partial(AvgStatsCallback,accuracy)]+listify(xtra_cb)\n",
        "    if progress: cbfs.append(ProgressCallback)\n",
        "    if cuda:     cbfs.append(CudaCallback)\n",
        "    if norm:     cbfs.append(partial(BatchTransformXCallback, norm))\n",
        "    if mixup:    cbfs.append(partial(MixUp, mixup))\n",
        "    arch_args = {}\n",
        "    if not c_in : c_in  = data.c_in\n",
        "    if not c_out: c_out = data.c_out\n",
        "    if c_in:  arch_args['c_in' ]=c_in\n",
        "    if c_out: arch_args['c_out']=c_out\n",
        "    return Learner(arch, data, loss_func, opt_func=opt_func, lr=lr, cb_funcs=cbfs, **kwargs)\n"
      ],
      "metadata": {
        "id": "GGpNDq0l976E"
      },
      "execution_count": 234,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = Model(c_out).cuda()"
      ],
      "metadata": {
        "id": "uUJTouSuJSDY"
      },
      "execution_count": 241,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def sgd_opt(xtra_step=None, **kwargs):\n",
        "    return partial(Optimizer, steppers=[weight_decay]+listify(xtra_step),\n",
        "                   stats=[AverageGrad(dampening=True), AverageSqrGrad(), StepCount()], **kwargs)"
      ],
      "metadata": {
        "id": "XCly0VbbjebN"
      },
      "execution_count": 254,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "opt_func = sgd_opt(lr=0.01, weight_decay=1e-3, momentum=0.8)"
      ],
      "metadata": {
        "id": "_FePvDsLispY"
      },
      "execution_count": 255,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "learn = learner(model, data, loss_func, opt_func=opt_func)"
      ],
      "metadata": {
        "id": "zwLQiDDK_XhW"
      },
      "execution_count": 256,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "learn.fit(300, cbs=cbscheds)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "hZF4d9AZAmAv",
        "outputId": "1130ef90-441a-44bb-dcf8-9495f6d261a8"
      },
      "execution_count": 257,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "        <style>\n",
              "            /* Turns off some styling */\n",
              "            progress {\n",
              "                /* gets rid of default border in Firefox and Opera. */\n",
              "                border: none;\n",
              "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
              "                background-size: auto;\n",
              "            }\n",
              "            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
              "                background: #F44336;\n",
              "            }\n",
              "        </style>\n",
              "      <progress value='0' class='' max='300' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      \n",
              "    </div>\n",
              "    \n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: left;\">\n",
              "      <th>epoch</th>\n",
              "      <th>train_loss</th>\n",
              "      <th>train_accuracy</th>\n",
              "      <th>valid_loss</th>\n",
              "      <th>valid_accuracy</th>\n",
              "      <th>time</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>0</td>\n",
              "      <td>2.520974</td>\n",
              "      <td>0.268680</td>\n",
              "      <td>1.658805</td>\n",
              "      <td>0.289404</td>\n",
              "      <td>00:10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>2.498805</td>\n",
              "      <td>0.259936</td>\n",
              "      <td>1.663060</td>\n",
              "      <td>0.288079</td>\n",
              "      <td>00:11</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>2.516170</td>\n",
              "      <td>0.267091</td>\n",
              "      <td>1.656659</td>\n",
              "      <td>0.288742</td>\n",
              "      <td>00:11</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>2.515422</td>\n",
              "      <td>0.264388</td>\n",
              "      <td>1.661606</td>\n",
              "      <td>0.288742</td>\n",
              "      <td>00:10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>2.506654</td>\n",
              "      <td>0.275676</td>\n",
              "      <td>1.659923</td>\n",
              "      <td>0.288742</td>\n",
              "      <td>00:10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>2.520093</td>\n",
              "      <td>0.267568</td>\n",
              "      <td>1.663013</td>\n",
              "      <td>0.288742</td>\n",
              "      <td>00:10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6</td>\n",
              "      <td>2.508234</td>\n",
              "      <td>0.262957</td>\n",
              "      <td>1.662208</td>\n",
              "      <td>0.288742</td>\n",
              "      <td>00:10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7</td>\n",
              "      <td>2.511315</td>\n",
              "      <td>0.261685</td>\n",
              "      <td>1.652186</td>\n",
              "      <td>0.289404</td>\n",
              "      <td>00:10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8</td>\n",
              "      <td>2.508366</td>\n",
              "      <td>0.269316</td>\n",
              "      <td>1.662082</td>\n",
              "      <td>0.288079</td>\n",
              "      <td>00:10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9</td>\n",
              "      <td>2.508068</td>\n",
              "      <td>0.268362</td>\n",
              "      <td>1.662718</td>\n",
              "      <td>0.288079</td>\n",
              "      <td>00:10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>10</td>\n",
              "      <td>2.517882</td>\n",
              "      <td>0.265819</td>\n",
              "      <td>1.665645</td>\n",
              "      <td>0.287417</td>\n",
              "      <td>00:10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>11</td>\n",
              "      <td>2.521839</td>\n",
              "      <td>0.263434</td>\n",
              "      <td>1.654901</td>\n",
              "      <td>0.289404</td>\n",
              "      <td>00:10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>12</td>\n",
              "      <td>2.504300</td>\n",
              "      <td>0.257075</td>\n",
              "      <td>1.661515</td>\n",
              "      <td>0.288742</td>\n",
              "      <td>00:10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>13</td>\n",
              "      <td>2.500654</td>\n",
              "      <td>0.264070</td>\n",
              "      <td>1.660983</td>\n",
              "      <td>0.288742</td>\n",
              "      <td>00:10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>14</td>\n",
              "      <td>2.509559</td>\n",
              "      <td>0.261367</td>\n",
              "      <td>1.660246</td>\n",
              "      <td>0.288742</td>\n",
              "      <td>00:10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>15</td>\n",
              "      <td>2.504022</td>\n",
              "      <td>0.263593</td>\n",
              "      <td>1.654781</td>\n",
              "      <td>0.289404</td>\n",
              "      <td>00:10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>16</td>\n",
              "      <td>2.514730</td>\n",
              "      <td>0.271383</td>\n",
              "      <td>1.667949</td>\n",
              "      <td>0.288079</td>\n",
              "      <td>00:10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>17</td>\n",
              "      <td>2.499478</td>\n",
              "      <td>0.265024</td>\n",
              "      <td>1.661151</td>\n",
              "      <td>0.288742</td>\n",
              "      <td>00:10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>18</td>\n",
              "      <td>2.527965</td>\n",
              "      <td>0.267409</td>\n",
              "      <td>1.652730</td>\n",
              "      <td>0.289404</td>\n",
              "      <td>00:10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>19</td>\n",
              "      <td>2.506129</td>\n",
              "      <td>0.265819</td>\n",
              "      <td>1.660629</td>\n",
              "      <td>0.288742</td>\n",
              "      <td>00:10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>20</td>\n",
              "      <td>2.520275</td>\n",
              "      <td>0.262003</td>\n",
              "      <td>1.658117</td>\n",
              "      <td>0.289404</td>\n",
              "      <td>00:10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>21</td>\n",
              "      <td>2.511476</td>\n",
              "      <td>0.265501</td>\n",
              "      <td>1.657171</td>\n",
              "      <td>0.288742</td>\n",
              "      <td>00:10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>22</td>\n",
              "      <td>2.501991</td>\n",
              "      <td>0.264865</td>\n",
              "      <td>1.659265</td>\n",
              "      <td>0.288742</td>\n",
              "      <td>00:10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>23</td>\n",
              "      <td>2.536958</td>\n",
              "      <td>0.261049</td>\n",
              "      <td>1.665455</td>\n",
              "      <td>0.288742</td>\n",
              "      <td>00:10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>24</td>\n",
              "      <td>2.521394</td>\n",
              "      <td>0.262321</td>\n",
              "      <td>1.662216</td>\n",
              "      <td>0.288742</td>\n",
              "      <td>00:10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>25</td>\n",
              "      <td>2.509966</td>\n",
              "      <td>0.268045</td>\n",
              "      <td>1.660347</td>\n",
              "      <td>0.288079</td>\n",
              "      <td>00:11</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>26</td>\n",
              "      <td>2.511726</td>\n",
              "      <td>0.268203</td>\n",
              "      <td>1.665633</td>\n",
              "      <td>0.288079</td>\n",
              "      <td>00:10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>27</td>\n",
              "      <td>2.519810</td>\n",
              "      <td>0.265819</td>\n",
              "      <td>1.665482</td>\n",
              "      <td>0.288742</td>\n",
              "      <td>00:10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>28</td>\n",
              "      <td>2.523236</td>\n",
              "      <td>0.264229</td>\n",
              "      <td>1.668280</td>\n",
              "      <td>0.286755</td>\n",
              "      <td>00:10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>29</td>\n",
              "      <td>2.512423</td>\n",
              "      <td>0.261367</td>\n",
              "      <td>1.659834</td>\n",
              "      <td>0.288079</td>\n",
              "      <td>00:10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>30</td>\n",
              "      <td>2.503942</td>\n",
              "      <td>0.267886</td>\n",
              "      <td>1.663175</td>\n",
              "      <td>0.288742</td>\n",
              "      <td>00:11</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>31</td>\n",
              "      <td>2.506759</td>\n",
              "      <td>0.265660</td>\n",
              "      <td>1.658109</td>\n",
              "      <td>0.288742</td>\n",
              "      <td>00:11</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>32</td>\n",
              "      <td>2.528551</td>\n",
              "      <td>0.262639</td>\n",
              "      <td>1.660244</td>\n",
              "      <td>0.288079</td>\n",
              "      <td>00:10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>33</td>\n",
              "      <td>2.523988</td>\n",
              "      <td>0.270429</td>\n",
              "      <td>1.660659</td>\n",
              "      <td>0.288742</td>\n",
              "      <td>00:10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>34</td>\n",
              "      <td>2.516207</td>\n",
              "      <td>0.260413</td>\n",
              "      <td>1.656273</td>\n",
              "      <td>0.289404</td>\n",
              "      <td>00:10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>35</td>\n",
              "      <td>2.504006</td>\n",
              "      <td>0.269634</td>\n",
              "      <td>1.659054</td>\n",
              "      <td>0.289404</td>\n",
              "      <td>00:10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>36</td>\n",
              "      <td>2.515283</td>\n",
              "      <td>0.261844</td>\n",
              "      <td>1.664172</td>\n",
              "      <td>0.288079</td>\n",
              "      <td>00:10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>37</td>\n",
              "      <td>2.514037</td>\n",
              "      <td>0.263434</td>\n",
              "      <td>1.654387</td>\n",
              "      <td>0.289404</td>\n",
              "      <td>00:10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>38</td>\n",
              "      <td>2.529308</td>\n",
              "      <td>0.265024</td>\n",
              "      <td>1.663545</td>\n",
              "      <td>0.288742</td>\n",
              "      <td>00:10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>39</td>\n",
              "      <td>2.507611</td>\n",
              "      <td>0.267409</td>\n",
              "      <td>1.659120</td>\n",
              "      <td>0.289404</td>\n",
              "      <td>00:10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>40</td>\n",
              "      <td>2.516878</td>\n",
              "      <td>0.265024</td>\n",
              "      <td>1.658861</td>\n",
              "      <td>0.288742</td>\n",
              "      <td>00:10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>41</td>\n",
              "      <td>2.506649</td>\n",
              "      <td>0.262957</td>\n",
              "      <td>1.660098</td>\n",
              "      <td>0.288742</td>\n",
              "      <td>00:10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>42</td>\n",
              "      <td>2.504647</td>\n",
              "      <td>0.265819</td>\n",
              "      <td>1.658799</td>\n",
              "      <td>0.289404</td>\n",
              "      <td>00:10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>43</td>\n",
              "      <td>2.510988</td>\n",
              "      <td>0.262003</td>\n",
              "      <td>1.656042</td>\n",
              "      <td>0.289404</td>\n",
              "      <td>00:10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>44</td>\n",
              "      <td>2.510871</td>\n",
              "      <td>0.262162</td>\n",
              "      <td>1.668322</td>\n",
              "      <td>0.288079</td>\n",
              "      <td>00:10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>45</td>\n",
              "      <td>2.521677</td>\n",
              "      <td>0.263911</td>\n",
              "      <td>1.662062</td>\n",
              "      <td>0.288742</td>\n",
              "      <td>00:10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>46</td>\n",
              "      <td>2.540296</td>\n",
              "      <td>0.265024</td>\n",
              "      <td>1.662781</td>\n",
              "      <td>0.288079</td>\n",
              "      <td>00:10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>47</td>\n",
              "      <td>2.512294</td>\n",
              "      <td>0.262639</td>\n",
              "      <td>1.659402</td>\n",
              "      <td>0.288742</td>\n",
              "      <td>00:10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>48</td>\n",
              "      <td>2.521197</td>\n",
              "      <td>0.266455</td>\n",
              "      <td>1.664656</td>\n",
              "      <td>0.288742</td>\n",
              "      <td>00:10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>49</td>\n",
              "      <td>2.497262</td>\n",
              "      <td>0.268203</td>\n",
              "      <td>1.663126</td>\n",
              "      <td>0.288742</td>\n",
              "      <td>00:10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>50</td>\n",
              "      <td>2.517431</td>\n",
              "      <td>0.265183</td>\n",
              "      <td>1.663035</td>\n",
              "      <td>0.288079</td>\n",
              "      <td>00:10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>51</td>\n",
              "      <td>2.518232</td>\n",
              "      <td>0.265660</td>\n",
              "      <td>1.661348</td>\n",
              "      <td>0.288742</td>\n",
              "      <td>00:10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>52</td>\n",
              "      <td>2.505525</td>\n",
              "      <td>0.263434</td>\n",
              "      <td>1.659222</td>\n",
              "      <td>0.288742</td>\n",
              "      <td>00:10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>53</td>\n",
              "      <td>2.511304</td>\n",
              "      <td>0.266296</td>\n",
              "      <td>1.661848</td>\n",
              "      <td>0.288742</td>\n",
              "      <td>00:10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>54</td>\n",
              "      <td>2.509920</td>\n",
              "      <td>0.262003</td>\n",
              "      <td>1.662711</td>\n",
              "      <td>0.288079</td>\n",
              "      <td>00:10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>55</td>\n",
              "      <td>2.507388</td>\n",
              "      <td>0.262321</td>\n",
              "      <td>1.657911</td>\n",
              "      <td>0.289404</td>\n",
              "      <td>00:10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>56</td>\n",
              "      <td>2.511547</td>\n",
              "      <td>0.263275</td>\n",
              "      <td>1.658419</td>\n",
              "      <td>0.289404</td>\n",
              "      <td>00:10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>57</td>\n",
              "      <td>2.498348</td>\n",
              "      <td>0.268680</td>\n",
              "      <td>1.658362</td>\n",
              "      <td>0.288742</td>\n",
              "      <td>00:10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>58</td>\n",
              "      <td>2.528175</td>\n",
              "      <td>0.262480</td>\n",
              "      <td>1.657006</td>\n",
              "      <td>0.289404</td>\n",
              "      <td>00:11</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>59</td>\n",
              "      <td>2.528132</td>\n",
              "      <td>0.263911</td>\n",
              "      <td>1.659989</td>\n",
              "      <td>0.288742</td>\n",
              "      <td>00:11</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>60</td>\n",
              "      <td>2.525308</td>\n",
              "      <td>0.261844</td>\n",
              "      <td>1.666789</td>\n",
              "      <td>0.288079</td>\n",
              "      <td>00:11</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>61</td>\n",
              "      <td>2.516577</td>\n",
              "      <td>0.257234</td>\n",
              "      <td>1.664258</td>\n",
              "      <td>0.287417</td>\n",
              "      <td>00:11</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>62</td>\n",
              "      <td>2.512734</td>\n",
              "      <td>0.261367</td>\n",
              "      <td>1.655345</td>\n",
              "      <td>0.289404</td>\n",
              "      <td>00:10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>63</td>\n",
              "      <td>2.520110</td>\n",
              "      <td>0.265024</td>\n",
              "      <td>1.660569</td>\n",
              "      <td>0.288742</td>\n",
              "      <td>00:10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>64</td>\n",
              "      <td>2.521049</td>\n",
              "      <td>0.268680</td>\n",
              "      <td>1.662504</td>\n",
              "      <td>0.288742</td>\n",
              "      <td>00:10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>65</td>\n",
              "      <td>2.502611</td>\n",
              "      <td>0.266932</td>\n",
              "      <td>1.656766</td>\n",
              "      <td>0.289404</td>\n",
              "      <td>00:10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>66</td>\n",
              "      <td>2.504377</td>\n",
              "      <td>0.263434</td>\n",
              "      <td>1.662383</td>\n",
              "      <td>0.288079</td>\n",
              "      <td>00:10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>67</td>\n",
              "      <td>2.522090</td>\n",
              "      <td>0.265660</td>\n",
              "      <td>1.661601</td>\n",
              "      <td>0.288742</td>\n",
              "      <td>00:10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>68</td>\n",
              "      <td>2.500913</td>\n",
              "      <td>0.263275</td>\n",
              "      <td>1.659105</td>\n",
              "      <td>0.288742</td>\n",
              "      <td>00:10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>69</td>\n",
              "      <td>2.504307</td>\n",
              "      <td>0.267409</td>\n",
              "      <td>1.658046</td>\n",
              "      <td>0.288742</td>\n",
              "      <td>00:10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>70</td>\n",
              "      <td>2.521640</td>\n",
              "      <td>0.264547</td>\n",
              "      <td>1.665291</td>\n",
              "      <td>0.288079</td>\n",
              "      <td>00:10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>71</td>\n",
              "      <td>2.520888</td>\n",
              "      <td>0.266137</td>\n",
              "      <td>1.658273</td>\n",
              "      <td>0.288742</td>\n",
              "      <td>00:10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>72</td>\n",
              "      <td>2.520726</td>\n",
              "      <td>0.262480</td>\n",
              "      <td>1.662070</td>\n",
              "      <td>0.288742</td>\n",
              "      <td>00:10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>73</td>\n",
              "      <td>2.506462</td>\n",
              "      <td>0.266455</td>\n",
              "      <td>1.662633</td>\n",
              "      <td>0.288742</td>\n",
              "      <td>00:10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>74</td>\n",
              "      <td>2.512303</td>\n",
              "      <td>0.273450</td>\n",
              "      <td>1.658586</td>\n",
              "      <td>0.288742</td>\n",
              "      <td>00:10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>75</td>\n",
              "      <td>2.517513</td>\n",
              "      <td>0.267091</td>\n",
              "      <td>1.656126</td>\n",
              "      <td>0.289404</td>\n",
              "      <td>00:10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>76</td>\n",
              "      <td>2.517802</td>\n",
              "      <td>0.260890</td>\n",
              "      <td>1.662606</td>\n",
              "      <td>0.288079</td>\n",
              "      <td>00:10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>77</td>\n",
              "      <td>2.524155</td>\n",
              "      <td>0.260254</td>\n",
              "      <td>1.657961</td>\n",
              "      <td>0.288742</td>\n",
              "      <td>00:11</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>78</td>\n",
              "      <td>2.516412</td>\n",
              "      <td>0.264070</td>\n",
              "      <td>1.666093</td>\n",
              "      <td>0.288079</td>\n",
              "      <td>00:10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>79</td>\n",
              "      <td>2.529102</td>\n",
              "      <td>0.262480</td>\n",
              "      <td>1.654304</td>\n",
              "      <td>0.289404</td>\n",
              "      <td>00:11</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>80</td>\n",
              "      <td>2.518635</td>\n",
              "      <td>0.264865</td>\n",
              "      <td>1.662371</td>\n",
              "      <td>0.288742</td>\n",
              "      <td>00:10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>81</td>\n",
              "      <td>2.529916</td>\n",
              "      <td>0.256598</td>\n",
              "      <td>1.655008</td>\n",
              "      <td>0.289404</td>\n",
              "      <td>00:10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>82</td>\n",
              "      <td>2.524224</td>\n",
              "      <td>0.265024</td>\n",
              "      <td>1.662865</td>\n",
              "      <td>0.288742</td>\n",
              "      <td>00:10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>83</td>\n",
              "      <td>2.505931</td>\n",
              "      <td>0.264070</td>\n",
              "      <td>1.666982</td>\n",
              "      <td>0.288079</td>\n",
              "      <td>00:11</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>84</td>\n",
              "      <td>2.510006</td>\n",
              "      <td>0.264388</td>\n",
              "      <td>1.659222</td>\n",
              "      <td>0.288742</td>\n",
              "      <td>00:10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>85</td>\n",
              "      <td>2.511321</td>\n",
              "      <td>0.267091</td>\n",
              "      <td>1.660302</td>\n",
              "      <td>0.288742</td>\n",
              "      <td>00:10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>86</td>\n",
              "      <td>2.520740</td>\n",
              "      <td>0.267727</td>\n",
              "      <td>1.663827</td>\n",
              "      <td>0.288079</td>\n",
              "      <td>00:10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>87</td>\n",
              "      <td>2.517274</td>\n",
              "      <td>0.264706</td>\n",
              "      <td>1.663996</td>\n",
              "      <td>0.288742</td>\n",
              "      <td>00:11</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>88</td>\n",
              "      <td>2.515063</td>\n",
              "      <td>0.263752</td>\n",
              "      <td>1.659349</td>\n",
              "      <td>0.288742</td>\n",
              "      <td>00:10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>89</td>\n",
              "      <td>2.522038</td>\n",
              "      <td>0.266137</td>\n",
              "      <td>1.666807</td>\n",
              "      <td>0.288079</td>\n",
              "      <td>00:10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>90</td>\n",
              "      <td>2.513163</td>\n",
              "      <td>0.267409</td>\n",
              "      <td>1.659234</td>\n",
              "      <td>0.289404</td>\n",
              "      <td>00:11</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>91</td>\n",
              "      <td>2.519261</td>\n",
              "      <td>0.266455</td>\n",
              "      <td>1.657236</td>\n",
              "      <td>0.288742</td>\n",
              "      <td>00:10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>92</td>\n",
              "      <td>2.521271</td>\n",
              "      <td>0.262003</td>\n",
              "      <td>1.661310</td>\n",
              "      <td>0.288079</td>\n",
              "      <td>00:10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>93</td>\n",
              "      <td>2.509990</td>\n",
              "      <td>0.268998</td>\n",
              "      <td>1.651636</td>\n",
              "      <td>0.289404</td>\n",
              "      <td>00:11</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>94</td>\n",
              "      <td>2.527431</td>\n",
              "      <td>0.261208</td>\n",
              "      <td>1.666439</td>\n",
              "      <td>0.288079</td>\n",
              "      <td>00:10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>95</td>\n",
              "      <td>2.525025</td>\n",
              "      <td>0.267409</td>\n",
              "      <td>1.660274</td>\n",
              "      <td>0.288742</td>\n",
              "      <td>00:11</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>96</td>\n",
              "      <td>2.518193</td>\n",
              "      <td>0.265183</td>\n",
              "      <td>1.658000</td>\n",
              "      <td>0.289404</td>\n",
              "      <td>00:10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>97</td>\n",
              "      <td>2.522512</td>\n",
              "      <td>0.261526</td>\n",
              "      <td>1.664218</td>\n",
              "      <td>0.288742</td>\n",
              "      <td>00:11</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>98</td>\n",
              "      <td>2.528880</td>\n",
              "      <td>0.260095</td>\n",
              "      <td>1.666506</td>\n",
              "      <td>0.288079</td>\n",
              "      <td>00:10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>99</td>\n",
              "      <td>2.500677</td>\n",
              "      <td>0.266455</td>\n",
              "      <td>1.665969</td>\n",
              "      <td>0.288079</td>\n",
              "      <td>00:10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>100</td>\n",
              "      <td>2.526488</td>\n",
              "      <td>0.263116</td>\n",
              "      <td>1.662455</td>\n",
              "      <td>0.288742</td>\n",
              "      <td>00:10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>101</td>\n",
              "      <td>2.515293</td>\n",
              "      <td>0.261526</td>\n",
              "      <td>1.654262</td>\n",
              "      <td>0.289404</td>\n",
              "      <td>00:11</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>102</td>\n",
              "      <td>2.498932</td>\n",
              "      <td>0.263752</td>\n",
              "      <td>1.669224</td>\n",
              "      <td>0.288079</td>\n",
              "      <td>00:10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>103</td>\n",
              "      <td>2.526022</td>\n",
              "      <td>0.260572</td>\n",
              "      <td>1.662710</td>\n",
              "      <td>0.288742</td>\n",
              "      <td>00:10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>104</td>\n",
              "      <td>2.510553</td>\n",
              "      <td>0.269793</td>\n",
              "      <td>1.666673</td>\n",
              "      <td>0.288079</td>\n",
              "      <td>00:10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>105</td>\n",
              "      <td>2.509347</td>\n",
              "      <td>0.269316</td>\n",
              "      <td>1.659584</td>\n",
              "      <td>0.289404</td>\n",
              "      <td>00:10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>106</td>\n",
              "      <td>2.513480</td>\n",
              "      <td>0.270111</td>\n",
              "      <td>1.660567</td>\n",
              "      <td>0.288079</td>\n",
              "      <td>00:10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>107</td>\n",
              "      <td>2.503391</td>\n",
              "      <td>0.267250</td>\n",
              "      <td>1.660135</td>\n",
              "      <td>0.288742</td>\n",
              "      <td>00:10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>108</td>\n",
              "      <td>2.514411</td>\n",
              "      <td>0.265183</td>\n",
              "      <td>1.659536</td>\n",
              "      <td>0.288742</td>\n",
              "      <td>00:10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>109</td>\n",
              "      <td>2.513158</td>\n",
              "      <td>0.266773</td>\n",
              "      <td>1.658256</td>\n",
              "      <td>0.289404</td>\n",
              "      <td>00:10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>110</td>\n",
              "      <td>2.515187</td>\n",
              "      <td>0.261844</td>\n",
              "      <td>1.663683</td>\n",
              "      <td>0.288079</td>\n",
              "      <td>00:10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>111</td>\n",
              "      <td>2.511799</td>\n",
              "      <td>0.267091</td>\n",
              "      <td>1.660535</td>\n",
              "      <td>0.288079</td>\n",
              "      <td>00:10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>112</td>\n",
              "      <td>2.501369</td>\n",
              "      <td>0.271383</td>\n",
              "      <td>1.665878</td>\n",
              "      <td>0.288742</td>\n",
              "      <td>00:10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>113</td>\n",
              "      <td>2.504908</td>\n",
              "      <td>0.262480</td>\n",
              "      <td>1.658213</td>\n",
              "      <td>0.288742</td>\n",
              "      <td>00:10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>114</td>\n",
              "      <td>2.514117</td>\n",
              "      <td>0.262639</td>\n",
              "      <td>1.662802</td>\n",
              "      <td>0.288079</td>\n",
              "      <td>00:10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>115</td>\n",
              "      <td>2.520847</td>\n",
              "      <td>0.270111</td>\n",
              "      <td>1.658995</td>\n",
              "      <td>0.288742</td>\n",
              "      <td>00:11</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>116</td>\n",
              "      <td>2.517364</td>\n",
              "      <td>0.263911</td>\n",
              "      <td>1.655917</td>\n",
              "      <td>0.289404</td>\n",
              "      <td>00:11</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>117</td>\n",
              "      <td>2.528074</td>\n",
              "      <td>0.265978</td>\n",
              "      <td>1.657866</td>\n",
              "      <td>0.288742</td>\n",
              "      <td>00:10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>118</td>\n",
              "      <td>2.524445</td>\n",
              "      <td>0.263911</td>\n",
              "      <td>1.659732</td>\n",
              "      <td>0.288742</td>\n",
              "      <td>00:10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>119</td>\n",
              "      <td>2.509079</td>\n",
              "      <td>0.267568</td>\n",
              "      <td>1.659004</td>\n",
              "      <td>0.288742</td>\n",
              "      <td>00:10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>120</td>\n",
              "      <td>2.506069</td>\n",
              "      <td>0.266614</td>\n",
              "      <td>1.660440</td>\n",
              "      <td>0.288742</td>\n",
              "      <td>00:10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>121</td>\n",
              "      <td>2.515641</td>\n",
              "      <td>0.263911</td>\n",
              "      <td>1.661096</td>\n",
              "      <td>0.288742</td>\n",
              "      <td>00:10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>122</td>\n",
              "      <td>2.508592</td>\n",
              "      <td>0.265024</td>\n",
              "      <td>1.661206</td>\n",
              "      <td>0.288079</td>\n",
              "      <td>00:10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>123</td>\n",
              "      <td>2.529423</td>\n",
              "      <td>0.261526</td>\n",
              "      <td>1.655959</td>\n",
              "      <td>0.289404</td>\n",
              "      <td>00:11</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>124</td>\n",
              "      <td>2.510508</td>\n",
              "      <td>0.264865</td>\n",
              "      <td>1.660325</td>\n",
              "      <td>0.288079</td>\n",
              "      <td>00:10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>125</td>\n",
              "      <td>2.523706</td>\n",
              "      <td>0.266932</td>\n",
              "      <td>1.663583</td>\n",
              "      <td>0.288742</td>\n",
              "      <td>00:10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>126</td>\n",
              "      <td>2.523540</td>\n",
              "      <td>0.265660</td>\n",
              "      <td>1.659331</td>\n",
              "      <td>0.288742</td>\n",
              "      <td>00:10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>127</td>\n",
              "      <td>2.503923</td>\n",
              "      <td>0.265024</td>\n",
              "      <td>1.665959</td>\n",
              "      <td>0.288079</td>\n",
              "      <td>00:10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>128</td>\n",
              "      <td>2.508501</td>\n",
              "      <td>0.264706</td>\n",
              "      <td>1.664586</td>\n",
              "      <td>0.288079</td>\n",
              "      <td>00:11</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>129</td>\n",
              "      <td>2.505301</td>\n",
              "      <td>0.268045</td>\n",
              "      <td>1.665059</td>\n",
              "      <td>0.288079</td>\n",
              "      <td>00:11</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>130</td>\n",
              "      <td>2.527978</td>\n",
              "      <td>0.256439</td>\n",
              "      <td>1.664584</td>\n",
              "      <td>0.288079</td>\n",
              "      <td>00:10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>131</td>\n",
              "      <td>2.514949</td>\n",
              "      <td>0.266296</td>\n",
              "      <td>1.663232</td>\n",
              "      <td>0.288079</td>\n",
              "      <td>00:10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>132</td>\n",
              "      <td>2.520632</td>\n",
              "      <td>0.264865</td>\n",
              "      <td>1.656580</td>\n",
              "      <td>0.289404</td>\n",
              "      <td>00:10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>133</td>\n",
              "      <td>2.507896</td>\n",
              "      <td>0.262957</td>\n",
              "      <td>1.656145</td>\n",
              "      <td>0.288742</td>\n",
              "      <td>00:10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>134</td>\n",
              "      <td>2.499418</td>\n",
              "      <td>0.266137</td>\n",
              "      <td>1.659600</td>\n",
              "      <td>0.288742</td>\n",
              "      <td>00:11</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>135</td>\n",
              "      <td>2.512427</td>\n",
              "      <td>0.262321</td>\n",
              "      <td>1.662516</td>\n",
              "      <td>0.288079</td>\n",
              "      <td>00:10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>136</td>\n",
              "      <td>2.518692</td>\n",
              "      <td>0.270906</td>\n",
              "      <td>1.667148</td>\n",
              "      <td>0.288079</td>\n",
              "      <td>00:10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>137</td>\n",
              "      <td>2.504856</td>\n",
              "      <td>0.265183</td>\n",
              "      <td>1.664990</td>\n",
              "      <td>0.288079</td>\n",
              "      <td>00:10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>138</td>\n",
              "      <td>2.522403</td>\n",
              "      <td>0.261049</td>\n",
              "      <td>1.662403</td>\n",
              "      <td>0.288079</td>\n",
              "      <td>00:10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>139</td>\n",
              "      <td>2.524562</td>\n",
              "      <td>0.261367</td>\n",
              "      <td>1.670787</td>\n",
              "      <td>0.287417</td>\n",
              "      <td>00:10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>140</td>\n",
              "      <td>2.509015</td>\n",
              "      <td>0.264865</td>\n",
              "      <td>1.659110</td>\n",
              "      <td>0.288742</td>\n",
              "      <td>00:10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>141</td>\n",
              "      <td>2.517447</td>\n",
              "      <td>0.260572</td>\n",
              "      <td>1.662224</td>\n",
              "      <td>0.288742</td>\n",
              "      <td>00:10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>142</td>\n",
              "      <td>2.526648</td>\n",
              "      <td>0.267886</td>\n",
              "      <td>1.659976</td>\n",
              "      <td>0.288742</td>\n",
              "      <td>00:10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>143</td>\n",
              "      <td>2.515364</td>\n",
              "      <td>0.259618</td>\n",
              "      <td>1.666970</td>\n",
              "      <td>0.287417</td>\n",
              "      <td>00:11</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>144</td>\n",
              "      <td>2.532896</td>\n",
              "      <td>0.259777</td>\n",
              "      <td>1.654251</td>\n",
              "      <td>0.289404</td>\n",
              "      <td>00:11</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>145</td>\n",
              "      <td>2.509865</td>\n",
              "      <td>0.260731</td>\n",
              "      <td>1.665223</td>\n",
              "      <td>0.288079</td>\n",
              "      <td>00:10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>146</td>\n",
              "      <td>2.507888</td>\n",
              "      <td>0.265819</td>\n",
              "      <td>1.657687</td>\n",
              "      <td>0.288079</td>\n",
              "      <td>00:11</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>147</td>\n",
              "      <td>2.514718</td>\n",
              "      <td>0.266455</td>\n",
              "      <td>1.668510</td>\n",
              "      <td>0.288079</td>\n",
              "      <td>00:10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>148</td>\n",
              "      <td>2.508273</td>\n",
              "      <td>0.268521</td>\n",
              "      <td>1.659834</td>\n",
              "      <td>0.288742</td>\n",
              "      <td>00:10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>149</td>\n",
              "      <td>2.526963</td>\n",
              "      <td>0.262321</td>\n",
              "      <td>1.663435</td>\n",
              "      <td>0.288742</td>\n",
              "      <td>00:10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>150</td>\n",
              "      <td>2.517608</td>\n",
              "      <td>0.264229</td>\n",
              "      <td>1.664691</td>\n",
              "      <td>0.288742</td>\n",
              "      <td>00:10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>151</td>\n",
              "      <td>2.512010</td>\n",
              "      <td>0.270429</td>\n",
              "      <td>1.658046</td>\n",
              "      <td>0.289404</td>\n",
              "      <td>00:10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>152</td>\n",
              "      <td>2.509561</td>\n",
              "      <td>0.268998</td>\n",
              "      <td>1.661815</td>\n",
              "      <td>0.288079</td>\n",
              "      <td>00:10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>153</td>\n",
              "      <td>2.517391</td>\n",
              "      <td>0.267727</td>\n",
              "      <td>1.664775</td>\n",
              "      <td>0.288079</td>\n",
              "      <td>00:10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>154</td>\n",
              "      <td>2.518022</td>\n",
              "      <td>0.265501</td>\n",
              "      <td>1.658894</td>\n",
              "      <td>0.289404</td>\n",
              "      <td>00:10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>155</td>\n",
              "      <td>2.529391</td>\n",
              "      <td>0.264070</td>\n",
              "      <td>1.660228</td>\n",
              "      <td>0.288742</td>\n",
              "      <td>00:10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>156</td>\n",
              "      <td>2.513080</td>\n",
              "      <td>0.267568</td>\n",
              "      <td>1.657372</td>\n",
              "      <td>0.288742</td>\n",
              "      <td>00:10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>157</td>\n",
              "      <td>2.511083</td>\n",
              "      <td>0.264229</td>\n",
              "      <td>1.665906</td>\n",
              "      <td>0.288079</td>\n",
              "      <td>00:10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>158</td>\n",
              "      <td>2.506693</td>\n",
              "      <td>0.264388</td>\n",
              "      <td>1.665932</td>\n",
              "      <td>0.288742</td>\n",
              "      <td>00:10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>159</td>\n",
              "      <td>2.521792</td>\n",
              "      <td>0.264388</td>\n",
              "      <td>1.656218</td>\n",
              "      <td>0.288742</td>\n",
              "      <td>00:10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>160</td>\n",
              "      <td>2.509692</td>\n",
              "      <td>0.268998</td>\n",
              "      <td>1.662009</td>\n",
              "      <td>0.288079</td>\n",
              "      <td>00:10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>161</td>\n",
              "      <td>2.514437</td>\n",
              "      <td>0.269952</td>\n",
              "      <td>1.661128</td>\n",
              "      <td>0.288742</td>\n",
              "      <td>00:10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>162</td>\n",
              "      <td>2.521451</td>\n",
              "      <td>0.262162</td>\n",
              "      <td>1.656588</td>\n",
              "      <td>0.289404</td>\n",
              "      <td>00:10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>163</td>\n",
              "      <td>2.523498</td>\n",
              "      <td>0.273132</td>\n",
              "      <td>1.663218</td>\n",
              "      <td>0.288742</td>\n",
              "      <td>00:10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>164</td>\n",
              "      <td>2.514954</td>\n",
              "      <td>0.267727</td>\n",
              "      <td>1.663898</td>\n",
              "      <td>0.288079</td>\n",
              "      <td>00:10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>165</td>\n",
              "      <td>2.526796</td>\n",
              "      <td>0.265501</td>\n",
              "      <td>1.662496</td>\n",
              "      <td>0.288079</td>\n",
              "      <td>00:10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>166</td>\n",
              "      <td>2.509957</td>\n",
              "      <td>0.266932</td>\n",
              "      <td>1.661697</td>\n",
              "      <td>0.288079</td>\n",
              "      <td>00:11</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>167</td>\n",
              "      <td>2.505841</td>\n",
              "      <td>0.264706</td>\n",
              "      <td>1.658005</td>\n",
              "      <td>0.289404</td>\n",
              "      <td>00:10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>168</td>\n",
              "      <td>2.514642</td>\n",
              "      <td>0.262162</td>\n",
              "      <td>1.655491</td>\n",
              "      <td>0.289404</td>\n",
              "      <td>00:10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>169</td>\n",
              "      <td>2.519937</td>\n",
              "      <td>0.267091</td>\n",
              "      <td>1.657329</td>\n",
              "      <td>0.289404</td>\n",
              "      <td>00:10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>170</td>\n",
              "      <td>2.521161</td>\n",
              "      <td>0.266614</td>\n",
              "      <td>1.660817</td>\n",
              "      <td>0.288079</td>\n",
              "      <td>00:10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>171</td>\n",
              "      <td>2.517984</td>\n",
              "      <td>0.263275</td>\n",
              "      <td>1.659261</td>\n",
              "      <td>0.288742</td>\n",
              "      <td>00:10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>172</td>\n",
              "      <td>2.519065</td>\n",
              "      <td>0.266137</td>\n",
              "      <td>1.656139</td>\n",
              "      <td>0.289404</td>\n",
              "      <td>00:11</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>173</td>\n",
              "      <td>2.509544</td>\n",
              "      <td>0.267568</td>\n",
              "      <td>1.664298</td>\n",
              "      <td>0.288079</td>\n",
              "      <td>00:11</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>174</td>\n",
              "      <td>2.491509</td>\n",
              "      <td>0.262321</td>\n",
              "      <td>1.659167</td>\n",
              "      <td>0.289404</td>\n",
              "      <td>00:10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>175</td>\n",
              "      <td>2.524908</td>\n",
              "      <td>0.264070</td>\n",
              "      <td>1.659525</td>\n",
              "      <td>0.289404</td>\n",
              "      <td>00:10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>176</td>\n",
              "      <td>2.508145</td>\n",
              "      <td>0.268839</td>\n",
              "      <td>1.665475</td>\n",
              "      <td>0.288079</td>\n",
              "      <td>00:10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>177</td>\n",
              "      <td>2.509598</td>\n",
              "      <td>0.267727</td>\n",
              "      <td>1.660258</td>\n",
              "      <td>0.288742</td>\n",
              "      <td>00:10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>178</td>\n",
              "      <td>2.512695</td>\n",
              "      <td>0.262639</td>\n",
              "      <td>1.665024</td>\n",
              "      <td>0.288079</td>\n",
              "      <td>00:10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>179</td>\n",
              "      <td>2.508386</td>\n",
              "      <td>0.265501</td>\n",
              "      <td>1.655310</td>\n",
              "      <td>0.288742</td>\n",
              "      <td>00:10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>180</td>\n",
              "      <td>2.517780</td>\n",
              "      <td>0.267409</td>\n",
              "      <td>1.663001</td>\n",
              "      <td>0.288079</td>\n",
              "      <td>00:10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>181</td>\n",
              "      <td>2.511879</td>\n",
              "      <td>0.261208</td>\n",
              "      <td>1.664047</td>\n",
              "      <td>0.288742</td>\n",
              "      <td>00:10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>182</td>\n",
              "      <td>2.524272</td>\n",
              "      <td>0.263752</td>\n",
              "      <td>1.655918</td>\n",
              "      <td>0.289404</td>\n",
              "      <td>00:10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>183</td>\n",
              "      <td>2.518824</td>\n",
              "      <td>0.266932</td>\n",
              "      <td>1.660233</td>\n",
              "      <td>0.289404</td>\n",
              "      <td>00:10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>184</td>\n",
              "      <td>2.499324</td>\n",
              "      <td>0.260890</td>\n",
              "      <td>1.660683</td>\n",
              "      <td>0.288742</td>\n",
              "      <td>00:10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>185</td>\n",
              "      <td>2.512591</td>\n",
              "      <td>0.263116</td>\n",
              "      <td>1.663445</td>\n",
              "      <td>0.286755</td>\n",
              "      <td>00:10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>186</td>\n",
              "      <td>2.534658</td>\n",
              "      <td>0.263593</td>\n",
              "      <td>1.665170</td>\n",
              "      <td>0.288079</td>\n",
              "      <td>00:10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>187</td>\n",
              "      <td>2.521369</td>\n",
              "      <td>0.264865</td>\n",
              "      <td>1.659593</td>\n",
              "      <td>0.289404</td>\n",
              "      <td>00:10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>188</td>\n",
              "      <td>2.515710</td>\n",
              "      <td>0.265501</td>\n",
              "      <td>1.658529</td>\n",
              "      <td>0.289404</td>\n",
              "      <td>00:10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>189</td>\n",
              "      <td>2.522372</td>\n",
              "      <td>0.266932</td>\n",
              "      <td>1.664018</td>\n",
              "      <td>0.288079</td>\n",
              "      <td>00:10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>190</td>\n",
              "      <td>2.528257</td>\n",
              "      <td>0.262798</td>\n",
              "      <td>1.664718</td>\n",
              "      <td>0.288742</td>\n",
              "      <td>00:10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>191</td>\n",
              "      <td>2.517947</td>\n",
              "      <td>0.264706</td>\n",
              "      <td>1.665424</td>\n",
              "      <td>0.288079</td>\n",
              "      <td>00:10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>192</td>\n",
              "      <td>2.510996</td>\n",
              "      <td>0.262639</td>\n",
              "      <td>1.661898</td>\n",
              "      <td>0.288742</td>\n",
              "      <td>00:10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>193</td>\n",
              "      <td>2.517322</td>\n",
              "      <td>0.265024</td>\n",
              "      <td>1.662400</td>\n",
              "      <td>0.288079</td>\n",
              "      <td>00:10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>194</td>\n",
              "      <td>2.515412</td>\n",
              "      <td>0.267886</td>\n",
              "      <td>1.661565</td>\n",
              "      <td>0.288079</td>\n",
              "      <td>00:10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>195</td>\n",
              "      <td>2.528183</td>\n",
              "      <td>0.259142</td>\n",
              "      <td>1.658366</td>\n",
              "      <td>0.289404</td>\n",
              "      <td>00:10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>196</td>\n",
              "      <td>2.509649</td>\n",
              "      <td>0.262639</td>\n",
              "      <td>1.667571</td>\n",
              "      <td>0.287417</td>\n",
              "      <td>00:10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>197</td>\n",
              "      <td>2.516777</td>\n",
              "      <td>0.262798</td>\n",
              "      <td>1.654409</td>\n",
              "      <td>0.288742</td>\n",
              "      <td>00:10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>198</td>\n",
              "      <td>2.501566</td>\n",
              "      <td>0.266932</td>\n",
              "      <td>1.656442</td>\n",
              "      <td>0.288742</td>\n",
              "      <td>00:10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>199</td>\n",
              "      <td>2.523338</td>\n",
              "      <td>0.263434</td>\n",
              "      <td>1.662233</td>\n",
              "      <td>0.288742</td>\n",
              "      <td>00:10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>200</td>\n",
              "      <td>2.519234</td>\n",
              "      <td>0.265819</td>\n",
              "      <td>1.661890</td>\n",
              "      <td>0.288742</td>\n",
              "      <td>00:10</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-257-eb0fe6ae3a76>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mlearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m300\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcbs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcbscheds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/content/course-v3/nbs/dl2/exp/nb_09b.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, epochs, cbs, reset_opt)\u001b[0m\n\u001b[1;32m     70\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdo_begin_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 72\u001b[0;31m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'begin_epoch'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mall_batches\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     73\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/course-v3/nbs/dl2/exp/nb_09b.py\u001b[0m in \u001b[0;36mall_batches\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     49\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miters\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxb\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0myb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mone_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mxb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0myb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mCancelEpochException\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'after_cancel_epoch'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    519\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sampler_iter\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    520\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 521\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    522\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    523\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    559\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    560\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 561\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    562\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    563\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/course-v3/nbs/dl2/exp/nb_08.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m    139\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__repr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mreturn\u001b[0m \u001b[0;34mf'{self.__class__.__name__}\\nx: {self.x}\\ny: {self.y}\\n'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 141\u001b[0;31m     \u001b[0;32mdef\u001b[0m \u001b[0m__getitem__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    142\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mreturn\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/course-v3/nbs/dl2/exp/nb_08.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     57\u001b[0m         \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getitem__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mres\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mo\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mres\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     60\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mImageList\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mItemList\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/course-v3/nbs/dl2/exp/nb_08.py\u001b[0m in \u001b[0;36m_get\u001b[0;34m(self, i)\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m     \u001b[0;32mdef\u001b[0m  \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mreturn\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m     \u001b[0;32mdef\u001b[0m \u001b[0m_get\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mreturn\u001b[0m \u001b[0mcompose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtfms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     55\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__getitem__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-12-65029f409554>\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, **pickle_load_args)\u001b[0m\n\u001b[1;32m    593\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    594\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0m_open_file_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mopened_file\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 595\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0m_is_zipfile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopened_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    596\u001b[0m             \u001b[0;31m# The zipfile reader is going to advance the current file position.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    597\u001b[0m             \u001b[0;31m# If we want to actually tail call to torch.jit.load, we need to\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36m_is_zipfile\u001b[0;34m(f)\u001b[0m\n\u001b[1;32m     55\u001b[0m     \u001b[0mstart\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtell\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 57\u001b[0;31m     \u001b[0mbyte\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     58\u001b[0m     \u001b[0;32mwhile\u001b[0m \u001b[0mbyte\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m\"\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m         \u001b[0mread_bytes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbyte\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -qq torchinfo"
      ],
      "metadata": {
        "id": "0qhi0BeDZSO8"
      },
      "execution_count": 192,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torchinfo import summary\n",
        "summary(Model(4).cuda(), (bs, 1, 26, 150))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xtNHl9YTXEE8",
        "outputId": "ac6e3284-df80-496a-f9f4-6448aaadc0e1"
      },
      "execution_count": 260,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "===============================================================================================\n",
              "Layer (type:depth-idx)                        Output Shape              Param #\n",
              "===============================================================================================\n",
              "Model                                         --                        --\n",
              "├─TransformerEncoder: 1                       --                        --\n",
              "│    └─ModuleList: 2-1                        --                        --\n",
              "├─Sequential: 1-1                             [64, 64, 1, 4]            --\n",
              "│    └─Conv2d: 2-2                            [64, 16, 26, 150]         160\n",
              "│    └─BatchNorm2d: 2-3                       [64, 16, 26, 150]         32\n",
              "│    └─ReLU: 2-4                              [64, 16, 26, 150]         --\n",
              "│    └─MaxPool2d: 2-5                         [64, 16, 13, 75]          --\n",
              "│    └─Dropout: 2-6                           [64, 16, 13, 75]          --\n",
              "│    └─Conv2d: 2-7                            [64, 32, 13, 75]          4,640\n",
              "│    └─BatchNorm2d: 2-8                       [64, 32, 13, 75]          64\n",
              "│    └─ReLU: 2-9                              [64, 32, 13, 75]          --\n",
              "│    └─MaxPool2d: 2-10                        [64, 32, 3, 18]           --\n",
              "│    └─Dropout: 2-11                          [64, 32, 3, 18]           --\n",
              "│    └─Conv2d: 2-12                           [64, 64, 3, 18]           18,496\n",
              "│    └─BatchNorm2d: 2-13                      [64, 64, 3, 18]           128\n",
              "│    └─ReLU: 2-14                             [64, 64, 3, 18]           --\n",
              "│    └─AdaptiveMaxPool2d: 2-15                [64, 64, 1, 4]            --\n",
              "│    └─Dropout: 2-16                          [64, 64, 1, 4]            --\n",
              "├─Sequential: 1-2                             [64, 64, 1, 4]            --\n",
              "│    └─Conv2d: 2-17                           [64, 16, 26, 150]         160\n",
              "│    └─BatchNorm2d: 2-18                      [64, 16, 26, 150]         32\n",
              "│    └─ReLU: 2-19                             [64, 16, 26, 150]         --\n",
              "│    └─MaxPool2d: 2-20                        [64, 16, 13, 75]          --\n",
              "│    └─Dropout: 2-21                          [64, 16, 13, 75]          --\n",
              "│    └─Conv2d: 2-22                           [64, 32, 13, 75]          4,640\n",
              "│    └─BatchNorm2d: 2-23                      [64, 32, 13, 75]          64\n",
              "│    └─ReLU: 2-24                             [64, 32, 13, 75]          --\n",
              "│    └─MaxPool2d: 2-25                        [64, 32, 3, 18]           --\n",
              "│    └─Dropout: 2-26                          [64, 32, 3, 18]           --\n",
              "│    └─Conv2d: 2-27                           [64, 64, 3, 18]           18,496\n",
              "│    └─BatchNorm2d: 2-28                      [64, 64, 3, 18]           128\n",
              "│    └─ReLU: 2-29                             [64, 64, 3, 18]           --\n",
              "│    └─AdaptiveMaxPool2d: 2-30                [64, 64, 1, 4]            --\n",
              "│    └─Dropout: 2-31                          [64, 64, 1, 4]            --\n",
              "├─MaxPool2d: 1-3                              [64, 1, 26, 37]           --\n",
              "├─TransformerEncoder: 1-4                     [37, 64, 26]              --\n",
              "│    └─ModuleList: 2-1                        --                        --\n",
              "│    │    └─TransformerEncoderLayer: 3-1      [37, 64, 26]              16,506\n",
              "│    │    └─TransformerEncoderLayer: 3-2      [37, 64, 26]              16,506\n",
              "│    │    └─TransformerEncoderLayer: 3-3      [37, 64, 26]              16,506\n",
              "│    │    └─TransformerEncoderLayer: 3-4      [37, 64, 26]              16,506\n",
              "├─Linear: 1-5                                 [64, 4]                   2,156\n",
              "===============================================================================================\n",
              "Total params: 103,988\n",
              "Trainable params: 103,988\n",
              "Non-trainable params: 0\n",
              "Total mult-adds (M): 788.98\n",
              "===============================================================================================\n",
              "Input size (MB): 1.00\n",
              "Forward/backward pass size (MB): 224.08\n",
              "Params size (MB): 0.42\n",
              "Estimated Total Size (MB): 225.50\n",
              "==============================================================================================="
            ]
          },
          "metadata": {},
          "execution_count": 260
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Save model for inference"
      ],
      "metadata": {
        "id": "2os3jtXZYAYi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GiRbgHeeYEzK",
        "outputId": "9fe849ba-fcd7-48bd-ca59-173b14e76a6c"
      },
      "execution_count": 185,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_path = Path('/content/model/v9'); model_path.mkdir(parents = True, exist_ok=True)"
      ],
      "metadata": {
        "id": "UQOJenoaYKHu"
      },
      "execution_count": 258,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.save(learn.model.state_dict(), model_path/'ser-v9-transformer-cnn-mh-2.pt')"
      ],
      "metadata": {
        "id": "W2iHDYlGYB5e"
      },
      "execution_count": 259,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!tar -czf model-v5-8.tar.gz model"
      ],
      "metadata": {
        "id": "X6FMLR-KskN4"
      },
      "execution_count": 264,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# References"
      ],
      "metadata": {
        "id": "a21jVDRDKvgK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- [Pytorch Transformer Encoder](https://pytorch.org/docs/stable/generated/torch.nn.TransformerEncoderLayer.html)\n",
        "- [fastai course-v3 part2, audio](https://github.com/fastai/course-v3/blob/master/nbs/dl2/audio.ipynb)\n",
        "- [Transformer-CNN-emotion recognition](https://github.com/IliaZenkov/transformer-cnn-emotion-recognition/blob/main/notebooks/Parallel_is_All_You_Want.ipynb)\n",
        "- \n"
      ],
      "metadata": {
        "id": "KMp_Qul3KwyF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "tmsc8x7wKw2F"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}