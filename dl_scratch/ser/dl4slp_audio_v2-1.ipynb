{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "dl4slp-audio-v5.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "##  Let me try [Parrelle is all you want](https://github.com/IliaZenkov/transformer-cnn-emotion-recognition/blob/main/notebooks/Parallel_is_All_You_Want.ipynb), but only adaopting its model.\n",
        " 20 Oct 2020"
      ],
      "metadata": {
        "id": "bYMZuk-QNKGl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/fastai/course-v3/ /content/course-v3\n",
        "%cd /content/course-v3/nbs/dl2"
      ],
      "metadata": {
        "id": "-p-B9N_qsun4",
        "outputId": "768e92bf-9739-45a0-a26a-936e561b7591",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into '/content/course-v3'...\n",
            "remote: Enumerating objects: 5893, done.\u001b[K\n",
            "remote: Counting objects: 100% (3/3), done.\u001b[K\n",
            "remote: Compressing objects: 100% (3/3), done.\u001b[K\n",
            "remote: Total 5893 (delta 0), reused 2 (delta 0), pack-reused 5890\u001b[K\n",
            "Receiving objects: 100% (5893/5893), 263.03 MiB | 33.21 MiB/s, done.\n",
            "Resolving deltas: 100% (3249/3249), done.\n",
            "/content/course-v3/nbs/dl2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- NOTE: if you do not have installed [nvidia/apex](https://github.com/nvidia/apex),  change `from exp.nb_10c import *` to `from exp.nb_10b import *` in `exp.nb_11`"
      ],
      "metadata": {
        "id": "nb6S4CPyqS_F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from exp.nb_12a import *"
      ],
      "metadata": {
        "id": "77V-bw0pqR2J"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Local data (which is divided into separate dataset) is saved to `Path('/gdrive/Shareddrives/Dion-Account/2122WS/8-dl4slp/coding-project/data/v1/ser'`\n"
      ],
      "metadata": {
        "id": "_U7CK3KXqnr5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/gdrive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VL3HjY7zp5jZ",
        "outputId": "251560f6-f346-490f-9e86-2aa59346a692"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /gdrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "root_path = Path('/gdrive/Shareddrives/Dion-Account/2122WS/8-dl4slp/coding-project/data/ser/v1'); root_path.ls()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V93zJPFhqnpC",
        "outputId": "49ffba4a-4106-4922-ff74-df7d88d5d578"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[PosixPath('/gdrive/Shareddrives/Dion-Account/2122WS/8-dl4slp/coding-project/data/ser/v1/train'),\n",
              " PosixPath('/gdrive/Shareddrives/Dion-Account/2122WS/8-dl4slp/coding-project/data/ser/v1/dev')]"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D1FVouZFT7-5"
      },
      "source": [
        "features-entry contains a list of a list with 26 items.\n",
        "- length of inner list: 26 (float numbers - represent one preprocessed speech frame (logMel))\n",
        "\n",
        "- length of outer list: number of frames per data-point, e.g. 10 or 15, ..."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_path = root_path/'train'"
      ],
      "metadata": {
        "id": "XxRfMtm7UTb_"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "audios = get_files(train_path)"
      ],
      "metadata": {
        "id": "vR7U0z1HmgoR"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# restart-run\n",
        "class AudioList(ItemList):\n",
        "    @classmethod\n",
        "    def from_files(cls, path, extensions = None, recurse=True, include=None, **kwargs):\n",
        "        return cls(get_files(path, extensions, recurse=recurse, include=include), path, **kwargs)\n",
        "    \n",
        "    def get(self, fn):\n",
        "        return torch.load(fn)"
      ],
      "metadata": {
        "id": "fPJ2emMUmglw"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "al=AudioList.from_files(train_path); al"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PSfrsOPbmgil",
        "outputId": "67057bd1-38c2-4293-a7ed-233cd64f2c96"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AudioList (7800 items)\n",
              "[PosixPath('/gdrive/Shareddrives/Dion-Account/2122WS/8-dl4slp/coding-project/data/ser/v1/train/7760_1_1.pt'), PosixPath('/gdrive/Shareddrives/Dion-Account/2122WS/8-dl4slp/coding-project/data/ser/v1/train/7761_0_0.pt'), PosixPath('/gdrive/Shareddrives/Dion-Account/2122WS/8-dl4slp/coding-project/data/ser/v1/train/7762_0_0.pt'), PosixPath('/gdrive/Shareddrives/Dion-Account/2122WS/8-dl4slp/coding-project/data/ser/v1/train/7763_1_0.pt'), PosixPath('/gdrive/Shareddrives/Dion-Account/2122WS/8-dl4slp/coding-project/data/ser/v1/train/7764_1_1.pt'), PosixPath('/gdrive/Shareddrives/Dion-Account/2122WS/8-dl4slp/coding-project/data/ser/v1/train/7765_1_0.pt'), PosixPath('/gdrive/Shareddrives/Dion-Account/2122WS/8-dl4slp/coding-project/data/ser/v1/train/7766_1_1.pt'), PosixPath('/gdrive/Shareddrives/Dion-Account/2122WS/8-dl4slp/coding-project/data/ser/v1/train/7767_1_0.pt'), PosixPath('/gdrive/Shareddrives/Dion-Account/2122WS/8-dl4slp/coding-project/data/ser/v1/train/7768_1_0.pt'), PosixPath('/gdrive/Shareddrives/Dion-Account/2122WS/8-dl4slp/coding-project/data/ser/v1/train/7769_1_1.pt')...]\n",
              "Path: /gdrive/Shareddrives/Dion-Account/2122WS/8-dl4slp/coding-project/data/ser/v1/train"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Use adaptive pooling to fit the frames"
      ],
      "metadata": {
        "id": "4lnGePmPwoUL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## input transform\n",
        "\n",
        "1. load tensor\n",
        "2. tocuda\n",
        "3. transpose, add dimension to first axis\n",
        "4. AdaptiveAvgPooling to fit width(time frames) 250\n"
      ],
      "metadata": {
        "id": "-VsPUxouyOcu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ToCuda():\n",
        "    _order=10\n",
        "    def __call__(self, ad):\n",
        "        return ad.cuda()"
      ],
      "metadata": {
        "id": "w-bI6BPNzmde"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Reshape():\n",
        "    _order=11\n",
        "    def __call__(self, item):\n",
        "        w, h = item.shape\n",
        "        return item.view(h, w)"
      ],
      "metadata": {
        "id": "4PjKjkXtzoQQ"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class DummyFeature():\n",
        "    # add features from function\n",
        "    _order= 13\n",
        "    def __init__(self, f, **kwargs):\n",
        "        self.p = partial(f, **kwargs)\n",
        "    def __call__(self, item):\n",
        "        return torch.vstack((item, self.p(item).unsqueeze(0)))"
      ],
      "metadata": {
        "id": "QRDT39VjbW16"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Mutants of input tensor\n",
        "class PadorTrim():\n",
        "    _order = 20\n",
        "    def __init__(self, max_len):\n",
        "        self.max_len = max_len\n",
        "    def __call__(self, ad):\n",
        "        # h - logmel, here 27, w - frames / various\n",
        "        h, w = ad.shape\n",
        "        pad_size = self.max_len - w\n",
        "        if pad_size >0: return torch.cat((ad, torch.zeros(h, pad_size).cuda()), dim=1)\n",
        "        else: return ad[:, :self.max_len]\n",
        "\n",
        "class TimeFramePooling():\n",
        "    _order= 12\n",
        "    def __init__(self, f, n_tf):\n",
        "        # n_tf: numner of time frames\n",
        "        self.p = f(n_tf)\n",
        "    def __call__(self, item):\n",
        "        return self.p(item)\n",
        "\n",
        "# TimeFramePooling(f= nn.AdaptiveMaxPool1d, n_tf = 150)\n",
        "\n",
        "class DummyChannel():\n",
        "    _order = 30\n",
        "    def __call__(self, item):\n",
        "        return item.unsqueeze(0)"
      ],
      "metadata": {
        "id": "OtJ4_itDz_s8"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tfms = [ToCuda(), Reshape(), DummyFeature(torch.mean, dim=0), PadorTrim(300), DummyChannel()]\n",
        "compose(al[0], tfms).shape"
      ],
      "metadata": {
        "id": "1EalvurFcVfU",
        "outputId": "b90b3353-0970-4393-a002-c43138b235c2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([1, 27, 300])"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "il = AudioList.from_files(train_path, tfms = tfms)"
      ],
      "metadata": {
        "id": "cVNZY7nn0uOM"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- This scaling process is annoying, because I should have this process 'before' training, while separating train and eval status"
      ],
      "metadata": {
        "id": "CCXGUEseXShG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "il[0].shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FCw0tHmT1wPq",
        "outputId": "cda51166-3a21-4e89-a1da-5fde119fb5b5"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([1, 27, 300])"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def re_labeler(fn, pat, subcl='act'):\n",
        "    assert subcl in ['act', 'val', 'all']\n",
        "    if subcl=='all': return ''.join(re.findall(pat, str(fn)))\n",
        "    else:\n",
        "        return re.findall(pat, str(fn))[0] if pat == 'act' else re.findall(pat, str(fn))[1]\n",
        "\n",
        "label_pat = r'_(\\d+)'\n",
        "emotion_labeler = partial(re_labeler, pat=label_pat, subcl='all')"
      ],
      "metadata": {
        "id": "kL9IVX4uzeOT"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sd = SplitData.split_by_func(il, partial(random_splitter, p_valid=0.2))\n",
        "ll = label_by_func(sd, emotion_labeler, proc_y=CategoryProcessor())"
      ],
      "metadata": {
        "id": "IqzVzlce1vOO"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bs=64\n",
        "c_in = ll.train[0][0].shape[0]\n",
        "c_out = len(uniqueify(ll.train.y))\n",
        "data = ll.to_databunch(bs,c_in=c_in,c_out=c_out)"
      ],
      "metadata": {
        "id": "XPMz25enABaj"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "c_in, c_out"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jjkO31kEAHeJ",
        "outputId": "4a00ba1e-2b1c-4cfa-cf41-58aafc19becd"
      },
      "execution_count": 149,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1, 4)"
            ]
          },
          "metadata": {},
          "execution_count": 149
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x, y= ll.train[0]"
      ],
      "metadata": {
        "id": "3D7EF4Mg2GpI"
      },
      "execution_count": 150,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x.shape, y"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uxARykjF2Izl",
        "outputId": "8983919a-ed6f-458f-966b-3a086143abc0"
      },
      "execution_count": 151,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(torch.Size([1, 27, 300]), 0)"
            ]
          },
          "metadata": {},
          "execution_count": 151
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "xb, yb = next(iter(data.train_dl))"
      ],
      "metadata": {
        "id": "F-8QI3L_iJqO"
      },
      "execution_count": 152,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "xb.shape, yb.shape"
      ],
      "metadata": {
        "id": "ijort6NdjzwP",
        "outputId": "1f3138f0-7b93-4a7f-81bd-6574b4db8480",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 153,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(torch.Size([64, 1, 27, 300]), torch.Size([64]))"
            ]
          },
          "metadata": {},
          "execution_count": 153
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_train = torch.cat(tuple(data.train_ds.x[i] for i in range(len(data.train_ds))), dim=0).unsqueeze(1).cuda()\n",
        "X_valid = torch.cat(tuple(data.valid_ds.x[i] for i in range(len(data.valid_ds))), dim=0).unsqueeze(1).cuda()\n",
        "y_train = torch.tensor(data.train_ds.y.items)\n",
        "y_valid = torch.tensor(data.valid_ds.y.items)\n",
        "X_train.shape, X_valid.shape"
      ],
      "metadata": {
        "id": "X_-4i0cfwALq",
        "outputId": "842f302b-d62d-4a98-9c8e-058831ae4bc3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(torch.Size([6176, 1, 27, 300]), torch.Size([1624, 1, 27, 300]))"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model"
      ],
      "metadata": {
        "id": "37S5tn75yN6j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/gdrive')"
      ],
      "metadata": {
        "id": "tvrfCpcQ5EcM",
        "outputId": "5c9746fe-aa30-4519-f498-d977f6a2d3fe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /gdrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os, glob\n",
        "import IPython\n",
        "from IPython.display import Audio\n",
        "from IPython.display import Image"
      ],
      "metadata": {
        "id": "oyuWR_y2z-CP"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pathlib import Path\n",
        "Path.ls = lambda x: list(x.iterdir())"
      ],
      "metadata": {
        "id": "P2D6w11w3Qrw"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "root_path = Path('/gdrive/Shareddrives/Dion-Account/2122WS/8-dl4slp/coding-project/data/ser'); root_path.ls()"
      ],
      "metadata": {
        "id": "o0-Np_X-3K1B",
        "outputId": "b1d7e610-90e6-46fa-87fb-b259f78f7e78",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[PosixPath('/gdrive/Shareddrives/Dion-Account/2122WS/8-dl4slp/coding-project/data/ser/v0'),\n",
              " PosixPath('/gdrive/Shareddrives/Dion-Account/2122WS/8-dl4slp/coding-project/data/ser/v1'),\n",
              " PosixPath('/gdrive/Shareddrives/Dion-Account/2122WS/8-dl4slp/coding-project/data/ser/v2.1'),\n",
              " PosixPath('/gdrive/Shareddrives/Dion-Account/2122WS/8-dl4slp/coding-project/data/ser/ser-data-v2-1.tar.gz')]"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!cp {root_path/'ser-data-v2-1.tar.gz'} ."
      ],
      "metadata": {
        "id": "oc6Rx1SA3MS2"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!tar -xzf ser-data-v2-1.tar.gz"
      ],
      "metadata": {
        "id": "XwPxNj2B3bAV"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_path = Path('/content/v2.1')"
      ],
      "metadata": {
        "id": "TzhrIs0Vysnz"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_data(fpath):\n",
        "    return [torch.load(i) for i in fpath.iterdir() if i.suffix=='.pt']"
      ],
      "metadata": {
        "id": "42rlZ_pZ3t7m"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, y_train, y_valid, X_valid = get_data(data_path)"
      ],
      "metadata": {
        "id": "Uxfk28cR4JR-"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "list(map(lambda x: print(f\"{x.shape}\"), (X_train, y_train, X_valid, y_valid)))"
      ],
      "metadata": {
        "id": "-_LwnhJp69Zv",
        "outputId": "c8c88162-8d22-4e1c-dc43-8635fb26ca6a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([6176, 1, 27, 300])\n",
            "torch.Size([6176])\n",
            "torch.Size([1624, 1, 27, 300])\n",
            "torch.Size([1624])\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[None, None, None, None]"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_train.shape, X_train.type(), X_train.device"
      ],
      "metadata": {
        "id": "H6l6mDMf1Z4o",
        "outputId": "97e1adff-3e4a-4370-d7f3-73164d5be022",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(torch.Size([6176, 1, 27, 300]), 'torch.FloatTensor', device(type='cpu'))"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title\n",
        "model_demo = nn.Sequential(\n",
        "    \n",
        " nn.Sequential(   nn.Conv2d(\n",
        "        in_channels=1, # input volume depth == input channel dim == 1\n",
        "        out_channels=16, # expand output feature map volume's depth to 16\n",
        "        kernel_size=3, # typical 3*3 stride 1 kernel\n",
        "        stride=1,\n",
        "        padding=1\n",
        "                ),\n",
        "    nn.BatchNorm2d(16), # batch normalize the output feature map before activation\n",
        "    nn.ReLU(), # feature map --> activation map\n",
        "    nn.MaxPool2d(kernel_size=2, stride=2), #typical maxpool kernel size\n",
        "    nn.Dropout(p=0.3), #randomly zero 30% of 1st layer's output feature map in training\n",
        "    ),\n",
        "    # 2nd 2D convolution layer identical to last except output dim, maxpool kernel\n",
        "nn.Sequential(    nn.Conv2d(\n",
        "        in_channels=16, \n",
        "        out_channels=32, # expand output feature map volume's depth to 32\n",
        "        kernel_size=3,\n",
        "        stride=1,\n",
        "        padding=1\n",
        "                ),\n",
        "    nn.BatchNorm2d(32),\n",
        "    nn.ReLU(),\n",
        "    nn.MaxPool2d(kernel_size=4, stride=4), # increase maxpool kernel for subsequent filters\n",
        "    nn.Dropout(p=0.3)),\n",
        "    \n",
        "    # 3rd 2D convolution layer identical to last except output dim\n",
        "nn.Sequential(    nn.Conv2d(\n",
        "        in_channels=32,\n",
        "        out_channels=64, # expand output feature map volume's depth to 64\n",
        "        kernel_size=3,\n",
        "        stride=1,\n",
        "        padding=1\n",
        "                ),\n",
        "    nn.BatchNorm2d(64),\n",
        "    nn.ReLU(),\n",
        "    nn.AdaptiveMaxPool2d((1,4)),\n",
        "    nn.Dropout(p=0.3),)\n",
        "    )"
      ],
      "metadata": {
        "cellView": "form",
        "id": "tobyTP6O4W2b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class parallel_all_you_want(nn.Module):\n",
        "    # Define all layers present in the network\n",
        "    def __init__(self,num_emotions):\n",
        "        super().__init__() \n",
        "        self.transformer_maxpool = nn.MaxPool2d(kernel_size=[1,4], stride=[1,4])\n",
        "        transformer_layer = nn.TransformerEncoderLayer(\n",
        "            d_model=27, # input feature (frequency) dim after maxpooling 26*250 -> 26*62 (MFC*time)\n",
        "            nhead=3, # 4 self-attention layers in each multi-head self-attention layer in each encoder block\n",
        "            dim_feedforward=256, # 2 linear layers in each encoder block's feedforward network: dim 26-->512--->26\n",
        "            dropout=0.4, \n",
        "            activation='gelu' # ReLU: avoid saturation/tame gradient/reduce compute time\n",
        "        )\n",
        "        self.transformer_encoder = nn.TransformerEncoder(transformer_layer, num_layers=3)\n",
        "        self.conv2Dblock1 = nn.Sequential(\n",
        "            \n",
        "            # 1st 2D convolution layer\n",
        "            nn.Conv2d(\n",
        "                in_channels=1, # input volume depth == input channel dim == 1\n",
        "                out_channels=16, # expand output feature map volume's depth to 16\n",
        "                kernel_size=3, # typical 3*3 stride 1 kernel\n",
        "                stride=1,\n",
        "                padding=1\n",
        "                      ),\n",
        "            nn.BatchNorm2d(16), # batch normalize the output feature map before activation\n",
        "            nn.ReLU(), # feature map --> activation map\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2), #typical maxpool kernel size\n",
        "            nn.Dropout(p=0.3), #randomly zero 30% of 1st layer's output feature map in training\n",
        "            \n",
        "            # 2nd 2D convolution layer identical to last except output dim, maxpool kernel\n",
        "            nn.Conv2d(\n",
        "                in_channels=16, \n",
        "                out_channels=32, # expand output feature map volume's depth to 32\n",
        "                kernel_size=3,\n",
        "                stride=1,\n",
        "                padding=1\n",
        "                      ),\n",
        "            nn.BatchNorm2d(32),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=4, stride=4), # increase maxpool kernel for subsequent filters\n",
        "            nn.Dropout(p=0.3), \n",
        "            \n",
        "            # 3rd 2D convolution layer identical to last except output dim\n",
        "            nn.Conv2d(\n",
        "                in_channels=32,\n",
        "                out_channels=64, # expand output feature map volume's depth to 64\n",
        "                kernel_size=3,\n",
        "                stride=1,\n",
        "                padding=1\n",
        "                      ),\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.ReLU(),\n",
        "            nn.AdaptiveMaxPool2d((1,4)),\n",
        "            nn.Dropout(p=0.3),\n",
        "        )\n",
        "        self.conv2Dblock2 = nn.Sequential(\n",
        "            \n",
        "            # 1st 2D convolution layer\n",
        "            nn.Conv2d(\n",
        "                in_channels=1, # input volume depth == input channel dim == 1\n",
        "                out_channels=16, # expand output feature map volume's depth to 16\n",
        "                kernel_size=3, # typical 3*3 stride 1 kernel\n",
        "                stride=1,\n",
        "                padding=1\n",
        "                      ),\n",
        "            nn.BatchNorm2d(16), # batch normalize the output feature map before activation\n",
        "            nn.ReLU(), # feature map --> activation map\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2), #typical maxpool kernel size\n",
        "            nn.Dropout(p=0.3), #randomly zero 30% of 1st layer's output feature map in training\n",
        "            \n",
        "            # 2nd 2D convolution layer identical to last except output dim, maxpool kernel\n",
        "            nn.Conv2d(\n",
        "                in_channels=16, \n",
        "                out_channels=32, # expand output feature map volume's depth to 32\n",
        "                kernel_size=3,\n",
        "                stride=1,\n",
        "                padding=1\n",
        "                      ),\n",
        "            nn.BatchNorm2d(32),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=4, stride=4), # increase maxpool kernel for subsequent filters\n",
        "            nn.Dropout(p=0.3), \n",
        "            \n",
        "            # 3rd 2D convolution layer identical to last except output dim\n",
        "            nn.Conv2d(\n",
        "                in_channels=32,\n",
        "                out_channels=64, # expand output feature map volume's depth to 64\n",
        "                kernel_size=3,\n",
        "                stride=1,\n",
        "                padding=1\n",
        "                      ),\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.ReLU(),\n",
        "            nn.AdaptiveMaxPool2d((1,4)),\n",
        "            nn.Dropout(p=0.3),\n",
        "        )\n",
        "        self.fc1_linear = nn.Linear(256*2+27, num_emotions)\n",
        "        self.softmax_out = nn.Softmax(dim=1) # dim==1 is the freq embedding\n",
        "    def forward(self,x):\n",
        "        conv2d_embedding1 = self.conv2Dblock1(x)\n",
        "        conv2d_embedding1 = torch.flatten(conv2d_embedding1, start_dim=1) \n",
        "        conv2d_embedding2 = self.conv2Dblock2(x)\n",
        "        conv2d_embedding2 = torch.flatten(conv2d_embedding2, start_dim=1)\n",
        "        x_maxpool = self.transformer_maxpool(x)\n",
        "        x_maxpool_reduced = torch.squeeze(x_maxpool,1)\n",
        "        x = x_maxpool_reduced.permute(2,0,1) \n",
        "        transformer_output = self.transformer_encoder(x)\n",
        "        transformer_embedding = torch.mean(transformer_output, dim=0) # dim 27x75 --> 27\n",
        "        complete_embedding = torch.cat([conv2d_embedding1, conv2d_embedding2,transformer_embedding], dim=1)\n",
        "        output_logits = self.fc1_linear(complete_embedding)\n",
        "        output_softmax = self.softmax_out(output_logits)\n",
        "        \n",
        "        return output_logits, output_softmax"
      ],
      "metadata": {
        "id": "7Wd4KI6Url1J"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def criterion(predictions, targets): \n",
        "    return nn.CrossEntropyLoss()(input=predictions, target=targets)"
      ],
      "metadata": {
        "id": "Xnpx800Qp9Fb"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def make_train_step(model, criterion, optimizer):\n",
        "    \n",
        "    # define the training step of the training phase\n",
        "    def train_step(X,Y):\n",
        "        \n",
        "        # forward pass\n",
        "        output_logits, output_softmax = model(X)\n",
        "        fscore = f1(output_softmax.argmax(dim=1).cpu(), Y.cpu())\n",
        "        \n",
        "        # compute loss on logits because nn.CrossEntropyLoss implements log softmax\n",
        "        loss = criterion(output_logits, Y) \n",
        "        \n",
        "        # compute gradients for the optimizer to use \n",
        "        loss.backward()\n",
        "        \n",
        "        # update network parameters based on gradient stored (by calling loss.backward())\n",
        "        optimizer.step()\n",
        "        \n",
        "        # zero out gradients for next pass\n",
        "        # pytorch accumulates gradients from backwards passes (convenient for RNNs)\n",
        "        optimizer.zero_grad() \n",
        "        \n",
        "        return loss.item(), fscore\n",
        "    return train_step"
      ],
      "metadata": {
        "id": "5FU2i1wLp9C_"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def make_validate_fnc(model,criterion):\n",
        "    def validate(X,Y):\n",
        "        \n",
        "        # don't want to update any network parameters on validation passes: don't need gradient\n",
        "        # wrap in torch.no_grad to save memory and compute in validation phase: \n",
        "        with torch.no_grad(): \n",
        "            \n",
        "            # set model to validation phase i.e. turn off dropout and batchnorm layers \n",
        "            model.eval()\n",
        "      \n",
        "            # get the model's predictions on the validation set\n",
        "            # print(X.shape)\n",
        "            output_logits, output_softmax = model(X)\n",
        "            fscore = f1(output_softmax.argmax(dim=1).cpu(), Y.cpu())\n",
        "            \n",
        "            # compute error from logits (nn.crossentropy implements softmax)\n",
        "            loss = criterion(output_logits,Y)\n",
        "            \n",
        "        return loss.item(), fscore, output_softmax.argmax(dim=1)\n",
        "    return validate\n"
      ],
      "metadata": {
        "id": "maqfPACsqEZ9"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def make_save_checkpoint(): \n",
        "    def save_checkpoint(optimizer, model, epoch, filename):\n",
        "        checkpoint_dict = {\n",
        "            'optimizer': optimizer.state_dict(),\n",
        "            'model': model.state_dict(),\n",
        "            'epoch': epoch\n",
        "        }\n",
        "        torch.save(checkpoint_dict, filename)\n",
        "    return save_checkpoint"
      ],
      "metadata": {
        "id": "UUFjLhzTrhJX"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_checkpoint(optimizer, model, filename):\n",
        "    checkpoint_dict = torch.load(filename)\n",
        "    epoch = checkpoint_dict['epoch']\n",
        "    model.load_state_dict(checkpoint_dict['model'])\n",
        "    if optimizer is not None:\n",
        "        optimizer.load_state_dict(checkpoint_dict['optimizer'])\n",
        "    return epoch"
      ],
      "metadata": {
        "id": "na8INw7yrjiU"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# get training set size to calculate # iterations and minibatch indices\n",
        "train_size = X_train.shape[0]\n",
        "\n",
        "# pick minibatch size (of 32... always)\n",
        "minibatch = 32\n",
        "\n",
        "# set device to GPU\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "print(f'{device} selected')\n",
        "\n",
        "# instantiate model and move to GPU for training\n",
        "model = parallel_all_you_want(num_emotions=4).to(device) \n",
        "print('Number of trainable params: ',sum(p.numel() for p in model.parameters()))\n",
        "\n",
        "# encountered bugs in google colab only, unless I explicitly defined optimizer in this cell...\n",
        "optimizer = torch.optim.SGD(model.parameters(),lr=0.01, weight_decay=1e-3, momentum=0.8)\n",
        "\n",
        "# instantiate the checkpoint save function\n",
        "save_checkpoint = make_save_checkpoint()\n",
        "\n",
        "# instantiate the training step function \n",
        "train_step = make_train_step(model, criterion, optimizer=optimizer)\n",
        "\n",
        "# instantiate the validation loop function\n",
        "validate = make_validate_fnc(model,criterion)\n",
        "\n",
        "# instantiate lists to hold scalar performance metrics to plot later\n",
        "train_losses=[]\n",
        "valid_losses = []"
      ],
      "metadata": {
        "id": "3Phl9gsqrlJL",
        "outputId": "bda6fdf9-8d4c-46e9-c218-e04e2176ebf6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda selected\n",
            "Number of trainable params:  100917\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_path = Path(f'/content/models/checkpoints')\n",
        "model_path.mkdir(parents = True, exist_ok=True)"
      ],
      "metadata": {
        "id": "APHO-BpWuj9r"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# create training loop for one complete epoch (entire training set)\n",
        "def train(optimizer, model, num_epochs, X_train, Y_train, X_valid, Y_valid):\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        \n",
        "        # set model to train phase\n",
        "        model.train()         \n",
        "        \n",
        "        # shuffle entire training set in each epoch to randomize minibatch order\n",
        "        train_indices = np.random.permutation(train_size) \n",
        "        \n",
        "        # shuffle the training set for each epoch:\n",
        "        X_train = X_train[train_indices,:,:,:] \n",
        "        Y_train = Y_train[train_indices]\n",
        "\n",
        "        # instantiate scalar values to keep track of progress after each epoch so we can stop training when appropriate \n",
        "        epoch_acc = 0 \n",
        "        epoch_loss = 0\n",
        "        num_iterations = int(train_size / minibatch)\n",
        "        \n",
        "        # create a loop for each minibatch of 32 samples:\n",
        "        for i in range(num_iterations):\n",
        "            \n",
        "            # we have to track and update minibatch position for the current minibatch\n",
        "            # if we take a random batch position from a set, we almost certainly will skip some of the data in that set\n",
        "            # track minibatch position based on iteration number:\n",
        "            batch_start = i * minibatch \n",
        "            # ensure we don't go out of the bounds of our training set:\n",
        "            batch_end = min(batch_start + minibatch, train_size) \n",
        "            # ensure we don't have an index error\n",
        "            actual_batch_size = batch_end-batch_start \n",
        "            \n",
        "            # get training minibatch with all channnels and 2D feature dims\n",
        "            X = X_train[batch_start:batch_end,:,:,:] \n",
        "            # get training minibatch labels \n",
        "            Y = Y_train[batch_start:batch_end] \n",
        "\n",
        "            # instantiate training tensors\n",
        "            X_tensor = X.clone().detach().to(device)\n",
        "            Y_tensor = Y.clone().detach().to(device)\n",
        "\n",
        "            # Pass input tensors thru 1 training step (fwd+backwards pass)\n",
        "            loss, acc = train_step(X_tensor, Y_tensor) \n",
        "            \n",
        "            # aggregate batch accuracy to measure progress of entire epoch\n",
        "            epoch_acc += acc * actual_batch_size / train_size\n",
        "            epoch_loss += loss * actual_batch_size / train_size\n",
        "            \n",
        "            # keep track of the iteration to see if the model's too slow\n",
        "            print('\\r'+f'Epoch {epoch}: iteration {i}/{num_iterations}',end='')\n",
        "        \n",
        "        \n",
        "        X_valid_tensor = X_valid.clone().detach().to(device)\n",
        "        Y_valid_tensor = Y_valid.clone().detach().to(device)\n",
        "        \n",
        "        # calculate validation metrics to keep track of progress; don't need predictions now\n",
        "        valid_loss, valid_acc, _ = validate(X_valid_tensor, Y_valid_tensor)\n",
        "        \n",
        "        # accumulate scalar performance metrics at each epoch to track and plot later\n",
        "        train_losses.append(epoch_loss)\n",
        "        valid_losses.append(valid_loss)\n",
        "\n",
        "        if epoch % 3 == 0:\n",
        "            save_checkpoint(optimizer, model, epoch, model_path/'v2.1-{epoch:03d}.pkl')\n",
        "        \n",
        "        # keep track of each epoch's progress\n",
        "        print(f'\\nEpoch {epoch} --- loss:{epoch_loss:.3f}, Epoch f-score:{epoch_acc:.2f}%, Validation loss:{valid_loss:.3f}, Validation f-score:{valid_acc:.2f}%')"
      ],
      "metadata": {
        "id": "zN_CPodLxyOn"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# choose number of epochs higher than reasonable so we can manually stop training \n",
        "num_epochs = 500\n",
        "\n",
        "# train it!\n",
        "train(optimizer, model, num_epochs, X_train, y_train, X_valid, y_valid)"
      ],
      "metadata": {
        "id": "uIT7vlvcrrB3",
        "outputId": "5f545677-4225-40dd-cf11-53e10447c5ff",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0: iteration 192/193\n",
            "Epoch 0 --- loss:1.828, Epoch f-score:0.39%, Validation loss:1.452, Validation f-score:0.31%\n",
            "Epoch 1: iteration 192/193\n",
            "Epoch 1 --- loss:1.410, Epoch f-score:0.40%, Validation loss:1.331, Validation f-score:0.33%\n",
            "Epoch 2: iteration 192/193\n",
            "Epoch 2 --- loss:1.307, Epoch f-score:0.41%, Validation loss:1.341, Validation f-score:0.38%\n",
            "Epoch 3: iteration 192/193\n",
            "Epoch 3 --- loss:1.265, Epoch f-score:0.42%, Validation loss:1.279, Validation f-score:0.36%\n",
            "Epoch 4: iteration 192/193\n",
            "Epoch 4 --- loss:1.230, Epoch f-score:0.43%, Validation loss:1.400, Validation f-score:0.35%\n",
            "Epoch 5: iteration 192/193\n",
            "Epoch 5 --- loss:1.199, Epoch f-score:0.45%, Validation loss:1.259, Validation f-score:0.38%\n",
            "Epoch 6: iteration 192/193\n",
            "Epoch 6 --- loss:1.188, Epoch f-score:0.45%, Validation loss:1.413, Validation f-score:0.34%\n",
            "Epoch 7: iteration 192/193\n",
            "Epoch 7 --- loss:1.183, Epoch f-score:0.45%, Validation loss:1.387, Validation f-score:0.35%\n",
            "Epoch 8: iteration 192/193\n",
            "Epoch 8 --- loss:1.173, Epoch f-score:0.46%, Validation loss:1.497, Validation f-score:0.31%\n",
            "Epoch 9: iteration 192/193\n",
            "Epoch 9 --- loss:1.169, Epoch f-score:0.46%, Validation loss:1.385, Validation f-score:0.33%\n",
            "Epoch 10: iteration 192/193\n",
            "Epoch 10 --- loss:1.153, Epoch f-score:0.47%, Validation loss:1.472, Validation f-score:0.32%\n",
            "Epoch 11: iteration 192/193\n",
            "Epoch 11 --- loss:1.145, Epoch f-score:0.47%, Validation loss:1.336, Validation f-score:0.38%\n",
            "Epoch 12: iteration 192/193\n",
            "Epoch 12 --- loss:1.144, Epoch f-score:0.46%, Validation loss:1.325, Validation f-score:0.36%\n",
            "Epoch 13: iteration 192/193\n",
            "Epoch 13 --- loss:1.139, Epoch f-score:0.47%, Validation loss:1.384, Validation f-score:0.35%\n",
            "Epoch 14: iteration 192/193\n",
            "Epoch 14 --- loss:1.135, Epoch f-score:0.47%, Validation loss:1.450, Validation f-score:0.33%\n",
            "Epoch 15: iteration 192/193\n",
            "Epoch 15 --- loss:1.132, Epoch f-score:0.47%, Validation loss:1.346, Validation f-score:0.35%\n",
            "Epoch 16: iteration 192/193\n",
            "Epoch 16 --- loss:1.124, Epoch f-score:0.47%, Validation loss:1.393, Validation f-score:0.32%\n",
            "Epoch 17: iteration 192/193\n",
            "Epoch 17 --- loss:1.121, Epoch f-score:0.48%, Validation loss:1.408, Validation f-score:0.33%\n",
            "Epoch 18: iteration 192/193\n",
            "Epoch 18 --- loss:1.118, Epoch f-score:0.49%, Validation loss:1.345, Validation f-score:0.34%\n",
            "Epoch 19: iteration 192/193\n",
            "Epoch 19 --- loss:1.106, Epoch f-score:0.49%, Validation loss:1.307, Validation f-score:0.39%\n",
            "Epoch 20: iteration 192/193\n",
            "Epoch 20 --- loss:1.119, Epoch f-score:0.48%, Validation loss:1.368, Validation f-score:0.36%\n",
            "Epoch 21: iteration 192/193\n",
            "Epoch 21 --- loss:1.108, Epoch f-score:0.49%, Validation loss:1.679, Validation f-score:0.24%\n",
            "Epoch 22: iteration 192/193\n",
            "Epoch 22 --- loss:1.103, Epoch f-score:0.50%, Validation loss:1.361, Validation f-score:0.35%\n",
            "Epoch 23: iteration 192/193\n",
            "Epoch 23 --- loss:1.103, Epoch f-score:0.49%, Validation loss:1.381, Validation f-score:0.35%\n",
            "Epoch 24: iteration 192/193\n",
            "Epoch 24 --- loss:1.103, Epoch f-score:0.50%, Validation loss:1.412, Validation f-score:0.34%\n",
            "Epoch 25: iteration 192/193\n",
            "Epoch 25 --- loss:1.105, Epoch f-score:0.49%, Validation loss:1.425, Validation f-score:0.33%\n",
            "Epoch 26: iteration 192/193\n",
            "Epoch 26 --- loss:1.096, Epoch f-score:0.49%, Validation loss:1.380, Validation f-score:0.35%\n",
            "Epoch 27: iteration 192/193\n",
            "Epoch 27 --- loss:1.099, Epoch f-score:0.50%, Validation loss:1.376, Validation f-score:0.35%\n",
            "Epoch 28: iteration 192/193\n",
            "Epoch 28 --- loss:1.096, Epoch f-score:0.49%, Validation loss:1.425, Validation f-score:0.33%\n",
            "Epoch 29: iteration 192/193\n",
            "Epoch 29 --- loss:1.093, Epoch f-score:0.50%, Validation loss:1.515, Validation f-score:0.30%\n",
            "Epoch 30: iteration 192/193\n",
            "Epoch 30 --- loss:1.092, Epoch f-score:0.49%, Validation loss:1.419, Validation f-score:0.32%\n",
            "Epoch 31: iteration 192/193\n",
            "Epoch 31 --- loss:1.091, Epoch f-score:0.50%, Validation loss:1.319, Validation f-score:0.39%\n",
            "Epoch 32: iteration 192/193\n",
            "Epoch 32 --- loss:1.091, Epoch f-score:0.51%, Validation loss:1.545, Validation f-score:0.27%\n",
            "Epoch 33: iteration 192/193\n",
            "Epoch 33 --- loss:1.090, Epoch f-score:0.50%, Validation loss:1.559, Validation f-score:0.27%\n",
            "Epoch 34: iteration 192/193\n",
            "Epoch 34 --- loss:1.093, Epoch f-score:0.50%, Validation loss:1.332, Validation f-score:0.37%\n",
            "Epoch 35: iteration 192/193\n",
            "Epoch 35 --- loss:1.089, Epoch f-score:0.50%, Validation loss:1.418, Validation f-score:0.32%\n",
            "Epoch 36: iteration 192/193\n",
            "Epoch 36 --- loss:1.086, Epoch f-score:0.50%, Validation loss:1.335, Validation f-score:0.36%\n",
            "Epoch 37: iteration 192/193\n",
            "Epoch 37 --- loss:1.082, Epoch f-score:0.51%, Validation loss:1.529, Validation f-score:0.29%\n",
            "Epoch 38: iteration 192/193\n",
            "Epoch 38 --- loss:1.084, Epoch f-score:0.50%, Validation loss:1.517, Validation f-score:0.31%\n",
            "Epoch 39: iteration 192/193\n",
            "Epoch 39 --- loss:1.082, Epoch f-score:0.50%, Validation loss:1.428, Validation f-score:0.30%\n",
            "Epoch 40: iteration 192/193\n",
            "Epoch 40 --- loss:1.084, Epoch f-score:0.51%, Validation loss:1.298, Validation f-score:0.38%\n",
            "Epoch 41: iteration 192/193\n",
            "Epoch 41 --- loss:1.086, Epoch f-score:0.51%, Validation loss:1.490, Validation f-score:0.29%\n",
            "Epoch 42: iteration 192/193\n",
            "Epoch 42 --- loss:1.079, Epoch f-score:0.51%, Validation loss:1.475, Validation f-score:0.32%\n",
            "Epoch 43: iteration 192/193\n",
            "Epoch 43 --- loss:1.082, Epoch f-score:0.50%, Validation loss:1.354, Validation f-score:0.34%\n",
            "Epoch 44: iteration 192/193\n",
            "Epoch 44 --- loss:1.081, Epoch f-score:0.50%, Validation loss:1.361, Validation f-score:0.36%\n",
            "Epoch 45: iteration 192/193\n",
            "Epoch 45 --- loss:1.073, Epoch f-score:0.51%, Validation loss:1.323, Validation f-score:0.37%\n",
            "Epoch 46: iteration 192/193\n",
            "Epoch 46 --- loss:1.078, Epoch f-score:0.51%, Validation loss:1.305, Validation f-score:0.37%\n",
            "Epoch 47: iteration 192/193\n",
            "Epoch 47 --- loss:1.075, Epoch f-score:0.51%, Validation loss:1.365, Validation f-score:0.33%\n",
            "Epoch 48: iteration 192/193\n",
            "Epoch 48 --- loss:1.073, Epoch f-score:0.51%, Validation loss:1.449, Validation f-score:0.32%\n",
            "Epoch 49: iteration 192/193\n",
            "Epoch 49 --- loss:1.076, Epoch f-score:0.51%, Validation loss:1.332, Validation f-score:0.37%\n",
            "Epoch 50: iteration 192/193\n",
            "Epoch 50 --- loss:1.074, Epoch f-score:0.52%, Validation loss:1.286, Validation f-score:0.38%\n",
            "Epoch 51: iteration 192/193\n",
            "Epoch 51 --- loss:1.071, Epoch f-score:0.51%, Validation loss:1.373, Validation f-score:0.36%\n",
            "Epoch 52: iteration 192/193\n",
            "Epoch 52 --- loss:1.065, Epoch f-score:0.52%, Validation loss:1.403, Validation f-score:0.35%\n",
            "Epoch 53: iteration 192/193\n",
            "Epoch 53 --- loss:1.065, Epoch f-score:0.51%, Validation loss:1.334, Validation f-score:0.37%\n",
            "Epoch 54: iteration 192/193\n",
            "Epoch 54 --- loss:1.062, Epoch f-score:0.52%, Validation loss:1.355, Validation f-score:0.37%\n",
            "Epoch 55: iteration 192/193\n",
            "Epoch 55 --- loss:1.068, Epoch f-score:0.51%, Validation loss:1.380, Validation f-score:0.36%\n",
            "Epoch 56: iteration 192/193\n",
            "Epoch 56 --- loss:1.068, Epoch f-score:0.51%, Validation loss:1.350, Validation f-score:0.36%\n",
            "Epoch 57: iteration 192/193\n",
            "Epoch 57 --- loss:1.064, Epoch f-score:0.52%, Validation loss:1.454, Validation f-score:0.34%\n",
            "Epoch 58: iteration 192/193\n",
            "Epoch 58 --- loss:1.068, Epoch f-score:0.52%, Validation loss:1.461, Validation f-score:0.33%\n",
            "Epoch 59: iteration 192/193\n",
            "Epoch 59 --- loss:1.060, Epoch f-score:0.52%, Validation loss:1.395, Validation f-score:0.34%\n",
            "Epoch 60: iteration 192/193\n",
            "Epoch 60 --- loss:1.067, Epoch f-score:0.52%, Validation loss:1.348, Validation f-score:0.37%\n",
            "Epoch 61: iteration 192/193\n",
            "Epoch 61 --- loss:1.062, Epoch f-score:0.52%, Validation loss:1.387, Validation f-score:0.33%\n",
            "Epoch 62: iteration 192/193\n",
            "Epoch 62 --- loss:1.059, Epoch f-score:0.52%, Validation loss:1.392, Validation f-score:0.36%\n",
            "Epoch 63: iteration 192/193\n",
            "Epoch 63 --- loss:1.060, Epoch f-score:0.52%, Validation loss:1.460, Validation f-score:0.33%\n",
            "Epoch 64: iteration 192/193\n",
            "Epoch 64 --- loss:1.059, Epoch f-score:0.52%, Validation loss:1.463, Validation f-score:0.33%\n",
            "Epoch 65: iteration 192/193\n",
            "Epoch 65 --- loss:1.066, Epoch f-score:0.52%, Validation loss:1.332, Validation f-score:0.36%\n",
            "Epoch 66: iteration 192/193\n",
            "Epoch 66 --- loss:1.059, Epoch f-score:0.52%, Validation loss:1.327, Validation f-score:0.37%\n",
            "Epoch 67: iteration 192/193\n",
            "Epoch 67 --- loss:1.054, Epoch f-score:0.53%, Validation loss:1.460, Validation f-score:0.33%\n",
            "Epoch 68: iteration 192/193\n",
            "Epoch 68 --- loss:1.046, Epoch f-score:0.52%, Validation loss:1.465, Validation f-score:0.32%\n",
            "Epoch 69: iteration 192/193\n",
            "Epoch 69 --- loss:1.047, Epoch f-score:0.53%, Validation loss:1.426, Validation f-score:0.34%\n",
            "Epoch 70: iteration 192/193\n",
            "Epoch 70 --- loss:1.052, Epoch f-score:0.53%, Validation loss:1.483, Validation f-score:0.31%\n",
            "Epoch 71: iteration 192/193\n",
            "Epoch 71 --- loss:1.062, Epoch f-score:0.52%, Validation loss:1.416, Validation f-score:0.33%\n",
            "Epoch 72: iteration 192/193\n",
            "Epoch 72 --- loss:1.054, Epoch f-score:0.53%, Validation loss:1.434, Validation f-score:0.34%\n",
            "Epoch 73: iteration 192/193\n",
            "Epoch 73 --- loss:1.051, Epoch f-score:0.53%, Validation loss:1.232, Validation f-score:0.41%\n",
            "Epoch 74: iteration 192/193\n",
            "Epoch 74 --- loss:1.044, Epoch f-score:0.53%, Validation loss:1.442, Validation f-score:0.33%\n",
            "Epoch 75: iteration 192/193\n",
            "Epoch 75 --- loss:1.050, Epoch f-score:0.52%, Validation loss:1.352, Validation f-score:0.37%\n",
            "Epoch 76: iteration 192/193\n",
            "Epoch 76 --- loss:1.050, Epoch f-score:0.53%, Validation loss:1.393, Validation f-score:0.35%\n",
            "Epoch 77: iteration 192/193\n",
            "Epoch 77 --- loss:1.052, Epoch f-score:0.52%, Validation loss:1.439, Validation f-score:0.33%\n",
            "Epoch 78: iteration 192/193\n",
            "Epoch 78 --- loss:1.041, Epoch f-score:0.53%, Validation loss:1.404, Validation f-score:0.35%\n",
            "Epoch 79: iteration 192/193\n",
            "Epoch 79 --- loss:1.044, Epoch f-score:0.53%, Validation loss:1.376, Validation f-score:0.34%\n",
            "Epoch 80: iteration 192/193\n",
            "Epoch 80 --- loss:1.044, Epoch f-score:0.53%, Validation loss:1.385, Validation f-score:0.35%\n",
            "Epoch 81: iteration 192/193\n",
            "Epoch 81 --- loss:1.047, Epoch f-score:0.53%, Validation loss:1.396, Validation f-score:0.34%\n",
            "Epoch 82: iteration 192/193\n",
            "Epoch 82 --- loss:1.042, Epoch f-score:0.53%, Validation loss:1.387, Validation f-score:0.36%\n",
            "Epoch 83: iteration 192/193\n",
            "Epoch 83 --- loss:1.045, Epoch f-score:0.53%, Validation loss:1.445, Validation f-score:0.32%\n",
            "Epoch 84: iteration 192/193\n",
            "Epoch 84 --- loss:1.036, Epoch f-score:0.54%, Validation loss:1.323, Validation f-score:0.38%\n",
            "Epoch 85: iteration 192/193\n",
            "Epoch 85 --- loss:1.039, Epoch f-score:0.54%, Validation loss:1.406, Validation f-score:0.33%\n",
            "Epoch 86: iteration 192/193\n",
            "Epoch 86 --- loss:1.038, Epoch f-score:0.54%, Validation loss:1.469, Validation f-score:0.32%\n",
            "Epoch 87: iteration 192/193\n",
            "Epoch 87 --- loss:1.034, Epoch f-score:0.53%, Validation loss:1.346, Validation f-score:0.37%\n",
            "Epoch 88: iteration 192/193\n",
            "Epoch 88 --- loss:1.038, Epoch f-score:0.53%, Validation loss:1.363, Validation f-score:0.34%\n",
            "Epoch 89: iteration 192/193\n",
            "Epoch 89 --- loss:1.032, Epoch f-score:0.54%, Validation loss:1.450, Validation f-score:0.34%\n",
            "Epoch 90: iteration 192/193\n",
            "Epoch 90 --- loss:1.037, Epoch f-score:0.54%, Validation loss:1.328, Validation f-score:0.38%\n",
            "Epoch 91: iteration 192/193\n",
            "Epoch 91 --- loss:1.041, Epoch f-score:0.54%, Validation loss:1.449, Validation f-score:0.34%\n",
            "Epoch 92: iteration 192/193\n",
            "Epoch 92 --- loss:1.029, Epoch f-score:0.54%, Validation loss:1.342, Validation f-score:0.37%\n",
            "Epoch 93: iteration 192/193\n",
            "Epoch 93 --- loss:1.032, Epoch f-score:0.54%, Validation loss:1.371, Validation f-score:0.35%\n",
            "Epoch 94: iteration 192/193\n",
            "Epoch 94 --- loss:1.033, Epoch f-score:0.54%, Validation loss:1.359, Validation f-score:0.37%\n",
            "Epoch 95: iteration 192/193\n",
            "Epoch 95 --- loss:1.027, Epoch f-score:0.55%, Validation loss:1.385, Validation f-score:0.34%\n",
            "Epoch 96: iteration 192/193\n",
            "Epoch 96 --- loss:1.026, Epoch f-score:0.54%, Validation loss:1.403, Validation f-score:0.36%\n",
            "Epoch 97: iteration 192/193\n",
            "Epoch 97 --- loss:1.028, Epoch f-score:0.55%, Validation loss:1.393, Validation f-score:0.36%\n",
            "Epoch 98: iteration 192/193\n",
            "Epoch 98 --- loss:1.024, Epoch f-score:0.54%, Validation loss:1.297, Validation f-score:0.39%\n",
            "Epoch 99: iteration 192/193\n",
            "Epoch 99 --- loss:1.025, Epoch f-score:0.55%, Validation loss:1.318, Validation f-score:0.39%\n",
            "Epoch 100: iteration 192/193\n",
            "Epoch 100 --- loss:1.030, Epoch f-score:0.54%, Validation loss:1.317, Validation f-score:0.40%\n",
            "Epoch 101: iteration 192/193\n",
            "Epoch 101 --- loss:1.026, Epoch f-score:0.55%, Validation loss:1.422, Validation f-score:0.34%\n",
            "Epoch 102: iteration 192/193\n",
            "Epoch 102 --- loss:1.009, Epoch f-score:0.55%, Validation loss:1.386, Validation f-score:0.36%\n",
            "Epoch 103: iteration 192/193\n",
            "Epoch 103 --- loss:1.020, Epoch f-score:0.55%, Validation loss:1.439, Validation f-score:0.35%\n",
            "Epoch 104: iteration 192/193\n",
            "Epoch 104 --- loss:1.016, Epoch f-score:0.55%, Validation loss:1.421, Validation f-score:0.35%\n",
            "Epoch 105: iteration 192/193\n",
            "Epoch 105 --- loss:1.016, Epoch f-score:0.55%, Validation loss:1.418, Validation f-score:0.33%\n",
            "Epoch 106: iteration 192/193\n",
            "Epoch 106 --- loss:1.013, Epoch f-score:0.55%, Validation loss:1.406, Validation f-score:0.34%\n",
            "Epoch 107: iteration 192/193\n",
            "Epoch 107 --- loss:1.015, Epoch f-score:0.55%, Validation loss:1.337, Validation f-score:0.39%\n",
            "Epoch 108: iteration 192/193\n",
            "Epoch 108 --- loss:1.011, Epoch f-score:0.55%, Validation loss:1.329, Validation f-score:0.37%\n",
            "Epoch 109: iteration 192/193\n",
            "Epoch 109 --- loss:1.014, Epoch f-score:0.55%, Validation loss:1.331, Validation f-score:0.37%\n",
            "Epoch 110: iteration 192/193\n",
            "Epoch 110 --- loss:1.012, Epoch f-score:0.56%, Validation loss:1.451, Validation f-score:0.34%\n",
            "Epoch 111: iteration 192/193\n",
            "Epoch 111 --- loss:1.020, Epoch f-score:0.55%, Validation loss:1.528, Validation f-score:0.32%\n",
            "Epoch 112: iteration 192/193\n",
            "Epoch 112 --- loss:1.005, Epoch f-score:0.55%, Validation loss:1.474, Validation f-score:0.34%\n",
            "Epoch 113: iteration 192/193\n",
            "Epoch 113 --- loss:1.008, Epoch f-score:0.56%, Validation loss:1.448, Validation f-score:0.32%\n",
            "Epoch 114: iteration 192/193\n",
            "Epoch 114 --- loss:1.020, Epoch f-score:0.55%, Validation loss:1.289, Validation f-score:0.41%\n",
            "Epoch 115: iteration 192/193\n",
            "Epoch 115 --- loss:1.013, Epoch f-score:0.56%, Validation loss:1.493, Validation f-score:0.32%\n",
            "Epoch 116: iteration 192/193\n",
            "Epoch 116 --- loss:1.009, Epoch f-score:0.55%, Validation loss:1.334, Validation f-score:0.38%\n",
            "Epoch 117: iteration 192/193\n",
            "Epoch 117 --- loss:1.000, Epoch f-score:0.56%, Validation loss:1.347, Validation f-score:0.37%\n",
            "Epoch 118: iteration 192/193\n",
            "Epoch 118 --- loss:0.999, Epoch f-score:0.56%, Validation loss:1.432, Validation f-score:0.34%\n",
            "Epoch 119: iteration 192/193\n",
            "Epoch 119 --- loss:1.004, Epoch f-score:0.56%, Validation loss:1.456, Validation f-score:0.33%\n",
            "Epoch 120: iteration 192/193\n",
            "Epoch 120 --- loss:0.998, Epoch f-score:0.56%, Validation loss:1.453, Validation f-score:0.35%\n",
            "Epoch 121: iteration 192/193\n",
            "Epoch 121 --- loss:0.995, Epoch f-score:0.56%, Validation loss:1.400, Validation f-score:0.34%\n",
            "Epoch 122: iteration 192/193\n",
            "Epoch 122 --- loss:1.000, Epoch f-score:0.56%, Validation loss:1.485, Validation f-score:0.31%\n",
            "Epoch 123: iteration 192/193\n",
            "Epoch 123 --- loss:0.995, Epoch f-score:0.56%, Validation loss:1.480, Validation f-score:0.32%\n",
            "Epoch 124: iteration 192/193\n",
            "Epoch 124 --- loss:0.997, Epoch f-score:0.56%, Validation loss:1.342, Validation f-score:0.38%\n",
            "Epoch 125: iteration 192/193\n",
            "Epoch 125 --- loss:0.998, Epoch f-score:0.56%, Validation loss:1.317, Validation f-score:0.41%\n",
            "Epoch 126: iteration 192/193\n",
            "Epoch 126 --- loss:0.997, Epoch f-score:0.56%, Validation loss:1.408, Validation f-score:0.35%\n",
            "Epoch 127: iteration 192/193\n",
            "Epoch 127 --- loss:0.998, Epoch f-score:0.56%, Validation loss:1.377, Validation f-score:0.38%\n",
            "Epoch 128: iteration 192/193\n",
            "Epoch 128 --- loss:0.998, Epoch f-score:0.56%, Validation loss:1.354, Validation f-score:0.38%\n",
            "Epoch 129: iteration 192/193\n",
            "Epoch 129 --- loss:0.991, Epoch f-score:0.56%, Validation loss:1.277, Validation f-score:0.40%\n",
            "Epoch 130: iteration 192/193\n",
            "Epoch 130 --- loss:0.987, Epoch f-score:0.56%, Validation loss:1.463, Validation f-score:0.33%\n",
            "Epoch 131: iteration 192/193\n",
            "Epoch 131 --- loss:0.991, Epoch f-score:0.56%, Validation loss:1.364, Validation f-score:0.37%\n",
            "Epoch 132: iteration 192/193\n",
            "Epoch 132 --- loss:0.992, Epoch f-score:0.56%, Validation loss:1.471, Validation f-score:0.33%\n",
            "Epoch 133: iteration 192/193\n",
            "Epoch 133 --- loss:0.985, Epoch f-score:0.56%, Validation loss:1.403, Validation f-score:0.36%\n",
            "Epoch 134: iteration 192/193\n",
            "Epoch 134 --- loss:0.979, Epoch f-score:0.57%, Validation loss:1.436, Validation f-score:0.33%\n",
            "Epoch 135: iteration 192/193\n",
            "Epoch 135 --- loss:0.991, Epoch f-score:0.56%, Validation loss:1.446, Validation f-score:0.32%\n",
            "Epoch 136: iteration 192/193\n",
            "Epoch 136 --- loss:0.989, Epoch f-score:0.57%, Validation loss:1.407, Validation f-score:0.37%\n",
            "Epoch 137: iteration 192/193\n",
            "Epoch 137 --- loss:0.988, Epoch f-score:0.57%, Validation loss:1.315, Validation f-score:0.39%\n",
            "Epoch 138: iteration 192/193\n",
            "Epoch 138 --- loss:0.986, Epoch f-score:0.57%, Validation loss:1.448, Validation f-score:0.35%\n",
            "Epoch 139: iteration 192/193\n",
            "Epoch 139 --- loss:0.986, Epoch f-score:0.57%, Validation loss:1.518, Validation f-score:0.33%\n",
            "Epoch 140: iteration 192/193\n",
            "Epoch 140 --- loss:0.984, Epoch f-score:0.57%, Validation loss:1.367, Validation f-score:0.37%\n",
            "Epoch 141: iteration 192/193\n",
            "Epoch 141 --- loss:0.990, Epoch f-score:0.57%, Validation loss:1.467, Validation f-score:0.31%\n",
            "Epoch 142: iteration 192/193\n",
            "Epoch 142 --- loss:0.975, Epoch f-score:0.58%, Validation loss:1.418, Validation f-score:0.35%\n",
            "Epoch 143: iteration 192/193\n",
            "Epoch 143 --- loss:0.977, Epoch f-score:0.58%, Validation loss:1.387, Validation f-score:0.36%\n",
            "Epoch 144: iteration 192/193\n",
            "Epoch 144 --- loss:0.974, Epoch f-score:0.57%, Validation loss:1.241, Validation f-score:0.41%\n",
            "Epoch 145: iteration 192/193\n",
            "Epoch 145 --- loss:0.989, Epoch f-score:0.57%, Validation loss:1.463, Validation f-score:0.34%\n",
            "Epoch 146: iteration 192/193\n",
            "Epoch 146 --- loss:0.979, Epoch f-score:0.57%, Validation loss:1.502, Validation f-score:0.33%\n",
            "Epoch 147: iteration 192/193\n",
            "Epoch 147 --- loss:0.975, Epoch f-score:0.57%, Validation loss:1.551, Validation f-score:0.31%\n",
            "Epoch 148: iteration 192/193\n",
            "Epoch 148 --- loss:0.972, Epoch f-score:0.58%, Validation loss:1.470, Validation f-score:0.32%\n",
            "Epoch 149: iteration 192/193\n",
            "Epoch 149 --- loss:0.974, Epoch f-score:0.58%, Validation loss:1.420, Validation f-score:0.35%\n",
            "Epoch 150: iteration 192/193\n",
            "Epoch 150 --- loss:0.981, Epoch f-score:0.58%, Validation loss:1.453, Validation f-score:0.34%\n",
            "Epoch 151: iteration 192/193\n",
            "Epoch 151 --- loss:0.973, Epoch f-score:0.57%, Validation loss:1.424, Validation f-score:0.35%\n",
            "Epoch 152: iteration 192/193\n",
            "Epoch 152 --- loss:0.976, Epoch f-score:0.57%, Validation loss:1.629, Validation f-score:0.31%\n",
            "Epoch 153: iteration 192/193\n",
            "Epoch 153 --- loss:0.971, Epoch f-score:0.57%, Validation loss:1.519, Validation f-score:0.34%\n",
            "Epoch 154: iteration 192/193\n",
            "Epoch 154 --- loss:0.964, Epoch f-score:0.57%, Validation loss:1.426, Validation f-score:0.34%\n",
            "Epoch 155: iteration 192/193\n",
            "Epoch 155 --- loss:0.969, Epoch f-score:0.57%, Validation loss:1.464, Validation f-score:0.33%\n",
            "Epoch 156: iteration 192/193\n",
            "Epoch 156 --- loss:0.967, Epoch f-score:0.59%, Validation loss:1.406, Validation f-score:0.37%\n",
            "Epoch 157: iteration 192/193\n",
            "Epoch 157 --- loss:0.967, Epoch f-score:0.57%, Validation loss:1.463, Validation f-score:0.34%\n",
            "Epoch 158: iteration 192/193\n",
            "Epoch 158 --- loss:0.981, Epoch f-score:0.58%, Validation loss:1.438, Validation f-score:0.34%\n",
            "Epoch 159: iteration 192/193\n",
            "Epoch 159 --- loss:0.967, Epoch f-score:0.57%, Validation loss:1.466, Validation f-score:0.34%\n",
            "Epoch 160: iteration 192/193\n",
            "Epoch 160 --- loss:0.978, Epoch f-score:0.57%, Validation loss:1.430, Validation f-score:0.33%\n",
            "Epoch 161: iteration 192/193\n",
            "Epoch 161 --- loss:0.958, Epoch f-score:0.58%, Validation loss:1.485, Validation f-score:0.33%\n",
            "Epoch 162: iteration 192/193\n",
            "Epoch 162 --- loss:0.966, Epoch f-score:0.58%, Validation loss:1.426, Validation f-score:0.37%\n",
            "Epoch 163: iteration 192/193\n",
            "Epoch 163 --- loss:0.973, Epoch f-score:0.57%, Validation loss:1.450, Validation f-score:0.33%\n",
            "Epoch 164: iteration 192/193\n",
            "Epoch 164 --- loss:0.963, Epoch f-score:0.58%, Validation loss:1.486, Validation f-score:0.32%\n",
            "Epoch 165: iteration 192/193\n",
            "Epoch 165 --- loss:0.974, Epoch f-score:0.58%, Validation loss:1.530, Validation f-score:0.29%\n",
            "Epoch 166: iteration 192/193\n",
            "Epoch 166 --- loss:0.970, Epoch f-score:0.57%, Validation loss:1.477, Validation f-score:0.32%\n",
            "Epoch 167: iteration 192/193\n",
            "Epoch 167 --- loss:0.958, Epoch f-score:0.58%, Validation loss:1.376, Validation f-score:0.37%\n",
            "Epoch 168: iteration 192/193\n",
            "Epoch 168 --- loss:0.958, Epoch f-score:0.59%, Validation loss:1.393, Validation f-score:0.36%\n",
            "Epoch 169: iteration 192/193\n",
            "Epoch 169 --- loss:0.958, Epoch f-score:0.58%, Validation loss:1.559, Validation f-score:0.27%\n",
            "Epoch 170: iteration 192/193\n",
            "Epoch 170 --- loss:0.965, Epoch f-score:0.58%, Validation loss:1.442, Validation f-score:0.37%\n",
            "Epoch 171: iteration 192/193\n",
            "Epoch 171 --- loss:0.961, Epoch f-score:0.59%, Validation loss:1.483, Validation f-score:0.31%\n",
            "Epoch 172: iteration 192/193\n",
            "Epoch 172 --- loss:0.959, Epoch f-score:0.58%, Validation loss:1.430, Validation f-score:0.33%\n",
            "Epoch 173: iteration 192/193\n",
            "Epoch 173 --- loss:0.962, Epoch f-score:0.58%, Validation loss:1.556, Validation f-score:0.28%\n",
            "Epoch 174: iteration 192/193\n",
            "Epoch 174 --- loss:0.957, Epoch f-score:0.59%, Validation loss:1.384, Validation f-score:0.38%\n",
            "Epoch 175: iteration 192/193\n",
            "Epoch 175 --- loss:0.945, Epoch f-score:0.58%, Validation loss:1.535, Validation f-score:0.30%\n",
            "Epoch 176: iteration 192/193\n",
            "Epoch 176 --- loss:0.948, Epoch f-score:0.59%, Validation loss:1.395, Validation f-score:0.36%\n",
            "Epoch 177: iteration 192/193\n",
            "Epoch 177 --- loss:0.953, Epoch f-score:0.58%, Validation loss:1.418, Validation f-score:0.36%\n",
            "Epoch 178: iteration 192/193\n",
            "Epoch 178 --- loss:0.950, Epoch f-score:0.59%, Validation loss:1.553, Validation f-score:0.27%\n",
            "Epoch 179: iteration 192/193\n",
            "Epoch 179 --- loss:0.949, Epoch f-score:0.59%, Validation loss:1.461, Validation f-score:0.34%\n",
            "Epoch 180: iteration 192/193\n",
            "Epoch 180 --- loss:0.953, Epoch f-score:0.59%, Validation loss:1.491, Validation f-score:0.32%\n",
            "Epoch 181: iteration 192/193\n",
            "Epoch 181 --- loss:0.958, Epoch f-score:0.58%, Validation loss:1.462, Validation f-score:0.31%\n",
            "Epoch 182: iteration 192/193\n",
            "Epoch 182 --- loss:0.945, Epoch f-score:0.59%, Validation loss:1.353, Validation f-score:0.37%\n",
            "Epoch 183: iteration 192/193\n",
            "Epoch 183 --- loss:0.965, Epoch f-score:0.58%, Validation loss:1.456, Validation f-score:0.29%\n",
            "Epoch 184: iteration 192/193\n",
            "Epoch 184 --- loss:0.945, Epoch f-score:0.59%, Validation loss:1.420, Validation f-score:0.35%\n",
            "Epoch 185: iteration 192/193\n",
            "Epoch 185 --- loss:0.943, Epoch f-score:0.59%, Validation loss:1.468, Validation f-score:0.32%\n",
            "Epoch 186: iteration 192/193\n",
            "Epoch 186 --- loss:0.957, Epoch f-score:0.58%, Validation loss:1.503, Validation f-score:0.34%\n",
            "Epoch 187: iteration 192/193\n",
            "Epoch 187 --- loss:0.951, Epoch f-score:0.58%, Validation loss:1.370, Validation f-score:0.36%\n",
            "Epoch 188: iteration 192/193\n",
            "Epoch 188 --- loss:0.951, Epoch f-score:0.59%, Validation loss:1.434, Validation f-score:0.33%\n",
            "Epoch 189: iteration 192/193\n",
            "Epoch 189 --- loss:0.948, Epoch f-score:0.59%, Validation loss:1.391, Validation f-score:0.36%\n",
            "Epoch 190: iteration 192/193\n",
            "Epoch 190 --- loss:0.941, Epoch f-score:0.59%, Validation loss:1.373, Validation f-score:0.35%\n",
            "Epoch 191: iteration 192/193\n",
            "Epoch 191 --- loss:0.941, Epoch f-score:0.60%, Validation loss:1.406, Validation f-score:0.37%\n",
            "Epoch 192: iteration 192/193\n",
            "Epoch 192 --- loss:0.930, Epoch f-score:0.61%, Validation loss:1.466, Validation f-score:0.34%\n",
            "Epoch 193: iteration 192/193\n",
            "Epoch 193 --- loss:0.947, Epoch f-score:0.59%, Validation loss:1.408, Validation f-score:0.35%\n",
            "Epoch 194: iteration 192/193\n",
            "Epoch 194 --- loss:0.948, Epoch f-score:0.58%, Validation loss:1.450, Validation f-score:0.35%\n",
            "Epoch 195: iteration 192/193\n",
            "Epoch 195 --- loss:0.943, Epoch f-score:0.59%, Validation loss:1.398, Validation f-score:0.36%\n",
            "Epoch 196: iteration 192/193\n",
            "Epoch 196 --- loss:0.948, Epoch f-score:0.58%, Validation loss:1.332, Validation f-score:0.39%\n",
            "Epoch 197: iteration 192/193\n",
            "Epoch 197 --- loss:0.936, Epoch f-score:0.60%, Validation loss:1.447, Validation f-score:0.34%\n",
            "Epoch 198: iteration 192/193\n",
            "Epoch 198 --- loss:0.952, Epoch f-score:0.59%, Validation loss:1.558, Validation f-score:0.25%\n",
            "Epoch 199: iteration 192/193\n",
            "Epoch 199 --- loss:0.938, Epoch f-score:0.59%, Validation loss:1.453, Validation f-score:0.31%\n",
            "Epoch 200: iteration 192/193\n",
            "Epoch 200 --- loss:0.951, Epoch f-score:0.59%, Validation loss:1.445, Validation f-score:0.32%\n",
            "Epoch 201: iteration 192/193\n",
            "Epoch 201 --- loss:0.932, Epoch f-score:0.60%, Validation loss:1.466, Validation f-score:0.29%\n",
            "Epoch 202: iteration 192/193\n",
            "Epoch 202 --- loss:0.940, Epoch f-score:0.60%, Validation loss:1.467, Validation f-score:0.31%\n",
            "Epoch 203: iteration 192/193\n",
            "Epoch 203 --- loss:0.939, Epoch f-score:0.60%, Validation loss:1.528, Validation f-score:0.30%\n",
            "Epoch 204: iteration 192/193\n",
            "Epoch 204 --- loss:0.941, Epoch f-score:0.59%, Validation loss:1.597, Validation f-score:0.27%\n",
            "Epoch 205: iteration 192/193\n",
            "Epoch 205 --- loss:0.927, Epoch f-score:0.60%, Validation loss:1.365, Validation f-score:0.36%\n",
            "Epoch 206: iteration 192/193\n",
            "Epoch 206 --- loss:0.943, Epoch f-score:0.59%, Validation loss:1.506, Validation f-score:0.34%\n",
            "Epoch 207: iteration 192/193\n",
            "Epoch 207 --- loss:0.943, Epoch f-score:0.59%, Validation loss:1.393, Validation f-score:0.33%\n",
            "Epoch 208: iteration 192/193\n",
            "Epoch 208 --- loss:0.934, Epoch f-score:0.60%, Validation loss:1.530, Validation f-score:0.30%\n",
            "Epoch 209: iteration 192/193\n",
            "Epoch 209 --- loss:0.938, Epoch f-score:0.60%, Validation loss:1.416, Validation f-score:0.32%\n",
            "Epoch 210: iteration 192/193\n",
            "Epoch 210 --- loss:0.928, Epoch f-score:0.61%, Validation loss:1.314, Validation f-score:0.38%\n",
            "Epoch 211: iteration 192/193\n",
            "Epoch 211 --- loss:0.939, Epoch f-score:0.60%, Validation loss:1.397, Validation f-score:0.33%\n",
            "Epoch 212: iteration 192/193\n",
            "Epoch 212 --- loss:0.923, Epoch f-score:0.60%, Validation loss:1.402, Validation f-score:0.35%\n",
            "Epoch 213: iteration 192/193\n",
            "Epoch 213 --- loss:0.933, Epoch f-score:0.60%, Validation loss:1.385, Validation f-score:0.35%\n",
            "Epoch 214: iteration 192/193\n",
            "Epoch 214 --- loss:0.927, Epoch f-score:0.60%, Validation loss:1.406, Validation f-score:0.36%\n",
            "Epoch 215: iteration 192/193\n",
            "Epoch 215 --- loss:0.922, Epoch f-score:0.60%, Validation loss:1.355, Validation f-score:0.38%\n",
            "Epoch 216: iteration 192/193\n",
            "Epoch 216 --- loss:0.937, Epoch f-score:0.60%, Validation loss:1.377, Validation f-score:0.34%\n",
            "Epoch 217: iteration 192/193\n",
            "Epoch 217 --- loss:0.932, Epoch f-score:0.60%, Validation loss:1.503, Validation f-score:0.31%\n",
            "Epoch 218: iteration 192/193\n",
            "Epoch 218 --- loss:0.935, Epoch f-score:0.60%, Validation loss:1.541, Validation f-score:0.26%\n",
            "Epoch 219: iteration 192/193\n",
            "Epoch 219 --- loss:0.939, Epoch f-score:0.59%, Validation loss:1.526, Validation f-score:0.31%\n",
            "Epoch 220: iteration 192/193\n",
            "Epoch 220 --- loss:0.920, Epoch f-score:0.60%, Validation loss:1.516, Validation f-score:0.31%\n",
            "Epoch 221: iteration 192/193\n",
            "Epoch 221 --- loss:0.925, Epoch f-score:0.60%, Validation loss:1.453, Validation f-score:0.32%\n",
            "Epoch 222: iteration 192/193\n",
            "Epoch 222 --- loss:0.925, Epoch f-score:0.60%, Validation loss:1.432, Validation f-score:0.35%\n",
            "Epoch 223: iteration 192/193\n",
            "Epoch 223 --- loss:0.926, Epoch f-score:0.60%, Validation loss:1.520, Validation f-score:0.29%\n",
            "Epoch 224: iteration 192/193\n",
            "Epoch 224 --- loss:0.928, Epoch f-score:0.60%, Validation loss:1.531, Validation f-score:0.32%\n",
            "Epoch 225: iteration 192/193\n",
            "Epoch 225 --- loss:0.919, Epoch f-score:0.60%, Validation loss:1.483, Validation f-score:0.32%\n",
            "Epoch 226: iteration 192/193\n",
            "Epoch 226 --- loss:0.926, Epoch f-score:0.60%, Validation loss:1.600, Validation f-score:0.28%\n",
            "Epoch 227: iteration 192/193\n",
            "Epoch 227 --- loss:0.915, Epoch f-score:0.60%, Validation loss:1.453, Validation f-score:0.32%\n",
            "Epoch 228: iteration 192/193\n",
            "Epoch 228 --- loss:0.918, Epoch f-score:0.60%, Validation loss:1.491, Validation f-score:0.31%\n",
            "Epoch 229: iteration 192/193\n",
            "Epoch 229 --- loss:0.930, Epoch f-score:0.60%, Validation loss:1.578, Validation f-score:0.31%\n",
            "Epoch 230: iteration 192/193\n",
            "Epoch 230 --- loss:0.916, Epoch f-score:0.61%, Validation loss:1.533, Validation f-score:0.32%\n",
            "Epoch 231: iteration 192/193\n",
            "Epoch 231 --- loss:0.926, Epoch f-score:0.60%, Validation loss:1.465, Validation f-score:0.31%\n",
            "Epoch 232: iteration 192/193\n",
            "Epoch 232 --- loss:0.911, Epoch f-score:0.61%, Validation loss:1.499, Validation f-score:0.31%\n",
            "Epoch 233: iteration 192/193\n",
            "Epoch 233 --- loss:0.912, Epoch f-score:0.61%, Validation loss:1.493, Validation f-score:0.32%\n",
            "Epoch 234: iteration 192/193\n",
            "Epoch 234 --- loss:0.912, Epoch f-score:0.61%, Validation loss:1.459, Validation f-score:0.35%\n",
            "Epoch 235: iteration 192/193\n",
            "Epoch 235 --- loss:0.913, Epoch f-score:0.61%, Validation loss:1.421, Validation f-score:0.34%\n",
            "Epoch 236: iteration 192/193\n",
            "Epoch 236 --- loss:0.907, Epoch f-score:0.61%, Validation loss:1.498, Validation f-score:0.31%\n",
            "Epoch 237: iteration 192/193\n",
            "Epoch 237 --- loss:0.915, Epoch f-score:0.60%, Validation loss:1.512, Validation f-score:0.30%\n",
            "Epoch 238: iteration 192/193\n",
            "Epoch 238 --- loss:0.925, Epoch f-score:0.59%, Validation loss:1.418, Validation f-score:0.33%\n",
            "Epoch 239: iteration 192/193\n",
            "Epoch 239 --- loss:0.908, Epoch f-score:0.61%, Validation loss:1.413, Validation f-score:0.35%\n",
            "Epoch 240: iteration 192/193\n",
            "Epoch 240 --- loss:0.911, Epoch f-score:0.60%, Validation loss:1.380, Validation f-score:0.37%\n",
            "Epoch 241: iteration 192/193\n",
            "Epoch 241 --- loss:0.922, Epoch f-score:0.60%, Validation loss:1.529, Validation f-score:0.33%\n",
            "Epoch 242: iteration 192/193\n",
            "Epoch 242 --- loss:0.909, Epoch f-score:0.61%, Validation loss:1.455, Validation f-score:0.32%\n",
            "Epoch 243: iteration 192/193\n",
            "Epoch 243 --- loss:0.902, Epoch f-score:0.60%, Validation loss:1.474, Validation f-score:0.33%\n",
            "Epoch 244: iteration 192/193\n",
            "Epoch 244 --- loss:0.923, Epoch f-score:0.60%, Validation loss:1.578, Validation f-score:0.25%\n",
            "Epoch 245: iteration 192/193\n",
            "Epoch 245 --- loss:0.903, Epoch f-score:0.61%, Validation loss:1.475, Validation f-score:0.34%\n",
            "Epoch 246: iteration 192/193\n",
            "Epoch 246 --- loss:0.911, Epoch f-score:0.61%, Validation loss:1.434, Validation f-score:0.34%\n",
            "Epoch 247: iteration 192/193\n",
            "Epoch 247 --- loss:0.912, Epoch f-score:0.61%, Validation loss:1.430, Validation f-score:0.35%\n",
            "Epoch 248: iteration 192/193\n",
            "Epoch 248 --- loss:0.910, Epoch f-score:0.61%, Validation loss:1.554, Validation f-score:0.30%\n",
            "Epoch 249: iteration 192/193\n",
            "Epoch 249 --- loss:0.924, Epoch f-score:0.60%, Validation loss:1.444, Validation f-score:0.31%\n",
            "Epoch 250: iteration 192/193\n",
            "Epoch 250 --- loss:0.908, Epoch f-score:0.61%, Validation loss:1.501, Validation f-score:0.32%\n",
            "Epoch 251: iteration 192/193\n",
            "Epoch 251 --- loss:0.904, Epoch f-score:0.61%, Validation loss:1.592, Validation f-score:0.26%\n",
            "Epoch 252: iteration 192/193\n",
            "Epoch 252 --- loss:0.919, Epoch f-score:0.60%, Validation loss:1.398, Validation f-score:0.33%\n",
            "Epoch 253: iteration 192/193\n",
            "Epoch 253 --- loss:0.905, Epoch f-score:0.62%, Validation loss:1.510, Validation f-score:0.29%\n",
            "Epoch 254: iteration 192/193\n",
            "Epoch 254 --- loss:0.907, Epoch f-score:0.61%, Validation loss:1.509, Validation f-score:0.27%\n",
            "Epoch 255: iteration 192/193\n",
            "Epoch 255 --- loss:0.900, Epoch f-score:0.61%, Validation loss:1.401, Validation f-score:0.37%\n",
            "Epoch 256: iteration 192/193\n",
            "Epoch 256 --- loss:0.910, Epoch f-score:0.61%, Validation loss:1.383, Validation f-score:0.37%\n",
            "Epoch 257: iteration 192/193\n",
            "Epoch 257 --- loss:0.899, Epoch f-score:0.62%, Validation loss:1.504, Validation f-score:0.29%\n",
            "Epoch 258: iteration 192/193\n",
            "Epoch 258 --- loss:0.898, Epoch f-score:0.62%, Validation loss:1.451, Validation f-score:0.32%\n",
            "Epoch 259: iteration 192/193\n",
            "Epoch 259 --- loss:0.895, Epoch f-score:0.62%, Validation loss:1.491, Validation f-score:0.33%\n",
            "Epoch 260: iteration 192/193\n",
            "Epoch 260 --- loss:0.925, Epoch f-score:0.60%, Validation loss:1.345, Validation f-score:0.37%\n",
            "Epoch 261: iteration 192/193\n",
            "Epoch 261 --- loss:0.924, Epoch f-score:0.60%, Validation loss:1.369, Validation f-score:0.35%\n",
            "Epoch 262: iteration 192/193\n",
            "Epoch 262 --- loss:0.903, Epoch f-score:0.61%, Validation loss:1.393, Validation f-score:0.36%\n",
            "Epoch 263: iteration 192/193\n",
            "Epoch 263 --- loss:0.905, Epoch f-score:0.62%, Validation loss:1.458, Validation f-score:0.31%\n",
            "Epoch 264: iteration 192/193\n",
            "Epoch 264 --- loss:0.902, Epoch f-score:0.62%, Validation loss:1.686, Validation f-score:0.25%\n",
            "Epoch 265: iteration 192/193\n",
            "Epoch 265 --- loss:0.901, Epoch f-score:0.62%, Validation loss:1.460, Validation f-score:0.31%\n",
            "Epoch 266: iteration 192/193\n",
            "Epoch 266 --- loss:0.906, Epoch f-score:0.61%, Validation loss:1.399, Validation f-score:0.36%\n",
            "Epoch 267: iteration 192/193\n",
            "Epoch 267 --- loss:0.900, Epoch f-score:0.61%, Validation loss:1.455, Validation f-score:0.33%\n",
            "Epoch 268: iteration 192/193\n",
            "Epoch 268 --- loss:0.906, Epoch f-score:0.61%, Validation loss:1.396, Validation f-score:0.35%\n",
            "Epoch 269: iteration 192/193\n",
            "Epoch 269 --- loss:0.894, Epoch f-score:0.61%, Validation loss:1.407, Validation f-score:0.36%\n",
            "Epoch 270: iteration 192/193\n",
            "Epoch 270 --- loss:0.900, Epoch f-score:0.61%, Validation loss:1.502, Validation f-score:0.32%\n",
            "Epoch 271: iteration 192/193\n",
            "Epoch 271 --- loss:0.910, Epoch f-score:0.60%, Validation loss:1.506, Validation f-score:0.29%\n",
            "Epoch 272: iteration 192/193\n",
            "Epoch 272 --- loss:0.895, Epoch f-score:0.62%, Validation loss:1.584, Validation f-score:0.28%\n",
            "Epoch 273: iteration 192/193\n",
            "Epoch 273 --- loss:0.905, Epoch f-score:0.61%, Validation loss:1.501, Validation f-score:0.32%\n",
            "Epoch 274: iteration 192/193\n",
            "Epoch 274 --- loss:0.898, Epoch f-score:0.62%, Validation loss:1.457, Validation f-score:0.33%\n",
            "Epoch 275: iteration 192/193\n",
            "Epoch 275 --- loss:0.889, Epoch f-score:0.62%, Validation loss:1.549, Validation f-score:0.30%\n",
            "Epoch 276: iteration 192/193\n",
            "Epoch 276 --- loss:0.896, Epoch f-score:0.62%, Validation loss:1.601, Validation f-score:0.25%\n",
            "Epoch 277: iteration 192/193\n",
            "Epoch 277 --- loss:0.893, Epoch f-score:0.62%, Validation loss:1.469, Validation f-score:0.29%\n",
            "Epoch 278: iteration 192/193\n",
            "Epoch 278 --- loss:0.887, Epoch f-score:0.63%, Validation loss:1.486, Validation f-score:0.33%\n",
            "Epoch 279: iteration 192/193\n",
            "Epoch 279 --- loss:0.904, Epoch f-score:0.61%, Validation loss:1.396, Validation f-score:0.34%\n",
            "Epoch 280: iteration 192/193\n",
            "Epoch 280 --- loss:0.889, Epoch f-score:0.62%, Validation loss:1.440, Validation f-score:0.34%\n",
            "Epoch 281: iteration 192/193\n",
            "Epoch 281 --- loss:0.904, Epoch f-score:0.61%, Validation loss:1.491, Validation f-score:0.29%\n",
            "Epoch 282: iteration 192/193\n",
            "Epoch 282 --- loss:0.891, Epoch f-score:0.62%, Validation loss:1.482, Validation f-score:0.30%\n",
            "Epoch 283: iteration 192/193\n",
            "Epoch 283 --- loss:0.890, Epoch f-score:0.62%, Validation loss:1.598, Validation f-score:0.28%\n",
            "Epoch 284: iteration 192/193\n",
            "Epoch 284 --- loss:0.885, Epoch f-score:0.63%, Validation loss:1.389, Validation f-score:0.34%\n",
            "Epoch 285: iteration 192/193\n",
            "Epoch 285 --- loss:0.884, Epoch f-score:0.62%, Validation loss:1.557, Validation f-score:0.31%\n",
            "Epoch 286: iteration 192/193\n",
            "Epoch 286 --- loss:0.901, Epoch f-score:0.61%, Validation loss:1.665, Validation f-score:0.25%\n",
            "Epoch 287: iteration 192/193\n",
            "Epoch 287 --- loss:0.899, Epoch f-score:0.62%, Validation loss:1.410, Validation f-score:0.35%\n",
            "Epoch 288: iteration 192/193\n",
            "Epoch 288 --- loss:0.892, Epoch f-score:0.62%, Validation loss:1.452, Validation f-score:0.33%\n",
            "Epoch 289: iteration 192/193\n",
            "Epoch 289 --- loss:0.890, Epoch f-score:0.62%, Validation loss:1.457, Validation f-score:0.33%\n",
            "Epoch 290: iteration 192/193\n",
            "Epoch 290 --- loss:0.882, Epoch f-score:0.62%, Validation loss:1.541, Validation f-score:0.29%\n",
            "Epoch 291: iteration 192/193\n",
            "Epoch 291 --- loss:0.893, Epoch f-score:0.62%, Validation loss:1.496, Validation f-score:0.30%\n",
            "Epoch 292: iteration 192/193\n",
            "Epoch 292 --- loss:0.892, Epoch f-score:0.62%, Validation loss:1.426, Validation f-score:0.32%\n",
            "Epoch 293: iteration 192/193\n",
            "Epoch 293 --- loss:0.889, Epoch f-score:0.62%, Validation loss:1.432, Validation f-score:0.31%\n",
            "Epoch 294: iteration 192/193\n",
            "Epoch 294 --- loss:0.886, Epoch f-score:0.63%, Validation loss:1.471, Validation f-score:0.33%\n",
            "Epoch 295: iteration 192/193\n",
            "Epoch 295 --- loss:0.877, Epoch f-score:0.63%, Validation loss:1.619, Validation f-score:0.26%\n",
            "Epoch 296: iteration 192/193\n",
            "Epoch 296 --- loss:0.885, Epoch f-score:0.62%, Validation loss:1.528, Validation f-score:0.33%\n",
            "Epoch 297: iteration 192/193\n",
            "Epoch 297 --- loss:0.892, Epoch f-score:0.61%, Validation loss:1.392, Validation f-score:0.36%\n",
            "Epoch 298: iteration 192/193\n",
            "Epoch 298 --- loss:0.888, Epoch f-score:0.62%, Validation loss:1.488, Validation f-score:0.32%\n",
            "Epoch 299: iteration 192/193\n",
            "Epoch 299 --- loss:0.891, Epoch f-score:0.61%, Validation loss:1.470, Validation f-score:0.31%\n",
            "Epoch 300: iteration 192/193\n",
            "Epoch 300 --- loss:0.889, Epoch f-score:0.62%, Validation loss:1.577, Validation f-score:0.27%\n",
            "Epoch 301: iteration 192/193\n",
            "Epoch 301 --- loss:0.886, Epoch f-score:0.62%, Validation loss:1.478, Validation f-score:0.29%\n",
            "Epoch 302: iteration 192/193\n",
            "Epoch 302 --- loss:0.887, Epoch f-score:0.62%, Validation loss:1.362, Validation f-score:0.35%\n",
            "Epoch 303: iteration 192/193\n",
            "Epoch 303 --- loss:0.881, Epoch f-score:0.62%, Validation loss:1.396, Validation f-score:0.37%\n",
            "Epoch 304: iteration 192/193\n",
            "Epoch 304 --- loss:0.891, Epoch f-score:0.63%, Validation loss:1.560, Validation f-score:0.28%\n",
            "Epoch 305: iteration 192/193\n",
            "Epoch 305 --- loss:0.894, Epoch f-score:0.61%, Validation loss:1.382, Validation f-score:0.37%\n",
            "Epoch 306: iteration 192/193\n",
            "Epoch 306 --- loss:0.880, Epoch f-score:0.63%, Validation loss:1.444, Validation f-score:0.33%\n",
            "Epoch 307: iteration 192/193\n",
            "Epoch 307 --- loss:0.886, Epoch f-score:0.62%, Validation loss:1.446, Validation f-score:0.32%\n",
            "Epoch 308: iteration 192/193\n",
            "Epoch 308 --- loss:0.882, Epoch f-score:0.62%, Validation loss:1.525, Validation f-score:0.28%\n",
            "Epoch 309: iteration 192/193\n",
            "Epoch 309 --- loss:0.880, Epoch f-score:0.63%, Validation loss:1.535, Validation f-score:0.33%\n",
            "Epoch 310: iteration 192/193\n",
            "Epoch 310 --- loss:0.882, Epoch f-score:0.62%, Validation loss:1.470, Validation f-score:0.34%\n",
            "Epoch 311: iteration 192/193\n",
            "Epoch 311 --- loss:0.882, Epoch f-score:0.63%, Validation loss:1.418, Validation f-score:0.34%\n",
            "Epoch 312: iteration 192/193\n",
            "Epoch 312 --- loss:0.880, Epoch f-score:0.62%, Validation loss:1.571, Validation f-score:0.28%\n",
            "Epoch 313: iteration 192/193\n",
            "Epoch 313 --- loss:0.884, Epoch f-score:0.62%, Validation loss:1.555, Validation f-score:0.30%\n",
            "Epoch 314: iteration 192/193\n",
            "Epoch 314 --- loss:0.881, Epoch f-score:0.62%, Validation loss:1.552, Validation f-score:0.30%\n",
            "Epoch 315: iteration 192/193\n",
            "Epoch 315 --- loss:0.879, Epoch f-score:0.63%, Validation loss:1.550, Validation f-score:0.28%\n",
            "Epoch 316: iteration 192/193\n",
            "Epoch 316 --- loss:0.877, Epoch f-score:0.62%, Validation loss:1.470, Validation f-score:0.32%\n",
            "Epoch 317: iteration 192/193\n",
            "Epoch 317 --- loss:0.885, Epoch f-score:0.62%, Validation loss:1.378, Validation f-score:0.38%\n",
            "Epoch 318: iteration 192/193\n",
            "Epoch 318 --- loss:0.882, Epoch f-score:0.62%, Validation loss:1.468, Validation f-score:0.33%\n",
            "Epoch 319: iteration 192/193\n",
            "Epoch 319 --- loss:0.872, Epoch f-score:0.63%, Validation loss:1.415, Validation f-score:0.37%\n",
            "Epoch 320: iteration 192/193\n",
            "Epoch 320 --- loss:0.873, Epoch f-score:0.62%, Validation loss:1.439, Validation f-score:0.35%\n",
            "Epoch 321: iteration 192/193\n",
            "Epoch 321 --- loss:0.884, Epoch f-score:0.62%, Validation loss:1.493, Validation f-score:0.30%\n",
            "Epoch 322: iteration 192/193\n",
            "Epoch 322 --- loss:0.873, Epoch f-score:0.63%, Validation loss:1.458, Validation f-score:0.31%\n",
            "Epoch 323: iteration 192/193\n",
            "Epoch 323 --- loss:0.877, Epoch f-score:0.62%, Validation loss:1.528, Validation f-score:0.28%\n",
            "Epoch 324: iteration 192/193\n",
            "Epoch 324 --- loss:0.882, Epoch f-score:0.63%, Validation loss:1.466, Validation f-score:0.33%\n",
            "Epoch 325: iteration 192/193\n",
            "Epoch 325 --- loss:0.869, Epoch f-score:0.63%, Validation loss:1.363, Validation f-score:0.35%\n",
            "Epoch 326: iteration 192/193\n",
            "Epoch 326 --- loss:0.876, Epoch f-score:0.63%, Validation loss:1.361, Validation f-score:0.38%\n",
            "Epoch 327: iteration 192/193\n",
            "Epoch 327 --- loss:0.877, Epoch f-score:0.62%, Validation loss:1.399, Validation f-score:0.37%\n",
            "Epoch 328: iteration 192/193\n",
            "Epoch 328 --- loss:0.867, Epoch f-score:0.63%, Validation loss:1.534, Validation f-score:0.32%\n",
            "Epoch 329: iteration 192/193\n",
            "Epoch 329 --- loss:0.877, Epoch f-score:0.63%, Validation loss:1.407, Validation f-score:0.36%\n",
            "Epoch 330: iteration 192/193\n",
            "Epoch 330 --- loss:0.871, Epoch f-score:0.63%, Validation loss:1.561, Validation f-score:0.32%\n",
            "Epoch 331: iteration 192/193\n",
            "Epoch 331 --- loss:0.875, Epoch f-score:0.62%, Validation loss:1.520, Validation f-score:0.29%\n",
            "Epoch 332: iteration 192/193\n",
            "Epoch 332 --- loss:0.855, Epoch f-score:0.64%, Validation loss:1.564, Validation f-score:0.28%\n",
            "Epoch 333: iteration 192/193\n",
            "Epoch 333 --- loss:0.878, Epoch f-score:0.63%, Validation loss:1.399, Validation f-score:0.37%\n",
            "Epoch 334: iteration 192/193\n",
            "Epoch 334 --- loss:0.885, Epoch f-score:0.62%, Validation loss:1.535, Validation f-score:0.28%\n",
            "Epoch 335: iteration 192/193\n",
            "Epoch 335 --- loss:0.875, Epoch f-score:0.63%, Validation loss:1.383, Validation f-score:0.37%\n",
            "Epoch 336: iteration 192/193\n",
            "Epoch 336 --- loss:0.867, Epoch f-score:0.63%, Validation loss:1.466, Validation f-score:0.34%\n",
            "Epoch 337: iteration 192/193\n",
            "Epoch 337 --- loss:0.874, Epoch f-score:0.63%, Validation loss:1.458, Validation f-score:0.32%\n",
            "Epoch 338: iteration 192/193\n",
            "Epoch 338 --- loss:0.859, Epoch f-score:0.64%, Validation loss:1.555, Validation f-score:0.32%\n",
            "Epoch 339: iteration 192/193\n",
            "Epoch 339 --- loss:0.867, Epoch f-score:0.63%, Validation loss:1.514, Validation f-score:0.31%\n",
            "Epoch 340: iteration 192/193\n",
            "Epoch 340 --- loss:0.884, Epoch f-score:0.63%, Validation loss:1.415, Validation f-score:0.35%\n",
            "Epoch 341: iteration 192/193\n",
            "Epoch 341 --- loss:0.873, Epoch f-score:0.63%, Validation loss:1.521, Validation f-score:0.30%\n",
            "Epoch 342: iteration 192/193\n",
            "Epoch 342 --- loss:0.867, Epoch f-score:0.63%, Validation loss:1.356, Validation f-score:0.39%\n",
            "Epoch 343: iteration 192/193\n",
            "Epoch 343 --- loss:0.860, Epoch f-score:0.63%, Validation loss:1.441, Validation f-score:0.34%\n",
            "Epoch 344: iteration 192/193\n",
            "Epoch 344 --- loss:0.878, Epoch f-score:0.63%, Validation loss:1.320, Validation f-score:0.40%\n",
            "Epoch 345: iteration 192/193\n",
            "Epoch 345 --- loss:0.874, Epoch f-score:0.62%, Validation loss:1.521, Validation f-score:0.29%\n",
            "Epoch 346: iteration 192/193\n",
            "Epoch 346 --- loss:0.870, Epoch f-score:0.64%, Validation loss:1.529, Validation f-score:0.30%\n",
            "Epoch 347: iteration 192/193\n",
            "Epoch 347 --- loss:0.866, Epoch f-score:0.63%, Validation loss:1.409, Validation f-score:0.36%\n",
            "Epoch 348: iteration 192/193\n",
            "Epoch 348 --- loss:0.870, Epoch f-score:0.63%, Validation loss:1.481, Validation f-score:0.31%\n",
            "Epoch 349: iteration 192/193\n",
            "Epoch 349 --- loss:0.871, Epoch f-score:0.63%, Validation loss:1.448, Validation f-score:0.32%\n",
            "Epoch 350: iteration 192/193\n",
            "Epoch 350 --- loss:0.864, Epoch f-score:0.63%, Validation loss:1.467, Validation f-score:0.31%\n",
            "Epoch 351: iteration 192/193\n",
            "Epoch 351 --- loss:0.877, Epoch f-score:0.63%, Validation loss:1.482, Validation f-score:0.31%\n",
            "Epoch 352: iteration 192/193\n",
            "Epoch 352 --- loss:0.873, Epoch f-score:0.63%, Validation loss:1.377, Validation f-score:0.37%\n",
            "Epoch 353: iteration 192/193\n",
            "Epoch 353 --- loss:0.858, Epoch f-score:0.64%, Validation loss:1.381, Validation f-score:0.36%\n",
            "Epoch 354: iteration 192/193\n",
            "Epoch 354 --- loss:0.868, Epoch f-score:0.63%, Validation loss:1.440, Validation f-score:0.33%\n",
            "Epoch 355: iteration 192/193\n",
            "Epoch 355 --- loss:0.869, Epoch f-score:0.63%, Validation loss:1.560, Validation f-score:0.30%\n",
            "Epoch 356: iteration 192/193\n",
            "Epoch 356 --- loss:0.868, Epoch f-score:0.63%, Validation loss:1.539, Validation f-score:0.34%\n",
            "Epoch 357: iteration 192/193\n",
            "Epoch 357 --- loss:0.858, Epoch f-score:0.63%, Validation loss:1.493, Validation f-score:0.32%\n",
            "Epoch 358: iteration 192/193\n",
            "Epoch 358 --- loss:0.872, Epoch f-score:0.63%, Validation loss:1.384, Validation f-score:0.36%\n",
            "Epoch 359: iteration 192/193\n",
            "Epoch 359 --- loss:0.876, Epoch f-score:0.63%, Validation loss:1.486, Validation f-score:0.30%\n",
            "Epoch 360: iteration 192/193\n",
            "Epoch 360 --- loss:0.865, Epoch f-score:0.63%, Validation loss:1.498, Validation f-score:0.35%\n",
            "Epoch 361: iteration 192/193\n",
            "Epoch 361 --- loss:0.862, Epoch f-score:0.64%, Validation loss:1.326, Validation f-score:0.37%\n",
            "Epoch 362: iteration 192/193\n",
            "Epoch 362 --- loss:0.869, Epoch f-score:0.63%, Validation loss:1.482, Validation f-score:0.33%\n",
            "Epoch 363: iteration 192/193\n",
            "Epoch 363 --- loss:0.859, Epoch f-score:0.64%, Validation loss:1.480, Validation f-score:0.31%\n",
            "Epoch 364: iteration 192/193\n",
            "Epoch 364 --- loss:0.863, Epoch f-score:0.64%, Validation loss:1.553, Validation f-score:0.27%\n",
            "Epoch 365: iteration 192/193\n",
            "Epoch 365 --- loss:0.850, Epoch f-score:0.64%, Validation loss:1.471, Validation f-score:0.32%\n",
            "Epoch 366: iteration 192/193\n",
            "Epoch 366 --- loss:0.858, Epoch f-score:0.63%, Validation loss:1.605, Validation f-score:0.26%\n",
            "Epoch 367: iteration 192/193\n",
            "Epoch 367 --- loss:0.858, Epoch f-score:0.64%, Validation loss:1.446, Validation f-score:0.35%\n",
            "Epoch 368: iteration 192/193\n",
            "Epoch 368 --- loss:0.872, Epoch f-score:0.63%, Validation loss:1.480, Validation f-score:0.33%\n",
            "Epoch 369: iteration 192/193\n",
            "Epoch 369 --- loss:0.867, Epoch f-score:0.63%, Validation loss:1.449, Validation f-score:0.33%\n",
            "Epoch 370: iteration 192/193\n",
            "Epoch 370 --- loss:0.875, Epoch f-score:0.63%, Validation loss:1.446, Validation f-score:0.34%\n",
            "Epoch 371: iteration 192/193\n",
            "Epoch 371 --- loss:0.863, Epoch f-score:0.63%, Validation loss:1.396, Validation f-score:0.37%\n",
            "Epoch 372: iteration 192/193\n",
            "Epoch 372 --- loss:0.870, Epoch f-score:0.63%, Validation loss:1.491, Validation f-score:0.31%\n",
            "Epoch 373: iteration 192/193\n",
            "Epoch 373 --- loss:0.868, Epoch f-score:0.63%, Validation loss:1.633, Validation f-score:0.30%\n",
            "Epoch 374: iteration 192/193\n",
            "Epoch 374 --- loss:0.875, Epoch f-score:0.63%, Validation loss:1.447, Validation f-score:0.33%\n",
            "Epoch 375: iteration 192/193\n",
            "Epoch 375 --- loss:0.863, Epoch f-score:0.63%, Validation loss:1.374, Validation f-score:0.36%\n",
            "Epoch 376: iteration 192/193\n",
            "Epoch 376 --- loss:0.852, Epoch f-score:0.64%, Validation loss:1.461, Validation f-score:0.32%\n",
            "Epoch 377: iteration 192/193\n",
            "Epoch 377 --- loss:0.877, Epoch f-score:0.63%, Validation loss:1.571, Validation f-score:0.28%\n",
            "Epoch 378: iteration 192/193\n",
            "Epoch 378 --- loss:0.852, Epoch f-score:0.64%, Validation loss:1.517, Validation f-score:0.32%\n",
            "Epoch 379: iteration 192/193\n",
            "Epoch 379 --- loss:0.871, Epoch f-score:0.63%, Validation loss:1.505, Validation f-score:0.31%\n",
            "Epoch 380: iteration 192/193\n",
            "Epoch 380 --- loss:0.853, Epoch f-score:0.64%, Validation loss:1.555, Validation f-score:0.30%\n",
            "Epoch 381: iteration 192/193\n",
            "Epoch 381 --- loss:0.861, Epoch f-score:0.64%, Validation loss:1.558, Validation f-score:0.29%\n",
            "Epoch 382: iteration 192/193\n",
            "Epoch 382 --- loss:0.863, Epoch f-score:0.63%, Validation loss:1.557, Validation f-score:0.31%\n",
            "Epoch 383: iteration 192/193\n",
            "Epoch 383 --- loss:0.869, Epoch f-score:0.63%, Validation loss:1.625, Validation f-score:0.28%\n",
            "Epoch 384: iteration 192/193\n",
            "Epoch 384 --- loss:0.862, Epoch f-score:0.63%, Validation loss:1.551, Validation f-score:0.29%\n",
            "Epoch 385: iteration 192/193\n",
            "Epoch 385 --- loss:0.857, Epoch f-score:0.64%, Validation loss:1.383, Validation f-score:0.36%\n",
            "Epoch 386: iteration 192/193\n",
            "Epoch 386 --- loss:0.856, Epoch f-score:0.63%, Validation loss:1.342, Validation f-score:0.38%\n",
            "Epoch 387: iteration 192/193\n",
            "Epoch 387 --- loss:0.861, Epoch f-score:0.63%, Validation loss:1.472, Validation f-score:0.32%\n",
            "Epoch 388: iteration 192/193\n",
            "Epoch 388 --- loss:0.860, Epoch f-score:0.63%, Validation loss:1.435, Validation f-score:0.33%\n",
            "Epoch 389: iteration 192/193\n",
            "Epoch 389 --- loss:0.852, Epoch f-score:0.63%, Validation loss:1.452, Validation f-score:0.33%\n",
            "Epoch 390: iteration 192/193\n",
            "Epoch 390 --- loss:0.866, Epoch f-score:0.63%, Validation loss:1.416, Validation f-score:0.36%\n",
            "Epoch 391: iteration 192/193\n",
            "Epoch 391 --- loss:0.860, Epoch f-score:0.64%, Validation loss:1.335, Validation f-score:0.38%\n",
            "Epoch 392: iteration 192/193\n",
            "Epoch 392 --- loss:0.876, Epoch f-score:0.63%, Validation loss:1.463, Validation f-score:0.34%\n",
            "Epoch 393: iteration 192/193\n",
            "Epoch 393 --- loss:0.847, Epoch f-score:0.64%, Validation loss:1.535, Validation f-score:0.31%\n",
            "Epoch 394: iteration 192/193\n",
            "Epoch 394 --- loss:0.852, Epoch f-score:0.63%, Validation loss:1.398, Validation f-score:0.34%\n",
            "Epoch 395: iteration 192/193\n",
            "Epoch 395 --- loss:0.866, Epoch f-score:0.63%, Validation loss:1.459, Validation f-score:0.32%\n",
            "Epoch 396: iteration 192/193\n",
            "Epoch 396 --- loss:0.855, Epoch f-score:0.63%, Validation loss:1.438, Validation f-score:0.34%\n",
            "Epoch 397: iteration 192/193\n",
            "Epoch 397 --- loss:0.860, Epoch f-score:0.62%, Validation loss:1.496, Validation f-score:0.33%\n",
            "Epoch 398: iteration 192/193\n",
            "Epoch 398 --- loss:0.856, Epoch f-score:0.64%, Validation loss:1.560, Validation f-score:0.32%\n",
            "Epoch 399: iteration 192/193\n",
            "Epoch 399 --- loss:0.852, Epoch f-score:0.64%, Validation loss:1.526, Validation f-score:0.30%\n",
            "Epoch 400: iteration 192/193\n",
            "Epoch 400 --- loss:0.855, Epoch f-score:0.64%, Validation loss:1.437, Validation f-score:0.33%\n",
            "Epoch 401: iteration 192/193\n",
            "Epoch 401 --- loss:0.861, Epoch f-score:0.63%, Validation loss:1.420, Validation f-score:0.36%\n",
            "Epoch 402: iteration 192/193\n",
            "Epoch 402 --- loss:0.854, Epoch f-score:0.63%, Validation loss:1.395, Validation f-score:0.37%\n",
            "Epoch 403: iteration 192/193\n",
            "Epoch 403 --- loss:0.855, Epoch f-score:0.63%, Validation loss:1.590, Validation f-score:0.29%\n",
            "Epoch 404: iteration 192/193\n",
            "Epoch 404 --- loss:0.853, Epoch f-score:0.64%, Validation loss:1.464, Validation f-score:0.32%\n",
            "Epoch 405: iteration 192/193\n",
            "Epoch 405 --- loss:0.863, Epoch f-score:0.63%, Validation loss:1.484, Validation f-score:0.33%\n",
            "Epoch 406: iteration 192/193\n",
            "Epoch 406 --- loss:0.861, Epoch f-score:0.63%, Validation loss:1.491, Validation f-score:0.32%\n",
            "Epoch 407: iteration 192/193\n",
            "Epoch 407 --- loss:0.858, Epoch f-score:0.64%, Validation loss:1.424, Validation f-score:0.33%\n",
            "Epoch 408: iteration 192/193\n",
            "Epoch 408 --- loss:0.853, Epoch f-score:0.64%, Validation loss:1.514, Validation f-score:0.30%\n",
            "Epoch 409: iteration 192/193\n",
            "Epoch 409 --- loss:0.847, Epoch f-score:0.64%, Validation loss:1.524, Validation f-score:0.29%\n",
            "Epoch 410: iteration 192/193\n",
            "Epoch 410 --- loss:0.858, Epoch f-score:0.64%, Validation loss:1.416, Validation f-score:0.36%\n",
            "Epoch 411: iteration 192/193\n",
            "Epoch 411 --- loss:0.855, Epoch f-score:0.63%, Validation loss:1.476, Validation f-score:0.35%\n",
            "Epoch 412: iteration 192/193\n",
            "Epoch 412 --- loss:0.857, Epoch f-score:0.64%, Validation loss:1.547, Validation f-score:0.30%\n",
            "Epoch 413: iteration 192/193\n",
            "Epoch 413 --- loss:0.858, Epoch f-score:0.63%, Validation loss:1.499, Validation f-score:0.31%\n",
            "Epoch 414: iteration 192/193\n",
            "Epoch 414 --- loss:0.854, Epoch f-score:0.64%, Validation loss:1.467, Validation f-score:0.30%\n",
            "Epoch 415: iteration 192/193\n",
            "Epoch 415 --- loss:0.853, Epoch f-score:0.63%, Validation loss:1.445, Validation f-score:0.35%\n",
            "Epoch 416: iteration 192/193\n",
            "Epoch 416 --- loss:0.852, Epoch f-score:0.64%, Validation loss:1.362, Validation f-score:0.38%\n",
            "Epoch 417: iteration 192/193\n",
            "Epoch 417 --- loss:0.856, Epoch f-score:0.64%, Validation loss:1.512, Validation f-score:0.32%\n",
            "Epoch 418: iteration 192/193\n",
            "Epoch 418 --- loss:0.850, Epoch f-score:0.64%, Validation loss:1.504, Validation f-score:0.30%\n",
            "Epoch 419: iteration 192/193\n",
            "Epoch 419 --- loss:0.844, Epoch f-score:0.64%, Validation loss:1.475, Validation f-score:0.31%\n",
            "Epoch 420: iteration 192/193\n",
            "Epoch 420 --- loss:0.843, Epoch f-score:0.64%, Validation loss:1.483, Validation f-score:0.32%\n",
            "Epoch 421: iteration 192/193\n",
            "Epoch 421 --- loss:0.845, Epoch f-score:0.64%, Validation loss:1.479, Validation f-score:0.34%\n",
            "Epoch 422: iteration 192/193\n",
            "Epoch 422 --- loss:0.851, Epoch f-score:0.64%, Validation loss:1.484, Validation f-score:0.33%\n",
            "Epoch 423: iteration 192/193\n",
            "Epoch 423 --- loss:0.857, Epoch f-score:0.63%, Validation loss:1.487, Validation f-score:0.32%\n",
            "Epoch 424: iteration 192/193\n",
            "Epoch 424 --- loss:0.848, Epoch f-score:0.65%, Validation loss:1.529, Validation f-score:0.30%\n",
            "Epoch 425: iteration 192/193\n",
            "Epoch 425 --- loss:0.857, Epoch f-score:0.64%, Validation loss:1.454, Validation f-score:0.34%\n",
            "Epoch 426: iteration 192/193\n",
            "Epoch 426 --- loss:0.846, Epoch f-score:0.64%, Validation loss:1.462, Validation f-score:0.34%\n",
            "Epoch 427: iteration 192/193\n",
            "Epoch 427 --- loss:0.860, Epoch f-score:0.64%, Validation loss:1.474, Validation f-score:0.33%\n",
            "Epoch 428: iteration 192/193\n",
            "Epoch 428 --- loss:0.857, Epoch f-score:0.64%, Validation loss:1.395, Validation f-score:0.35%\n",
            "Epoch 429: iteration 192/193\n",
            "Epoch 429 --- loss:0.854, Epoch f-score:0.64%, Validation loss:1.357, Validation f-score:0.37%\n",
            "Epoch 430: iteration 192/193\n",
            "Epoch 430 --- loss:0.847, Epoch f-score:0.64%, Validation loss:1.482, Validation f-score:0.33%\n",
            "Epoch 431: iteration 192/193\n",
            "Epoch 431 --- loss:0.836, Epoch f-score:0.64%, Validation loss:1.498, Validation f-score:0.33%\n",
            "Epoch 432: iteration 192/193\n",
            "Epoch 432 --- loss:0.857, Epoch f-score:0.62%, Validation loss:1.460, Validation f-score:0.34%\n",
            "Epoch 433: iteration 192/193\n",
            "Epoch 433 --- loss:0.846, Epoch f-score:0.65%, Validation loss:1.417, Validation f-score:0.37%\n",
            "Epoch 434: iteration 192/193\n",
            "Epoch 434 --- loss:0.846, Epoch f-score:0.64%, Validation loss:1.535, Validation f-score:0.32%\n",
            "Epoch 435: iteration 192/193\n",
            "Epoch 435 --- loss:0.852, Epoch f-score:0.64%, Validation loss:1.604, Validation f-score:0.28%\n",
            "Epoch 436: iteration 192/193\n",
            "Epoch 436 --- loss:0.841, Epoch f-score:0.64%, Validation loss:1.522, Validation f-score:0.32%\n",
            "Epoch 437: iteration 192/193\n",
            "Epoch 437 --- loss:0.834, Epoch f-score:0.65%, Validation loss:1.485, Validation f-score:0.32%\n",
            "Epoch 438: iteration 192/193\n",
            "Epoch 438 --- loss:0.861, Epoch f-score:0.64%, Validation loss:1.523, Validation f-score:0.30%\n",
            "Epoch 439: iteration 192/193\n",
            "Epoch 439 --- loss:0.844, Epoch f-score:0.64%, Validation loss:1.488, Validation f-score:0.29%\n",
            "Epoch 440: iteration 192/193\n",
            "Epoch 440 --- loss:0.852, Epoch f-score:0.64%, Validation loss:1.488, Validation f-score:0.33%\n",
            "Epoch 441: iteration 192/193\n",
            "Epoch 441 --- loss:0.841, Epoch f-score:0.64%, Validation loss:1.420, Validation f-score:0.35%\n",
            "Epoch 442: iteration 192/193\n",
            "Epoch 442 --- loss:0.859, Epoch f-score:0.64%, Validation loss:1.566, Validation f-score:0.29%\n",
            "Epoch 443: iteration 192/193\n",
            "Epoch 443 --- loss:0.833, Epoch f-score:0.65%, Validation loss:1.463, Validation f-score:0.35%\n",
            "Epoch 444: iteration 192/193\n",
            "Epoch 444 --- loss:0.833, Epoch f-score:0.65%, Validation loss:1.474, Validation f-score:0.34%\n",
            "Epoch 445: iteration 192/193\n",
            "Epoch 445 --- loss:0.857, Epoch f-score:0.64%, Validation loss:1.464, Validation f-score:0.32%\n",
            "Epoch 446: iteration 192/193\n",
            "Epoch 446 --- loss:0.851, Epoch f-score:0.64%, Validation loss:1.501, Validation f-score:0.30%\n",
            "Epoch 447: iteration 192/193\n",
            "Epoch 447 --- loss:0.855, Epoch f-score:0.64%, Validation loss:1.473, Validation f-score:0.31%\n",
            "Epoch 448: iteration 192/193\n",
            "Epoch 448 --- loss:0.842, Epoch f-score:0.64%, Validation loss:1.426, Validation f-score:0.36%\n",
            "Epoch 449: iteration 192/193\n",
            "Epoch 449 --- loss:0.842, Epoch f-score:0.65%, Validation loss:1.450, Validation f-score:0.34%\n",
            "Epoch 450: iteration 192/193\n",
            "Epoch 450 --- loss:0.852, Epoch f-score:0.63%, Validation loss:1.495, Validation f-score:0.31%\n",
            "Epoch 451: iteration 192/193\n",
            "Epoch 451 --- loss:0.846, Epoch f-score:0.65%, Validation loss:1.454, Validation f-score:0.34%\n",
            "Epoch 452: iteration 192/193\n",
            "Epoch 452 --- loss:0.854, Epoch f-score:0.64%, Validation loss:1.394, Validation f-score:0.36%\n",
            "Epoch 453: iteration 192/193\n",
            "Epoch 453 --- loss:0.848, Epoch f-score:0.64%, Validation loss:1.569, Validation f-score:0.29%\n",
            "Epoch 454: iteration 192/193\n",
            "Epoch 454 --- loss:0.854, Epoch f-score:0.64%, Validation loss:1.551, Validation f-score:0.31%\n",
            "Epoch 455: iteration 192/193\n",
            "Epoch 455 --- loss:0.842, Epoch f-score:0.64%, Validation loss:1.373, Validation f-score:0.36%\n",
            "Epoch 456: iteration 192/193\n",
            "Epoch 456 --- loss:0.854, Epoch f-score:0.64%, Validation loss:1.519, Validation f-score:0.30%\n",
            "Epoch 457: iteration 192/193\n",
            "Epoch 457 --- loss:0.848, Epoch f-score:0.64%, Validation loss:1.526, Validation f-score:0.32%\n",
            "Epoch 458: iteration 192/193\n",
            "Epoch 458 --- loss:0.844, Epoch f-score:0.64%, Validation loss:1.491, Validation f-score:0.33%\n",
            "Epoch 459: iteration 192/193\n",
            "Epoch 459 --- loss:0.833, Epoch f-score:0.65%, Validation loss:1.542, Validation f-score:0.29%\n",
            "Epoch 460: iteration 192/193\n",
            "Epoch 460 --- loss:0.852, Epoch f-score:0.64%, Validation loss:1.409, Validation f-score:0.36%\n",
            "Epoch 461: iteration 192/193\n",
            "Epoch 461 --- loss:0.850, Epoch f-score:0.64%, Validation loss:1.466, Validation f-score:0.33%\n",
            "Epoch 462: iteration 192/193\n",
            "Epoch 462 --- loss:0.844, Epoch f-score:0.64%, Validation loss:1.498, Validation f-score:0.32%\n",
            "Epoch 463: iteration 192/193\n",
            "Epoch 463 --- loss:0.848, Epoch f-score:0.63%, Validation loss:1.451, Validation f-score:0.35%\n",
            "Epoch 464: iteration 192/193\n",
            "Epoch 464 --- loss:0.853, Epoch f-score:0.64%, Validation loss:1.542, Validation f-score:0.30%\n",
            "Epoch 465: iteration 192/193\n",
            "Epoch 465 --- loss:0.845, Epoch f-score:0.64%, Validation loss:1.564, Validation f-score:0.30%\n",
            "Epoch 466: iteration 192/193\n",
            "Epoch 466 --- loss:0.836, Epoch f-score:0.65%, Validation loss:1.583, Validation f-score:0.26%\n",
            "Epoch 467: iteration 192/193\n",
            "Epoch 467 --- loss:0.850, Epoch f-score:0.64%, Validation loss:1.441, Validation f-score:0.33%\n",
            "Epoch 468: iteration 192/193\n",
            "Epoch 468 --- loss:0.845, Epoch f-score:0.64%, Validation loss:1.437, Validation f-score:0.34%\n",
            "Epoch 469: iteration 192/193\n",
            "Epoch 469 --- loss:0.848, Epoch f-score:0.63%, Validation loss:1.486, Validation f-score:0.31%\n",
            "Epoch 470: iteration 192/193\n",
            "Epoch 470 --- loss:0.859, Epoch f-score:0.64%, Validation loss:1.474, Validation f-score:0.30%\n",
            "Epoch 471: iteration 192/193\n",
            "Epoch 471 --- loss:0.846, Epoch f-score:0.64%, Validation loss:1.564, Validation f-score:0.28%\n",
            "Epoch 472: iteration 192/193\n",
            "Epoch 472 --- loss:0.833, Epoch f-score:0.65%, Validation loss:1.447, Validation f-score:0.34%\n",
            "Epoch 473: iteration 192/193\n",
            "Epoch 473 --- loss:0.832, Epoch f-score:0.65%, Validation loss:1.457, Validation f-score:0.32%\n",
            "Epoch 474: iteration 192/193\n",
            "Epoch 474 --- loss:0.837, Epoch f-score:0.65%, Validation loss:1.489, Validation f-score:0.32%\n",
            "Epoch 475: iteration 192/193\n",
            "Epoch 475 --- loss:0.845, Epoch f-score:0.64%, Validation loss:1.554, Validation f-score:0.29%\n",
            "Epoch 476: iteration 192/193\n",
            "Epoch 476 --- loss:0.836, Epoch f-score:0.65%, Validation loss:1.529, Validation f-score:0.32%\n",
            "Epoch 477: iteration 192/193\n",
            "Epoch 477 --- loss:0.833, Epoch f-score:0.65%, Validation loss:1.518, Validation f-score:0.32%\n",
            "Epoch 478: iteration 192/193\n",
            "Epoch 478 --- loss:0.847, Epoch f-score:0.64%, Validation loss:1.432, Validation f-score:0.35%\n",
            "Epoch 479: iteration 192/193\n",
            "Epoch 479 --- loss:0.837, Epoch f-score:0.65%, Validation loss:1.512, Validation f-score:0.30%\n",
            "Epoch 480: iteration 192/193\n",
            "Epoch 480 --- loss:0.854, Epoch f-score:0.64%, Validation loss:1.441, Validation f-score:0.36%\n",
            "Epoch 481: iteration 192/193\n",
            "Epoch 481 --- loss:0.846, Epoch f-score:0.64%, Validation loss:1.526, Validation f-score:0.33%\n",
            "Epoch 482: iteration 192/193\n",
            "Epoch 482 --- loss:0.851, Epoch f-score:0.64%, Validation loss:1.338, Validation f-score:0.40%\n",
            "Epoch 483: iteration 192/193\n",
            "Epoch 483 --- loss:0.844, Epoch f-score:0.64%, Validation loss:1.435, Validation f-score:0.34%\n",
            "Epoch 484: iteration 192/193\n",
            "Epoch 484 --- loss:0.830, Epoch f-score:0.65%, Validation loss:1.401, Validation f-score:0.37%\n",
            "Epoch 485: iteration 192/193\n",
            "Epoch 485 --- loss:0.845, Epoch f-score:0.64%, Validation loss:1.463, Validation f-score:0.32%\n",
            "Epoch 486: iteration 192/193\n",
            "Epoch 486 --- loss:0.841, Epoch f-score:0.64%, Validation loss:1.488, Validation f-score:0.33%\n",
            "Epoch 487: iteration 192/193\n",
            "Epoch 487 --- loss:0.837, Epoch f-score:0.65%, Validation loss:1.419, Validation f-score:0.36%\n",
            "Epoch 488: iteration 192/193\n",
            "Epoch 488 --- loss:0.851, Epoch f-score:0.64%, Validation loss:1.421, Validation f-score:0.33%\n",
            "Epoch 489: iteration 192/193\n",
            "Epoch 489 --- loss:0.843, Epoch f-score:0.64%, Validation loss:1.440, Validation f-score:0.33%\n",
            "Epoch 490: iteration 192/193\n",
            "Epoch 490 --- loss:0.839, Epoch f-score:0.65%, Validation loss:1.504, Validation f-score:0.35%\n",
            "Epoch 491: iteration 192/193\n",
            "Epoch 491 --- loss:0.847, Epoch f-score:0.64%, Validation loss:1.483, Validation f-score:0.31%\n",
            "Epoch 492: iteration 192/193\n",
            "Epoch 492 --- loss:0.858, Epoch f-score:0.64%, Validation loss:1.593, Validation f-score:0.31%\n",
            "Epoch 493: iteration 192/193\n",
            "Epoch 493 --- loss:0.837, Epoch f-score:0.65%, Validation loss:1.341, Validation f-score:0.39%\n",
            "Epoch 494: iteration 192/193\n",
            "Epoch 494 --- loss:0.843, Epoch f-score:0.64%, Validation loss:1.500, Validation f-score:0.32%\n",
            "Epoch 495: iteration 192/193\n",
            "Epoch 495 --- loss:0.841, Epoch f-score:0.65%, Validation loss:1.525, Validation f-score:0.28%\n",
            "Epoch 496: iteration 192/193\n",
            "Epoch 496 --- loss:0.839, Epoch f-score:0.65%, Validation loss:1.313, Validation f-score:0.40%\n",
            "Epoch 497: iteration 192/193\n",
            "Epoch 497 --- loss:0.845, Epoch f-score:0.64%, Validation loss:1.466, Validation f-score:0.32%\n",
            "Epoch 498: iteration 192/193\n",
            "Epoch 498 --- loss:0.826, Epoch f-score:0.65%, Validation loss:1.613, Validation f-score:0.24%\n",
            "Epoch 499: iteration 192/193\n",
            "Epoch 499 --- loss:0.844, Epoch f-score:0.64%, Validation loss:1.408, Validation f-score:0.34%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "root_path = Path('/gdrive/Shareddrives/Dion-Account/2122WS/8-dl4slp/coding-project/data/ser'); root_path.ls()"
      ],
      "metadata": {
        "id": "_4jC45ga93mW",
        "outputId": "e5d7abe1-5018-4982-92c6-e1ad7e0a0eb9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[PosixPath('/gdrive/Shareddrives/Dion-Account/2122WS/8-dl4slp/coding-project/data/ser/v0'),\n",
              " PosixPath('/gdrive/Shareddrives/Dion-Account/2122WS/8-dl4slp/coding-project/data/ser/v1'),\n",
              " PosixPath('/gdrive/Shareddrives/Dion-Account/2122WS/8-dl4slp/coding-project/data/ser/v2.1'),\n",
              " PosixPath('/gdrive/Shareddrives/Dion-Account/2122WS/8-dl4slp/coding-project/data/ser/ser-data-v2-1.tar.gz')]"
            ]
          },
          "metadata": {},
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "root_path.parent.parent.ls()"
      ],
      "metadata": {
        "id": "wMlWELP3DzbH",
        "outputId": "2668c1e4-9763-4806-ee3f-bcfb208ce50c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[PosixPath('/gdrive/Shareddrives/Dion-Account/2122WS/8-dl4slp/coding-project/NLU-dl.ipynb'),\n",
              " PosixPath('/gdrive/Shareddrives/Dion-Account/2122WS/8-dl4slp/coding-project/data'),\n",
              " PosixPath('/gdrive/Shareddrives/Dion-Account/2122WS/8-dl4slp/coding-project/dl4slp-playground.ipynb'),\n",
              " PosixPath('/gdrive/Shareddrives/Dion-Account/2122WS/8-dl4slp/coding-project/dl4slp-audio.ipynb'),\n",
              " PosixPath('/gdrive/Shareddrives/Dion-Account/2122WS/8-dl4slp/coding-project/result-prediction'),\n",
              " PosixPath('/gdrive/Shareddrives/Dion-Account/2122WS/8-dl4slp/coding-project/nlu'),\n",
              " PosixPath('/gdrive/Shareddrives/Dion-Account/2122WS/8-dl4slp/coding-project/ser')]"
            ]
          },
          "metadata": {},
          "execution_count": 59
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ser_path = root_path.parent.parent/'ser'; ser_path.exists()"
      ],
      "metadata": {
        "id": "MS61IBi4ENzK",
        "outputId": "bc4f4416-baa9-4524-83a9-7de3502cd4ba",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 61
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!cp -R '/content/models/checkpoints'  {ser_path}"
      ],
      "metadata": {
        "id": "drV6wdN0EHeK"
      },
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ls {ser_path}"
      ],
      "metadata": {
        "id": "CrGANTF2EauG",
        "outputId": "a6cc111b-15de-4778-b0cc-23d50814e184",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "checkpoints\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---"
      ],
      "metadata": {
        "id": "k8Gp0Bx_p8b-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Move back to origin framework"
      ],
      "metadata": {
        "id": "s_eMCXpCDu4-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# opt_func = adam_opt(mom=0.9, mom_sqr=0.99, eps=1e-6, wd=1e-2)\n",
        "loss_func = LabelSmoothingCrossEntropy()\n",
        "lr = 1e-2\n",
        "pct_start = 0.5\n",
        "phases = create_phases(pct_start)\n",
        "sched_lr  = combine_scheds(phases, cos_1cycle_anneal(lr/10., lr, lr/1e5))\n",
        "sched_mom = combine_scheds(phases, cos_1cycle_anneal(0.95,0.85, 0.95))\n",
        "cbscheds = [ParamScheduler('lr', sched_lr), \n",
        "            ParamScheduler('mom', sched_mom)]"
      ],
      "metadata": {
        "id": "sDKXz6KZ9MNR"
      },
      "execution_count": 194,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -qq torchmetrics\n",
        "from torchmetrics import F1\n",
        "f1 = F1(num_classes=4)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ao_l1zIz_sMt",
        "outputId": "6c43748f-dedc-4cb6-e363-8a1bd9071150"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l\r\u001b[K     |█                               | 10 kB 20.3 MB/s eta 0:00:01\r\u001b[K     |██                              | 20 kB 17.9 MB/s eta 0:00:01\r\u001b[K     |███                             | 30 kB 10.3 MB/s eta 0:00:01\r\u001b[K     |████                            | 40 kB 8.7 MB/s eta 0:00:01\r\u001b[K     |█████                           | 51 kB 4.8 MB/s eta 0:00:01\r\u001b[K     |██████                          | 61 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |███████                         | 71 kB 5.6 MB/s eta 0:00:01\r\u001b[K     |███████▉                        | 81 kB 6.3 MB/s eta 0:00:01\r\u001b[K     |████████▉                       | 92 kB 6.5 MB/s eta 0:00:01\r\u001b[K     |█████████▉                      | 102 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |██████████▉                     | 112 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |███████████▉                    | 122 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |████████████▉                   | 133 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |█████████████▉                  | 143 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |██████████████▉                 | 153 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |███████████████▊                | 163 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |████████████████▊               | 174 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |█████████████████▊              | 184 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |██████████████████▊             | 194 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |███████████████████▊            | 204 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |████████████████████▊           | 215 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▊          | 225 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▋         | 235 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▋        | 245 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▋       | 256 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▋      | 266 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▋     | 276 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▋    | 286 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▋   | 296 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▋  | 307 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▌ | 317 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▌| 327 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 332 kB 5.1 MB/s \n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import inspect\n",
        "def show_source(m): print(inspect.getsource(m))"
      ],
      "metadata": {
        "id": "nJWKVwZToTek"
      },
      "execution_count": 188,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "show_source(accuracy)"
      ],
      "metadata": {
        "id": "gubWn5s3oYOe",
        "outputId": "4900fcad-29a0-4a85-a9c6-a1e0cad5f5fb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 191,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "def accuracy(out, yb): return (torch.argmax(out, dim=1)==yb).float().mean()\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Learner():\n",
        "    def __init__(self, model, data, loss_func, opt_func=sgd_opt, lr=1e-2, splitter=param_getter,\n",
        "                 cbs=None, cb_funcs=None):\n",
        "        self.model,self.data,self.loss_func,self.opt_func,self.lr,self.splitter = model,data,loss_func,opt_func,lr,splitter\n",
        "        self.in_train,self.logger,self.opt = False,print,None\n",
        "\n",
        "        # NB: Things marked \"NEW\" are covered in lesson 12\n",
        "        # NEW: avoid need for set_runner\n",
        "        self.cbs = []\n",
        "        self.add_cb(TrainEvalCallback())\n",
        "        self.add_cbs(cbs)\n",
        "        self.add_cbs(cbf() for cbf in listify(cb_funcs))\n",
        "\n",
        "    def add_cbs(self, cbs):\n",
        "        for cb in listify(cbs): self.add_cb(cb)\n",
        "\n",
        "    def add_cb(self, cb):\n",
        "        cb.set_runner(self)\n",
        "        setattr(self, cb.name, cb)\n",
        "        self.cbs.append(cb)\n",
        "\n",
        "    def remove_cbs(self, cbs):\n",
        "        for cb in listify(cbs): self.cbs.remove(cb)\n",
        "\n",
        "    def one_batch(self, i, xb, yb):\n",
        "        try:\n",
        "            self.iter = i\n",
        "            self.xb,self.yb = xb,yb;                        self('begin_batch')\n",
        "            self.pred = self.model(self.xb);                self('after_pred')\n",
        "            self.loss = self.loss_func(self.pred, self.yb); self('after_loss')\n",
        "            if not self.in_train: return\n",
        "            self.loss.backward();                           self('after_backward')\n",
        "            self.opt.step();                                self('after_step')\n",
        "            self.opt.zero_grad()\n",
        "        except CancelBatchException:                        self('after_cancel_batch')\n",
        "        finally:                                            self('after_batch')\n",
        "\n",
        "    def all_batches(self):\n",
        "        self.iters = len(self.dl)\n",
        "        try:\n",
        "            for i,(xb,yb) in enumerate(self.dl): self.one_batch(i, xb, yb)\n",
        "        except CancelEpochException: self('after_cancel_epoch')\n",
        "\n",
        "    def do_begin_fit(self, epochs):\n",
        "        self.epochs,self.loss = epochs,tensor(0.)\n",
        "        self('begin_fit')\n",
        "\n",
        "    def do_begin_epoch(self, epoch):\n",
        "        self.epoch,self.dl = epoch,self.data.train_dl\n",
        "        return self('begin_epoch')\n",
        "\n",
        "    def fit(self, epochs, cbs=None, reset_opt=False):\n",
        "        # NEW: pass callbacks to fit() and have them removed when done\n",
        "        self.add_cbs(cbs)\n",
        "        # NEW: create optimizer on fit(), optionally replacing existing\n",
        "        if reset_opt or not self.opt: self.opt = self.opt_func(self.splitter(self.model), lr=self.lr)\n",
        "\n",
        "        try:\n",
        "            self.do_begin_fit(epochs)\n",
        "            for epoch in range(epochs):\n",
        "                self.do_begin_epoch(epoch)\n",
        "                if not self('begin_epoch'): self.all_batches()\n",
        "\n",
        "                with torch.no_grad():\n",
        "                    self.dl = self.data.valid_dl\n",
        "                    if not self('begin_validate'): self.all_batches()\n",
        "                self('after_epoch')\n",
        "\n",
        "        except CancelTrainException: self('after_cancel_train')\n",
        "        finally:\n",
        "            self('after_fit')\n",
        "            self.remove_cbs(cbs)\n",
        "\n",
        "    ALL_CBS = {'begin_batch', 'after_pred', 'after_loss', 'after_backward', 'after_step',\n",
        "        'after_cancel_batch', 'after_batch', 'after_cancel_epoch', 'begin_fit',\n",
        "        'begin_epoch', 'begin_epoch', 'begin_validate', 'after_epoch',\n",
        "        'after_cancel_train', 'after_fit'}\n",
        "\n",
        "    def __call__(self, cb_name):\n",
        "        res = False\n",
        "        assert cb_name in self.ALL_CBS\n",
        "        for cb in sorted(self.cbs, key=lambda x: x._order): res = cb(cb_name) and res\n",
        "        return res\n"
      ],
      "metadata": {
        "id": "989AMfY7oa7G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def learner(arch, data, loss_func, opt_func, c_in=None, c_out=None,\n",
        "                lr=1e-2, cuda=True, norm=None, progress=True, mixup=0, xtra_cb=None, **kwargs):\n",
        "    cbfs = [partial(AvgStatsCallback,accuracy)]+listify(xtra_cb)\n",
        "    if progress: cbfs.append(ProgressCallback)\n",
        "    if cuda:     cbfs.append(CudaCallback)\n",
        "    if norm:     cbfs.append(partial(BatchTransformXCallback, norm))\n",
        "    if mixup:    cbfs.append(partial(MixUp, mixup))\n",
        "    arch_args = {}\n",
        "    if not c_in : c_in  = data.c_in\n",
        "    if not c_out: c_out = data.c_out\n",
        "    if c_in:  arch_args['c_in' ]=c_in\n",
        "    if c_out: arch_args['c_out']=c_out\n",
        "    return Learner(arch, data, loss_func, opt_func=opt_func, lr=lr, cb_funcs=cbfs, **kwargs)\n"
      ],
      "metadata": {
        "id": "GGpNDq0l976E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = Model(c_out).cuda()"
      ],
      "metadata": {
        "id": "uUJTouSuJSDY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def sgd_opt(xtra_step=None, **kwargs):\n",
        "    return partial(Optimizer, steppers=[weight_decay]+listify(xtra_step),\n",
        "                   stats=[AverageGrad(dampening=True), AverageSqrGrad(), StepCount()], **kwargs)"
      ],
      "metadata": {
        "id": "XCly0VbbjebN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "opt_func = sgd_opt(lr=0.01, weight_decay=1e-3, momentum=0.8)"
      ],
      "metadata": {
        "id": "_FePvDsLispY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "learn = learner(model, data, loss_func, opt_func=opt_func)"
      ],
      "metadata": {
        "id": "zwLQiDDK_XhW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "learn.fit(300, cbs=cbscheds)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "hZF4d9AZAmAv",
        "outputId": "1130ef90-441a-44bb-dcf8-9495f6d261a8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "        <style>\n",
              "            /* Turns off some styling */\n",
              "            progress {\n",
              "                /* gets rid of default border in Firefox and Opera. */\n",
              "                border: none;\n",
              "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
              "                background-size: auto;\n",
              "            }\n",
              "            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
              "                background: #F44336;\n",
              "            }\n",
              "        </style>\n",
              "      <progress value='0' class='' max='300' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      \n",
              "    </div>\n",
              "    \n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: left;\">\n",
              "      <th>epoch</th>\n",
              "      <th>train_loss</th>\n",
              "      <th>train_accuracy</th>\n",
              "      <th>valid_loss</th>\n",
              "      <th>valid_accuracy</th>\n",
              "      <th>time</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>0</td>\n",
              "      <td>2.520974</td>\n",
              "      <td>0.268680</td>\n",
              "      <td>1.658805</td>\n",
              "      <td>0.289404</td>\n",
              "      <td>00:10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>2.498805</td>\n",
              "      <td>0.259936</td>\n",
              "      <td>1.663060</td>\n",
              "      <td>0.288079</td>\n",
              "      <td>00:11</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>2.516170</td>\n",
              "      <td>0.267091</td>\n",
              "      <td>1.656659</td>\n",
              "      <td>0.288742</td>\n",
              "      <td>00:11</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>2.515422</td>\n",
              "      <td>0.264388</td>\n",
              "      <td>1.661606</td>\n",
              "      <td>0.288742</td>\n",
              "      <td>00:10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>2.506654</td>\n",
              "      <td>0.275676</td>\n",
              "      <td>1.659923</td>\n",
              "      <td>0.288742</td>\n",
              "      <td>00:10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>2.520093</td>\n",
              "      <td>0.267568</td>\n",
              "      <td>1.663013</td>\n",
              "      <td>0.288742</td>\n",
              "      <td>00:10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6</td>\n",
              "      <td>2.508234</td>\n",
              "      <td>0.262957</td>\n",
              "      <td>1.662208</td>\n",
              "      <td>0.288742</td>\n",
              "      <td>00:10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7</td>\n",
              "      <td>2.511315</td>\n",
              "      <td>0.261685</td>\n",
              "      <td>1.652186</td>\n",
              "      <td>0.289404</td>\n",
              "      <td>00:10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8</td>\n",
              "      <td>2.508366</td>\n",
              "      <td>0.269316</td>\n",
              "      <td>1.662082</td>\n",
              "      <td>0.288079</td>\n",
              "      <td>00:10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9</td>\n",
              "      <td>2.508068</td>\n",
              "      <td>0.268362</td>\n",
              "      <td>1.662718</td>\n",
              "      <td>0.288079</td>\n",
              "      <td>00:10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>10</td>\n",
              "      <td>2.517882</td>\n",
              "      <td>0.265819</td>\n",
              "      <td>1.665645</td>\n",
              "      <td>0.287417</td>\n",
              "      <td>00:10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>11</td>\n",
              "      <td>2.521839</td>\n",
              "      <td>0.263434</td>\n",
              "      <td>1.654901</td>\n",
              "      <td>0.289404</td>\n",
              "      <td>00:10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>12</td>\n",
              "      <td>2.504300</td>\n",
              "      <td>0.257075</td>\n",
              "      <td>1.661515</td>\n",
              "      <td>0.288742</td>\n",
              "      <td>00:10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>13</td>\n",
              "      <td>2.500654</td>\n",
              "      <td>0.264070</td>\n",
              "      <td>1.660983</td>\n",
              "      <td>0.288742</td>\n",
              "      <td>00:10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>14</td>\n",
              "      <td>2.509559</td>\n",
              "      <td>0.261367</td>\n",
              "      <td>1.660246</td>\n",
              "      <td>0.288742</td>\n",
              "      <td>00:10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>15</td>\n",
              "      <td>2.504022</td>\n",
              "      <td>0.263593</td>\n",
              "      <td>1.654781</td>\n",
              "      <td>0.289404</td>\n",
              "      <td>00:10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>16</td>\n",
              "      <td>2.514730</td>\n",
              "      <td>0.271383</td>\n",
              "      <td>1.667949</td>\n",
              "      <td>0.288079</td>\n",
              "      <td>00:10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>17</td>\n",
              "      <td>2.499478</td>\n",
              "      <td>0.265024</td>\n",
              "      <td>1.661151</td>\n",
              "      <td>0.288742</td>\n",
              "      <td>00:10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>18</td>\n",
              "      <td>2.527965</td>\n",
              "      <td>0.267409</td>\n",
              "      <td>1.652730</td>\n",
              "      <td>0.289404</td>\n",
              "      <td>00:10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>19</td>\n",
              "      <td>2.506129</td>\n",
              "      <td>0.265819</td>\n",
              "      <td>1.660629</td>\n",
              "      <td>0.288742</td>\n",
              "      <td>00:10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>20</td>\n",
              "      <td>2.520275</td>\n",
              "      <td>0.262003</td>\n",
              "      <td>1.658117</td>\n",
              "      <td>0.289404</td>\n",
              "      <td>00:10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>21</td>\n",
              "      <td>2.511476</td>\n",
              "      <td>0.265501</td>\n",
              "      <td>1.657171</td>\n",
              "      <td>0.288742</td>\n",
              "      <td>00:10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>22</td>\n",
              "      <td>2.501991</td>\n",
              "      <td>0.264865</td>\n",
              "      <td>1.659265</td>\n",
              "      <td>0.288742</td>\n",
              "      <td>00:10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>23</td>\n",
              "      <td>2.536958</td>\n",
              "      <td>0.261049</td>\n",
              "      <td>1.665455</td>\n",
              "      <td>0.288742</td>\n",
              "      <td>00:10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>24</td>\n",
              "      <td>2.521394</td>\n",
              "      <td>0.262321</td>\n",
              "      <td>1.662216</td>\n",
              "      <td>0.288742</td>\n",
              "      <td>00:10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>25</td>\n",
              "      <td>2.509966</td>\n",
              "      <td>0.268045</td>\n",
              "      <td>1.660347</td>\n",
              "      <td>0.288079</td>\n",
              "      <td>00:11</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>26</td>\n",
              "      <td>2.511726</td>\n",
              "      <td>0.268203</td>\n",
              "      <td>1.665633</td>\n",
              "      <td>0.288079</td>\n",
              "      <td>00:10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>27</td>\n",
              "      <td>2.519810</td>\n",
              "      <td>0.265819</td>\n",
              "      <td>1.665482</td>\n",
              "      <td>0.288742</td>\n",
              "      <td>00:10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>28</td>\n",
              "      <td>2.523236</td>\n",
              "      <td>0.264229</td>\n",
              "      <td>1.668280</td>\n",
              "      <td>0.286755</td>\n",
              "      <td>00:10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>29</td>\n",
              "      <td>2.512423</td>\n",
              "      <td>0.261367</td>\n",
              "      <td>1.659834</td>\n",
              "      <td>0.288079</td>\n",
              "      <td>00:10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>30</td>\n",
              "      <td>2.503942</td>\n",
              "      <td>0.267886</td>\n",
              "      <td>1.663175</td>\n",
              "      <td>0.288742</td>\n",
              "      <td>00:11</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>31</td>\n",
              "      <td>2.506759</td>\n",
              "      <td>0.265660</td>\n",
              "      <td>1.658109</td>\n",
              "      <td>0.288742</td>\n",
              "      <td>00:11</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>32</td>\n",
              "      <td>2.528551</td>\n",
              "      <td>0.262639</td>\n",
              "      <td>1.660244</td>\n",
              "      <td>0.288079</td>\n",
              "      <td>00:10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>33</td>\n",
              "      <td>2.523988</td>\n",
              "      <td>0.270429</td>\n",
              "      <td>1.660659</td>\n",
              "      <td>0.288742</td>\n",
              "      <td>00:10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>34</td>\n",
              "      <td>2.516207</td>\n",
              "      <td>0.260413</td>\n",
              "      <td>1.656273</td>\n",
              "      <td>0.289404</td>\n",
              "      <td>00:10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>35</td>\n",
              "      <td>2.504006</td>\n",
              "      <td>0.269634</td>\n",
              "      <td>1.659054</td>\n",
              "      <td>0.289404</td>\n",
              "      <td>00:10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>36</td>\n",
              "      <td>2.515283</td>\n",
              "      <td>0.261844</td>\n",
              "      <td>1.664172</td>\n",
              "      <td>0.288079</td>\n",
              "      <td>00:10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>37</td>\n",
              "      <td>2.514037</td>\n",
              "      <td>0.263434</td>\n",
              "      <td>1.654387</td>\n",
              "      <td>0.289404</td>\n",
              "      <td>00:10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>38</td>\n",
              "      <td>2.529308</td>\n",
              "      <td>0.265024</td>\n",
              "      <td>1.663545</td>\n",
              "      <td>0.288742</td>\n",
              "      <td>00:10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>39</td>\n",
              "      <td>2.507611</td>\n",
              "      <td>0.267409</td>\n",
              "      <td>1.659120</td>\n",
              "      <td>0.289404</td>\n",
              "      <td>00:10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>40</td>\n",
              "      <td>2.516878</td>\n",
              "      <td>0.265024</td>\n",
              "      <td>1.658861</td>\n",
              "      <td>0.288742</td>\n",
              "      <td>00:10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>41</td>\n",
              "      <td>2.506649</td>\n",
              "      <td>0.262957</td>\n",
              "      <td>1.660098</td>\n",
              "      <td>0.288742</td>\n",
              "      <td>00:10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>42</td>\n",
              "      <td>2.504647</td>\n",
              "      <td>0.265819</td>\n",
              "      <td>1.658799</td>\n",
              "      <td>0.289404</td>\n",
              "      <td>00:10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>43</td>\n",
              "      <td>2.510988</td>\n",
              "      <td>0.262003</td>\n",
              "      <td>1.656042</td>\n",
              "      <td>0.289404</td>\n",
              "      <td>00:10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>44</td>\n",
              "      <td>2.510871</td>\n",
              "      <td>0.262162</td>\n",
              "      <td>1.668322</td>\n",
              "      <td>0.288079</td>\n",
              "      <td>00:10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>45</td>\n",
              "      <td>2.521677</td>\n",
              "      <td>0.263911</td>\n",
              "      <td>1.662062</td>\n",
              "      <td>0.288742</td>\n",
              "      <td>00:10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>46</td>\n",
              "      <td>2.540296</td>\n",
              "      <td>0.265024</td>\n",
              "      <td>1.662781</td>\n",
              "      <td>0.288079</td>\n",
              "      <td>00:10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>47</td>\n",
              "      <td>2.512294</td>\n",
              "      <td>0.262639</td>\n",
              "      <td>1.659402</td>\n",
              "      <td>0.288742</td>\n",
              "      <td>00:10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>48</td>\n",
              "      <td>2.521197</td>\n",
              "      <td>0.266455</td>\n",
              "      <td>1.664656</td>\n",
              "      <td>0.288742</td>\n",
              "      <td>00:10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>49</td>\n",
              "      <td>2.497262</td>\n",
              "      <td>0.268203</td>\n",
              "      <td>1.663126</td>\n",
              "      <td>0.288742</td>\n",
              "      <td>00:10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>50</td>\n",
              "      <td>2.517431</td>\n",
              "      <td>0.265183</td>\n",
              "      <td>1.663035</td>\n",
              "      <td>0.288079</td>\n",
              "      <td>00:10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>51</td>\n",
              "      <td>2.518232</td>\n",
              "      <td>0.265660</td>\n",
              "      <td>1.661348</td>\n",
              "      <td>0.288742</td>\n",
              "      <td>00:10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>52</td>\n",
              "      <td>2.505525</td>\n",
              "      <td>0.263434</td>\n",
              "      <td>1.659222</td>\n",
              "      <td>0.288742</td>\n",
              "      <td>00:10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>53</td>\n",
              "      <td>2.511304</td>\n",
              "      <td>0.266296</td>\n",
              "      <td>1.661848</td>\n",
              "      <td>0.288742</td>\n",
              "      <td>00:10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>54</td>\n",
              "      <td>2.509920</td>\n",
              "      <td>0.262003</td>\n",
              "      <td>1.662711</td>\n",
              "      <td>0.288079</td>\n",
              "      <td>00:10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>55</td>\n",
              "      <td>2.507388</td>\n",
              "      <td>0.262321</td>\n",
              "      <td>1.657911</td>\n",
              "      <td>0.289404</td>\n",
              "      <td>00:10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>56</td>\n",
              "      <td>2.511547</td>\n",
              "      <td>0.263275</td>\n",
              "      <td>1.658419</td>\n",
              "      <td>0.289404</td>\n",
              "      <td>00:10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>57</td>\n",
              "      <td>2.498348</td>\n",
              "      <td>0.268680</td>\n",
              "      <td>1.658362</td>\n",
              "      <td>0.288742</td>\n",
              "      <td>00:10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>58</td>\n",
              "      <td>2.528175</td>\n",
              "      <td>0.262480</td>\n",
              "      <td>1.657006</td>\n",
              "      <td>0.289404</td>\n",
              "      <td>00:11</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>59</td>\n",
              "      <td>2.528132</td>\n",
              "      <td>0.263911</td>\n",
              "      <td>1.659989</td>\n",
              "      <td>0.288742</td>\n",
              "      <td>00:11</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>60</td>\n",
              "      <td>2.525308</td>\n",
              "      <td>0.261844</td>\n",
              "      <td>1.666789</td>\n",
              "      <td>0.288079</td>\n",
              "      <td>00:11</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>61</td>\n",
              "      <td>2.516577</td>\n",
              "      <td>0.257234</td>\n",
              "      <td>1.664258</td>\n",
              "      <td>0.287417</td>\n",
              "      <td>00:11</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>62</td>\n",
              "      <td>2.512734</td>\n",
              "      <td>0.261367</td>\n",
              "      <td>1.655345</td>\n",
              "      <td>0.289404</td>\n",
              "      <td>00:10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>63</td>\n",
              "      <td>2.520110</td>\n",
              "      <td>0.265024</td>\n",
              "      <td>1.660569</td>\n",
              "      <td>0.288742</td>\n",
              "      <td>00:10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>64</td>\n",
              "      <td>2.521049</td>\n",
              "      <td>0.268680</td>\n",
              "      <td>1.662504</td>\n",
              "      <td>0.288742</td>\n",
              "      <td>00:10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>65</td>\n",
              "      <td>2.502611</td>\n",
              "      <td>0.266932</td>\n",
              "      <td>1.656766</td>\n",
              "      <td>0.289404</td>\n",
              "      <td>00:10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>66</td>\n",
              "      <td>2.504377</td>\n",
              "      <td>0.263434</td>\n",
              "      <td>1.662383</td>\n",
              "      <td>0.288079</td>\n",
              "      <td>00:10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>67</td>\n",
              "      <td>2.522090</td>\n",
              "      <td>0.265660</td>\n",
              "      <td>1.661601</td>\n",
              "      <td>0.288742</td>\n",
              "      <td>00:10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>68</td>\n",
              "      <td>2.500913</td>\n",
              "      <td>0.263275</td>\n",
              "      <td>1.659105</td>\n",
              "      <td>0.288742</td>\n",
              "      <td>00:10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>69</td>\n",
              "      <td>2.504307</td>\n",
              "      <td>0.267409</td>\n",
              "      <td>1.658046</td>\n",
              "      <td>0.288742</td>\n",
              "      <td>00:10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>70</td>\n",
              "      <td>2.521640</td>\n",
              "      <td>0.264547</td>\n",
              "      <td>1.665291</td>\n",
              "      <td>0.288079</td>\n",
              "      <td>00:10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>71</td>\n",
              "      <td>2.520888</td>\n",
              "      <td>0.266137</td>\n",
              "      <td>1.658273</td>\n",
              "      <td>0.288742</td>\n",
              "      <td>00:10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>72</td>\n",
              "      <td>2.520726</td>\n",
              "      <td>0.262480</td>\n",
              "      <td>1.662070</td>\n",
              "      <td>0.288742</td>\n",
              "      <td>00:10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>73</td>\n",
              "      <td>2.506462</td>\n",
              "      <td>0.266455</td>\n",
              "      <td>1.662633</td>\n",
              "      <td>0.288742</td>\n",
              "      <td>00:10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>74</td>\n",
              "      <td>2.512303</td>\n",
              "      <td>0.273450</td>\n",
              "      <td>1.658586</td>\n",
              "      <td>0.288742</td>\n",
              "      <td>00:10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>75</td>\n",
              "      <td>2.517513</td>\n",
              "      <td>0.267091</td>\n",
              "      <td>1.656126</td>\n",
              "      <td>0.289404</td>\n",
              "      <td>00:10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>76</td>\n",
              "      <td>2.517802</td>\n",
              "      <td>0.260890</td>\n",
              "      <td>1.662606</td>\n",
              "      <td>0.288079</td>\n",
              "      <td>00:10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>77</td>\n",
              "      <td>2.524155</td>\n",
              "      <td>0.260254</td>\n",
              "      <td>1.657961</td>\n",
              "      <td>0.288742</td>\n",
              "      <td>00:11</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>78</td>\n",
              "      <td>2.516412</td>\n",
              "      <td>0.264070</td>\n",
              "      <td>1.666093</td>\n",
              "      <td>0.288079</td>\n",
              "      <td>00:10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>79</td>\n",
              "      <td>2.529102</td>\n",
              "      <td>0.262480</td>\n",
              "      <td>1.654304</td>\n",
              "      <td>0.289404</td>\n",
              "      <td>00:11</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>80</td>\n",
              "      <td>2.518635</td>\n",
              "      <td>0.264865</td>\n",
              "      <td>1.662371</td>\n",
              "      <td>0.288742</td>\n",
              "      <td>00:10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>81</td>\n",
              "      <td>2.529916</td>\n",
              "      <td>0.256598</td>\n",
              "      <td>1.655008</td>\n",
              "      <td>0.289404</td>\n",
              "      <td>00:10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>82</td>\n",
              "      <td>2.524224</td>\n",
              "      <td>0.265024</td>\n",
              "      <td>1.662865</td>\n",
              "      <td>0.288742</td>\n",
              "      <td>00:10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>83</td>\n",
              "      <td>2.505931</td>\n",
              "      <td>0.264070</td>\n",
              "      <td>1.666982</td>\n",
              "      <td>0.288079</td>\n",
              "      <td>00:11</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>84</td>\n",
              "      <td>2.510006</td>\n",
              "      <td>0.264388</td>\n",
              "      <td>1.659222</td>\n",
              "      <td>0.288742</td>\n",
              "      <td>00:10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>85</td>\n",
              "      <td>2.511321</td>\n",
              "      <td>0.267091</td>\n",
              "      <td>1.660302</td>\n",
              "      <td>0.288742</td>\n",
              "      <td>00:10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>86</td>\n",
              "      <td>2.520740</td>\n",
              "      <td>0.267727</td>\n",
              "      <td>1.663827</td>\n",
              "      <td>0.288079</td>\n",
              "      <td>00:10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>87</td>\n",
              "      <td>2.517274</td>\n",
              "      <td>0.264706</td>\n",
              "      <td>1.663996</td>\n",
              "      <td>0.288742</td>\n",
              "      <td>00:11</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>88</td>\n",
              "      <td>2.515063</td>\n",
              "      <td>0.263752</td>\n",
              "      <td>1.659349</td>\n",
              "      <td>0.288742</td>\n",
              "      <td>00:10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>89</td>\n",
              "      <td>2.522038</td>\n",
              "      <td>0.266137</td>\n",
              "      <td>1.666807</td>\n",
              "      <td>0.288079</td>\n",
              "      <td>00:10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>90</td>\n",
              "      <td>2.513163</td>\n",
              "      <td>0.267409</td>\n",
              "      <td>1.659234</td>\n",
              "      <td>0.289404</td>\n",
              "      <td>00:11</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>91</td>\n",
              "      <td>2.519261</td>\n",
              "      <td>0.266455</td>\n",
              "      <td>1.657236</td>\n",
              "      <td>0.288742</td>\n",
              "      <td>00:10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>92</td>\n",
              "      <td>2.521271</td>\n",
              "      <td>0.262003</td>\n",
              "      <td>1.661310</td>\n",
              "      <td>0.288079</td>\n",
              "      <td>00:10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>93</td>\n",
              "      <td>2.509990</td>\n",
              "      <td>0.268998</td>\n",
              "      <td>1.651636</td>\n",
              "      <td>0.289404</td>\n",
              "      <td>00:11</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>94</td>\n",
              "      <td>2.527431</td>\n",
              "      <td>0.261208</td>\n",
              "      <td>1.666439</td>\n",
              "      <td>0.288079</td>\n",
              "      <td>00:10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>95</td>\n",
              "      <td>2.525025</td>\n",
              "      <td>0.267409</td>\n",
              "      <td>1.660274</td>\n",
              "      <td>0.288742</td>\n",
              "      <td>00:11</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>96</td>\n",
              "      <td>2.518193</td>\n",
              "      <td>0.265183</td>\n",
              "      <td>1.658000</td>\n",
              "      <td>0.289404</td>\n",
              "      <td>00:10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>97</td>\n",
              "      <td>2.522512</td>\n",
              "      <td>0.261526</td>\n",
              "      <td>1.664218</td>\n",
              "      <td>0.288742</td>\n",
              "      <td>00:11</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>98</td>\n",
              "      <td>2.528880</td>\n",
              "      <td>0.260095</td>\n",
              "      <td>1.666506</td>\n",
              "      <td>0.288079</td>\n",
              "      <td>00:10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>99</td>\n",
              "      <td>2.500677</td>\n",
              "      <td>0.266455</td>\n",
              "      <td>1.665969</td>\n",
              "      <td>0.288079</td>\n",
              "      <td>00:10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>100</td>\n",
              "      <td>2.526488</td>\n",
              "      <td>0.263116</td>\n",
              "      <td>1.662455</td>\n",
              "      <td>0.288742</td>\n",
              "      <td>00:10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>101</td>\n",
              "      <td>2.515293</td>\n",
              "      <td>0.261526</td>\n",
              "      <td>1.654262</td>\n",
              "      <td>0.289404</td>\n",
              "      <td>00:11</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>102</td>\n",
              "      <td>2.498932</td>\n",
              "      <td>0.263752</td>\n",
              "      <td>1.669224</td>\n",
              "      <td>0.288079</td>\n",
              "      <td>00:10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>103</td>\n",
              "      <td>2.526022</td>\n",
              "      <td>0.260572</td>\n",
              "      <td>1.662710</td>\n",
              "      <td>0.288742</td>\n",
              "      <td>00:10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>104</td>\n",
              "      <td>2.510553</td>\n",
              "      <td>0.269793</td>\n",
              "      <td>1.666673</td>\n",
              "      <td>0.288079</td>\n",
              "      <td>00:10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>105</td>\n",
              "      <td>2.509347</td>\n",
              "      <td>0.269316</td>\n",
              "      <td>1.659584</td>\n",
              "      <td>0.289404</td>\n",
              "      <td>00:10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>106</td>\n",
              "      <td>2.513480</td>\n",
              "      <td>0.270111</td>\n",
              "      <td>1.660567</td>\n",
              "      <td>0.288079</td>\n",
              "      <td>00:10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>107</td>\n",
              "      <td>2.503391</td>\n",
              "      <td>0.267250</td>\n",
              "      <td>1.660135</td>\n",
              "      <td>0.288742</td>\n",
              "      <td>00:10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>108</td>\n",
              "      <td>2.514411</td>\n",
              "      <td>0.265183</td>\n",
              "      <td>1.659536</td>\n",
              "      <td>0.288742</td>\n",
              "      <td>00:10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>109</td>\n",
              "      <td>2.513158</td>\n",
              "      <td>0.266773</td>\n",
              "      <td>1.658256</td>\n",
              "      <td>0.289404</td>\n",
              "      <td>00:10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>110</td>\n",
              "      <td>2.515187</td>\n",
              "      <td>0.261844</td>\n",
              "      <td>1.663683</td>\n",
              "      <td>0.288079</td>\n",
              "      <td>00:10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>111</td>\n",
              "      <td>2.511799</td>\n",
              "      <td>0.267091</td>\n",
              "      <td>1.660535</td>\n",
              "      <td>0.288079</td>\n",
              "      <td>00:10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>112</td>\n",
              "      <td>2.501369</td>\n",
              "      <td>0.271383</td>\n",
              "      <td>1.665878</td>\n",
              "      <td>0.288742</td>\n",
              "      <td>00:10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>113</td>\n",
              "      <td>2.504908</td>\n",
              "      <td>0.262480</td>\n",
              "      <td>1.658213</td>\n",
              "      <td>0.288742</td>\n",
              "      <td>00:10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>114</td>\n",
              "      <td>2.514117</td>\n",
              "      <td>0.262639</td>\n",
              "      <td>1.662802</td>\n",
              "      <td>0.288079</td>\n",
              "      <td>00:10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>115</td>\n",
              "      <td>2.520847</td>\n",
              "      <td>0.270111</td>\n",
              "      <td>1.658995</td>\n",
              "      <td>0.288742</td>\n",
              "      <td>00:11</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>116</td>\n",
              "      <td>2.517364</td>\n",
              "      <td>0.263911</td>\n",
              "      <td>1.655917</td>\n",
              "      <td>0.289404</td>\n",
              "      <td>00:11</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>117</td>\n",
              "      <td>2.528074</td>\n",
              "      <td>0.265978</td>\n",
              "      <td>1.657866</td>\n",
              "      <td>0.288742</td>\n",
              "      <td>00:10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>118</td>\n",
              "      <td>2.524445</td>\n",
              "      <td>0.263911</td>\n",
              "      <td>1.659732</td>\n",
              "      <td>0.288742</td>\n",
              "      <td>00:10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>119</td>\n",
              "      <td>2.509079</td>\n",
              "      <td>0.267568</td>\n",
              "      <td>1.659004</td>\n",
              "      <td>0.288742</td>\n",
              "      <td>00:10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>120</td>\n",
              "      <td>2.506069</td>\n",
              "      <td>0.266614</td>\n",
              "      <td>1.660440</td>\n",
              "      <td>0.288742</td>\n",
              "      <td>00:10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>121</td>\n",
              "      <td>2.515641</td>\n",
              "      <td>0.263911</td>\n",
              "      <td>1.661096</td>\n",
              "      <td>0.288742</td>\n",
              "      <td>00:10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>122</td>\n",
              "      <td>2.508592</td>\n",
              "      <td>0.265024</td>\n",
              "      <td>1.661206</td>\n",
              "      <td>0.288079</td>\n",
              "      <td>00:10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>123</td>\n",
              "      <td>2.529423</td>\n",
              "      <td>0.261526</td>\n",
              "      <td>1.655959</td>\n",
              "      <td>0.289404</td>\n",
              "      <td>00:11</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>124</td>\n",
              "      <td>2.510508</td>\n",
              "      <td>0.264865</td>\n",
              "      <td>1.660325</td>\n",
              "      <td>0.288079</td>\n",
              "      <td>00:10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>125</td>\n",
              "      <td>2.523706</td>\n",
              "      <td>0.266932</td>\n",
              "      <td>1.663583</td>\n",
              "      <td>0.288742</td>\n",
              "      <td>00:10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>126</td>\n",
              "      <td>2.523540</td>\n",
              "      <td>0.265660</td>\n",
              "      <td>1.659331</td>\n",
              "      <td>0.288742</td>\n",
              "      <td>00:10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>127</td>\n",
              "      <td>2.503923</td>\n",
              "      <td>0.265024</td>\n",
              "      <td>1.665959</td>\n",
              "      <td>0.288079</td>\n",
              "      <td>00:10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>128</td>\n",
              "      <td>2.508501</td>\n",
              "      <td>0.264706</td>\n",
              "      <td>1.664586</td>\n",
              "      <td>0.288079</td>\n",
              "      <td>00:11</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>129</td>\n",
              "      <td>2.505301</td>\n",
              "      <td>0.268045</td>\n",
              "      <td>1.665059</td>\n",
              "      <td>0.288079</td>\n",
              "      <td>00:11</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>130</td>\n",
              "      <td>2.527978</td>\n",
              "      <td>0.256439</td>\n",
              "      <td>1.664584</td>\n",
              "      <td>0.288079</td>\n",
              "      <td>00:10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>131</td>\n",
              "      <td>2.514949</td>\n",
              "      <td>0.266296</td>\n",
              "      <td>1.663232</td>\n",
              "      <td>0.288079</td>\n",
              "      <td>00:10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>132</td>\n",
              "      <td>2.520632</td>\n",
              "      <td>0.264865</td>\n",
              "      <td>1.656580</td>\n",
              "      <td>0.289404</td>\n",
              "      <td>00:10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>133</td>\n",
              "      <td>2.507896</td>\n",
              "      <td>0.262957</td>\n",
              "      <td>1.656145</td>\n",
              "      <td>0.288742</td>\n",
              "      <td>00:10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>134</td>\n",
              "      <td>2.499418</td>\n",
              "      <td>0.266137</td>\n",
              "      <td>1.659600</td>\n",
              "      <td>0.288742</td>\n",
              "      <td>00:11</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>135</td>\n",
              "      <td>2.512427</td>\n",
              "      <td>0.262321</td>\n",
              "      <td>1.662516</td>\n",
              "      <td>0.288079</td>\n",
              "      <td>00:10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>136</td>\n",
              "      <td>2.518692</td>\n",
              "      <td>0.270906</td>\n",
              "      <td>1.667148</td>\n",
              "      <td>0.288079</td>\n",
              "      <td>00:10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>137</td>\n",
              "      <td>2.504856</td>\n",
              "      <td>0.265183</td>\n",
              "      <td>1.664990</td>\n",
              "      <td>0.288079</td>\n",
              "      <td>00:10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>138</td>\n",
              "      <td>2.522403</td>\n",
              "      <td>0.261049</td>\n",
              "      <td>1.662403</td>\n",
              "      <td>0.288079</td>\n",
              "      <td>00:10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>139</td>\n",
              "      <td>2.524562</td>\n",
              "      <td>0.261367</td>\n",
              "      <td>1.670787</td>\n",
              "      <td>0.287417</td>\n",
              "      <td>00:10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>140</td>\n",
              "      <td>2.509015</td>\n",
              "      <td>0.264865</td>\n",
              "      <td>1.659110</td>\n",
              "      <td>0.288742</td>\n",
              "      <td>00:10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>141</td>\n",
              "      <td>2.517447</td>\n",
              "      <td>0.260572</td>\n",
              "      <td>1.662224</td>\n",
              "      <td>0.288742</td>\n",
              "      <td>00:10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>142</td>\n",
              "      <td>2.526648</td>\n",
              "      <td>0.267886</td>\n",
              "      <td>1.659976</td>\n",
              "      <td>0.288742</td>\n",
              "      <td>00:10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>143</td>\n",
              "      <td>2.515364</td>\n",
              "      <td>0.259618</td>\n",
              "      <td>1.666970</td>\n",
              "      <td>0.287417</td>\n",
              "      <td>00:11</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>144</td>\n",
              "      <td>2.532896</td>\n",
              "      <td>0.259777</td>\n",
              "      <td>1.654251</td>\n",
              "      <td>0.289404</td>\n",
              "      <td>00:11</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>145</td>\n",
              "      <td>2.509865</td>\n",
              "      <td>0.260731</td>\n",
              "      <td>1.665223</td>\n",
              "      <td>0.288079</td>\n",
              "      <td>00:10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>146</td>\n",
              "      <td>2.507888</td>\n",
              "      <td>0.265819</td>\n",
              "      <td>1.657687</td>\n",
              "      <td>0.288079</td>\n",
              "      <td>00:11</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>147</td>\n",
              "      <td>2.514718</td>\n",
              "      <td>0.266455</td>\n",
              "      <td>1.668510</td>\n",
              "      <td>0.288079</td>\n",
              "      <td>00:10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>148</td>\n",
              "      <td>2.508273</td>\n",
              "      <td>0.268521</td>\n",
              "      <td>1.659834</td>\n",
              "      <td>0.288742</td>\n",
              "      <td>00:10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>149</td>\n",
              "      <td>2.526963</td>\n",
              "      <td>0.262321</td>\n",
              "      <td>1.663435</td>\n",
              "      <td>0.288742</td>\n",
              "      <td>00:10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>150</td>\n",
              "      <td>2.517608</td>\n",
              "      <td>0.264229</td>\n",
              "      <td>1.664691</td>\n",
              "      <td>0.288742</td>\n",
              "      <td>00:10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>151</td>\n",
              "      <td>2.512010</td>\n",
              "      <td>0.270429</td>\n",
              "      <td>1.658046</td>\n",
              "      <td>0.289404</td>\n",
              "      <td>00:10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>152</td>\n",
              "      <td>2.509561</td>\n",
              "      <td>0.268998</td>\n",
              "      <td>1.661815</td>\n",
              "      <td>0.288079</td>\n",
              "      <td>00:10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>153</td>\n",
              "      <td>2.517391</td>\n",
              "      <td>0.267727</td>\n",
              "      <td>1.664775</td>\n",
              "      <td>0.288079</td>\n",
              "      <td>00:10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>154</td>\n",
              "      <td>2.518022</td>\n",
              "      <td>0.265501</td>\n",
              "      <td>1.658894</td>\n",
              "      <td>0.289404</td>\n",
              "      <td>00:10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>155</td>\n",
              "      <td>2.529391</td>\n",
              "      <td>0.264070</td>\n",
              "      <td>1.660228</td>\n",
              "      <td>0.288742</td>\n",
              "      <td>00:10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>156</td>\n",
              "      <td>2.513080</td>\n",
              "      <td>0.267568</td>\n",
              "      <td>1.657372</td>\n",
              "      <td>0.288742</td>\n",
              "      <td>00:10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>157</td>\n",
              "      <td>2.511083</td>\n",
              "      <td>0.264229</td>\n",
              "      <td>1.665906</td>\n",
              "      <td>0.288079</td>\n",
              "      <td>00:10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>158</td>\n",
              "      <td>2.506693</td>\n",
              "      <td>0.264388</td>\n",
              "      <td>1.665932</td>\n",
              "      <td>0.288742</td>\n",
              "      <td>00:10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>159</td>\n",
              "      <td>2.521792</td>\n",
              "      <td>0.264388</td>\n",
              "      <td>1.656218</td>\n",
              "      <td>0.288742</td>\n",
              "      <td>00:10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>160</td>\n",
              "      <td>2.509692</td>\n",
              "      <td>0.268998</td>\n",
              "      <td>1.662009</td>\n",
              "      <td>0.288079</td>\n",
              "      <td>00:10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>161</td>\n",
              "      <td>2.514437</td>\n",
              "      <td>0.269952</td>\n",
              "      <td>1.661128</td>\n",
              "      <td>0.288742</td>\n",
              "      <td>00:10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>162</td>\n",
              "      <td>2.521451</td>\n",
              "      <td>0.262162</td>\n",
              "      <td>1.656588</td>\n",
              "      <td>0.289404</td>\n",
              "      <td>00:10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>163</td>\n",
              "      <td>2.523498</td>\n",
              "      <td>0.273132</td>\n",
              "      <td>1.663218</td>\n",
              "      <td>0.288742</td>\n",
              "      <td>00:10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>164</td>\n",
              "      <td>2.514954</td>\n",
              "      <td>0.267727</td>\n",
              "      <td>1.663898</td>\n",
              "      <td>0.288079</td>\n",
              "      <td>00:10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>165</td>\n",
              "      <td>2.526796</td>\n",
              "      <td>0.265501</td>\n",
              "      <td>1.662496</td>\n",
              "      <td>0.288079</td>\n",
              "      <td>00:10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>166</td>\n",
              "      <td>2.509957</td>\n",
              "      <td>0.266932</td>\n",
              "      <td>1.661697</td>\n",
              "      <td>0.288079</td>\n",
              "      <td>00:11</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>167</td>\n",
              "      <td>2.505841</td>\n",
              "      <td>0.264706</td>\n",
              "      <td>1.658005</td>\n",
              "      <td>0.289404</td>\n",
              "      <td>00:10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>168</td>\n",
              "      <td>2.514642</td>\n",
              "      <td>0.262162</td>\n",
              "      <td>1.655491</td>\n",
              "      <td>0.289404</td>\n",
              "      <td>00:10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>169</td>\n",
              "      <td>2.519937</td>\n",
              "      <td>0.267091</td>\n",
              "      <td>1.657329</td>\n",
              "      <td>0.289404</td>\n",
              "      <td>00:10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>170</td>\n",
              "      <td>2.521161</td>\n",
              "      <td>0.266614</td>\n",
              "      <td>1.660817</td>\n",
              "      <td>0.288079</td>\n",
              "      <td>00:10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>171</td>\n",
              "      <td>2.517984</td>\n",
              "      <td>0.263275</td>\n",
              "      <td>1.659261</td>\n",
              "      <td>0.288742</td>\n",
              "      <td>00:10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>172</td>\n",
              "      <td>2.519065</td>\n",
              "      <td>0.266137</td>\n",
              "      <td>1.656139</td>\n",
              "      <td>0.289404</td>\n",
              "      <td>00:11</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>173</td>\n",
              "      <td>2.509544</td>\n",
              "      <td>0.267568</td>\n",
              "      <td>1.664298</td>\n",
              "      <td>0.288079</td>\n",
              "      <td>00:11</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>174</td>\n",
              "      <td>2.491509</td>\n",
              "      <td>0.262321</td>\n",
              "      <td>1.659167</td>\n",
              "      <td>0.289404</td>\n",
              "      <td>00:10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>175</td>\n",
              "      <td>2.524908</td>\n",
              "      <td>0.264070</td>\n",
              "      <td>1.659525</td>\n",
              "      <td>0.289404</td>\n",
              "      <td>00:10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>176</td>\n",
              "      <td>2.508145</td>\n",
              "      <td>0.268839</td>\n",
              "      <td>1.665475</td>\n",
              "      <td>0.288079</td>\n",
              "      <td>00:10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>177</td>\n",
              "      <td>2.509598</td>\n",
              "      <td>0.267727</td>\n",
              "      <td>1.660258</td>\n",
              "      <td>0.288742</td>\n",
              "      <td>00:10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>178</td>\n",
              "      <td>2.512695</td>\n",
              "      <td>0.262639</td>\n",
              "      <td>1.665024</td>\n",
              "      <td>0.288079</td>\n",
              "      <td>00:10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>179</td>\n",
              "      <td>2.508386</td>\n",
              "      <td>0.265501</td>\n",
              "      <td>1.655310</td>\n",
              "      <td>0.288742</td>\n",
              "      <td>00:10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>180</td>\n",
              "      <td>2.517780</td>\n",
              "      <td>0.267409</td>\n",
              "      <td>1.663001</td>\n",
              "      <td>0.288079</td>\n",
              "      <td>00:10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>181</td>\n",
              "      <td>2.511879</td>\n",
              "      <td>0.261208</td>\n",
              "      <td>1.664047</td>\n",
              "      <td>0.288742</td>\n",
              "      <td>00:10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>182</td>\n",
              "      <td>2.524272</td>\n",
              "      <td>0.263752</td>\n",
              "      <td>1.655918</td>\n",
              "      <td>0.289404</td>\n",
              "      <td>00:10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>183</td>\n",
              "      <td>2.518824</td>\n",
              "      <td>0.266932</td>\n",
              "      <td>1.660233</td>\n",
              "      <td>0.289404</td>\n",
              "      <td>00:10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>184</td>\n",
              "      <td>2.499324</td>\n",
              "      <td>0.260890</td>\n",
              "      <td>1.660683</td>\n",
              "      <td>0.288742</td>\n",
              "      <td>00:10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>185</td>\n",
              "      <td>2.512591</td>\n",
              "      <td>0.263116</td>\n",
              "      <td>1.663445</td>\n",
              "      <td>0.286755</td>\n",
              "      <td>00:10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>186</td>\n",
              "      <td>2.534658</td>\n",
              "      <td>0.263593</td>\n",
              "      <td>1.665170</td>\n",
              "      <td>0.288079</td>\n",
              "      <td>00:10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>187</td>\n",
              "      <td>2.521369</td>\n",
              "      <td>0.264865</td>\n",
              "      <td>1.659593</td>\n",
              "      <td>0.289404</td>\n",
              "      <td>00:10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>188</td>\n",
              "      <td>2.515710</td>\n",
              "      <td>0.265501</td>\n",
              "      <td>1.658529</td>\n",
              "      <td>0.289404</td>\n",
              "      <td>00:10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>189</td>\n",
              "      <td>2.522372</td>\n",
              "      <td>0.266932</td>\n",
              "      <td>1.664018</td>\n",
              "      <td>0.288079</td>\n",
              "      <td>00:10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>190</td>\n",
              "      <td>2.528257</td>\n",
              "      <td>0.262798</td>\n",
              "      <td>1.664718</td>\n",
              "      <td>0.288742</td>\n",
              "      <td>00:10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>191</td>\n",
              "      <td>2.517947</td>\n",
              "      <td>0.264706</td>\n",
              "      <td>1.665424</td>\n",
              "      <td>0.288079</td>\n",
              "      <td>00:10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>192</td>\n",
              "      <td>2.510996</td>\n",
              "      <td>0.262639</td>\n",
              "      <td>1.661898</td>\n",
              "      <td>0.288742</td>\n",
              "      <td>00:10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>193</td>\n",
              "      <td>2.517322</td>\n",
              "      <td>0.265024</td>\n",
              "      <td>1.662400</td>\n",
              "      <td>0.288079</td>\n",
              "      <td>00:10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>194</td>\n",
              "      <td>2.515412</td>\n",
              "      <td>0.267886</td>\n",
              "      <td>1.661565</td>\n",
              "      <td>0.288079</td>\n",
              "      <td>00:10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>195</td>\n",
              "      <td>2.528183</td>\n",
              "      <td>0.259142</td>\n",
              "      <td>1.658366</td>\n",
              "      <td>0.289404</td>\n",
              "      <td>00:10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>196</td>\n",
              "      <td>2.509649</td>\n",
              "      <td>0.262639</td>\n",
              "      <td>1.667571</td>\n",
              "      <td>0.287417</td>\n",
              "      <td>00:10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>197</td>\n",
              "      <td>2.516777</td>\n",
              "      <td>0.262798</td>\n",
              "      <td>1.654409</td>\n",
              "      <td>0.288742</td>\n",
              "      <td>00:10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>198</td>\n",
              "      <td>2.501566</td>\n",
              "      <td>0.266932</td>\n",
              "      <td>1.656442</td>\n",
              "      <td>0.288742</td>\n",
              "      <td>00:10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>199</td>\n",
              "      <td>2.523338</td>\n",
              "      <td>0.263434</td>\n",
              "      <td>1.662233</td>\n",
              "      <td>0.288742</td>\n",
              "      <td>00:10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>200</td>\n",
              "      <td>2.519234</td>\n",
              "      <td>0.265819</td>\n",
              "      <td>1.661890</td>\n",
              "      <td>0.288742</td>\n",
              "      <td>00:10</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-257-eb0fe6ae3a76>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mlearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m300\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcbs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcbscheds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/content/course-v3/nbs/dl2/exp/nb_09b.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, epochs, cbs, reset_opt)\u001b[0m\n\u001b[1;32m     70\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdo_begin_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 72\u001b[0;31m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'begin_epoch'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mall_batches\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     73\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/course-v3/nbs/dl2/exp/nb_09b.py\u001b[0m in \u001b[0;36mall_batches\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     49\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miters\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxb\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0myb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mone_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mxb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0myb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mCancelEpochException\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'after_cancel_epoch'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    519\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sampler_iter\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    520\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 521\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    522\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    523\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    559\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    560\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 561\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    562\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    563\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/course-v3/nbs/dl2/exp/nb_08.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m    139\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__repr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mreturn\u001b[0m \u001b[0;34mf'{self.__class__.__name__}\\nx: {self.x}\\ny: {self.y}\\n'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 141\u001b[0;31m     \u001b[0;32mdef\u001b[0m \u001b[0m__getitem__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    142\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mreturn\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/course-v3/nbs/dl2/exp/nb_08.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     57\u001b[0m         \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getitem__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mres\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mo\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mres\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     60\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mImageList\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mItemList\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/course-v3/nbs/dl2/exp/nb_08.py\u001b[0m in \u001b[0;36m_get\u001b[0;34m(self, i)\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m     \u001b[0;32mdef\u001b[0m  \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mreturn\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m     \u001b[0;32mdef\u001b[0m \u001b[0m_get\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mreturn\u001b[0m \u001b[0mcompose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtfms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     55\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__getitem__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-12-65029f409554>\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, **pickle_load_args)\u001b[0m\n\u001b[1;32m    593\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    594\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0m_open_file_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mopened_file\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 595\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0m_is_zipfile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopened_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    596\u001b[0m             \u001b[0;31m# The zipfile reader is going to advance the current file position.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    597\u001b[0m             \u001b[0;31m# If we want to actually tail call to torch.jit.load, we need to\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36m_is_zipfile\u001b[0;34m(f)\u001b[0m\n\u001b[1;32m     55\u001b[0m     \u001b[0mstart\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtell\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 57\u001b[0;31m     \u001b[0mbyte\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     58\u001b[0m     \u001b[0;32mwhile\u001b[0m \u001b[0mbyte\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m\"\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m         \u001b[0mread_bytes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbyte\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Save model for inference"
      ],
      "metadata": {
        "id": "2os3jtXZYAYi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GiRbgHeeYEzK",
        "outputId": "9fe849ba-fcd7-48bd-ca59-173b14e76a6c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_path = Path('/content/model/v9'); model_path.mkdir(parents = True, exist_ok=True)"
      ],
      "metadata": {
        "id": "UQOJenoaYKHu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.save(learn.model.state_dict(), model_path/'ser-v9-transformer-cnn-mh-2.pt')"
      ],
      "metadata": {
        "id": "W2iHDYlGYB5e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!tar -czf model-v5-8.tar.gz model"
      ],
      "metadata": {
        "id": "X6FMLR-KskN4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# References"
      ],
      "metadata": {
        "id": "a21jVDRDKvgK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- [Pytorch Transformer Encoder](https://pytorch.org/docs/stable/generated/torch.nn.TransformerEncoderLayer.html)\n",
        "- [fastai course-v3 part2, audio](https://github.com/fastai/course-v3/blob/master/nbs/dl2/audio.ipynb)\n",
        "- [Transformer-CNN-emotion recognition](https://github.com/IliaZenkov/transformer-cnn-emotion-recognition/blob/main/notebooks/Parallel_is_All_You_Want.ipynb)\n",
        "- \n"
      ],
      "metadata": {
        "id": "KMp_Qul3KwyF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "tmsc8x7wKw2F"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}