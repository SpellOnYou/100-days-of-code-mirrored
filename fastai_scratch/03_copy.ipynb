{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "03_copy.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aR7WdFvw8XPN",
        "colab_type": "text"
      },
      "source": [
        "- 01_matmul\n",
        "- 02_fully_connected\n",
        "- 02a_why_sqrt5\n",
        "- 02b_initialization\n",
        "- 03_minibatch_training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ios0-s_4_ZwU",
        "colab_type": "text"
      },
      "source": [
        "Table of contents\n",
        "1. Initial Setup\n",
        "2. Basic training Loop\n",
        "3. Using parameters and optim\n",
        "4. Dataset and DataLoader\n",
        "5. Validation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fxTB-ysp_jun",
        "colab_type": "text"
      },
      "source": [
        "# 1. Initial Setup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "be1B6I_hCh1c",
        "colab_type": "text"
      },
      "source": [
        "## Data\n",
        "\n",
        "### Fastai for Colab env"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tMCFq0PZ8tOS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!git clone http://github.com/fastai/course-v3.git"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E3cSi27X88l6",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "1f59f6ec-2118-4e78-c164-81abf5853d48"
      },
      "source": [
        "%cd course-v3/nbs/dl2"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/course-v3/nbs/dl2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FacmFUWm9E-O",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from exp.nb_02 import *"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fAdev1qg-LSD",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 415
        },
        "outputId": "8ba6511a-27c7-4d70-d0c0-3db38f5c09b2"
      },
      "source": [
        "!cat exp/nb_02.py"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "#################################################\n",
            "### THIS FILE WAS AUTOGENERATED! DO NOT EDIT! ###\n",
            "#################################################\n",
            "# file to edit: dev_nb/02_fully_connected.ipynb\n",
            "\n",
            "from exp.nb_01 import *\n",
            "\n",
            "def get_data():\n",
            "    path = datasets.download_data(MNIST_URL, ext='.gz')\n",
            "    with gzip.open(path, 'rb') as f:\n",
            "        ((x_train, y_train), (x_valid, y_valid), _) = pickle.load(f, encoding='latin-1')\n",
            "    return map(tensor, (x_train,y_train,x_valid,y_valid))\n",
            "\n",
            "def normalize(x, m, s): return (x-m)/s\n",
            "\n",
            "def test_near_zero(a,tol=1e-3): assert a.abs()<tol, f\"Near zero: {a}\"\n",
            "\n",
            "from torch.nn import init\n",
            "\n",
            "def mse(output, targ): return (output.squeeze(-1) - targ).pow(2).mean()\n",
            "\n",
            "from torch import nn"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K5tAEA4_9NKW",
        "colab_type": "text"
      },
      "source": [
        "### import data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OTgE-wJ49UaE",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "1551aaba-43ff-481b-9436-56a1cb0a487f"
      },
      "source": [
        "train_x, train_y, valid_x, valid_y = get_data()"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading http://deeplearning.net/data/mnist/mnist.pkl.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              ""
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HoL0CV2v9gLU",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "36960237-5566-4235-f88d-27b510900518"
      },
      "source": [
        "train_x.shape, train_x.type()"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(torch.Size([50000, 784]), 'torch.FloatTensor')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fWzQQ9W49hwN",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "c3c7ea0c-3e6e-4e0b-fe6a-983dd075f587"
      },
      "source": [
        "n_in, nh, n_out = train_x.shape[1], 32, train_y.max().item()+1; n_in, nh, n_out"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(784, 32, 10)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mCU0bL8XA76r",
        "colab_type": "text"
      },
      "source": [
        "[^1]"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "euyGKZhK94Qw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Model(nn.Module):\n",
        "    def __init__(self, n_in, nh, n_out):\n",
        "        super().__init__()\n",
        "        self.layers = [nn.Linear(n_in, nh), nn.ReLU(), nn.Linear(nh, n_out)]\n",
        "    def __call__(self, x):\n",
        "        for l in self.layers: x = l(x)\n",
        "        return x\n",
        "    # def forward(self, x, y):\n",
        "    #     for l in self.layers: x = l(x)\n",
        "    #     return(self.loss(x, y))"
      ],
      "execution_count": 70,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b2WPI509B_-F",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "m = Model(n_in, nh, n_out)"
      ],
      "execution_count": 72,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sPwo2EesCEsZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "y_hat = m(train_x)"
      ],
      "execution_count": 74,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YJQReyi-CNVm",
        "colab_type": "text"
      },
      "source": [
        "## Loss Function"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hGhM0NZrCsfI",
        "colab_type": "text"
      },
      "source": [
        "### Softmax"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MyX4TB0YCzpZ",
        "colab_type": "text"
      },
      "source": [
        "* formula of softmax : $f(x) = \\frac{exp(x)}{\\sum exp(x_i)}$\n",
        "* We need softmax at last layer since we have to map it to the probabilistic space\n",
        "* code of softmax is below"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IzJcewmjDhyC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def softmax(x): return x.exp() / x.exp().sum(-1, keepdim=True)\n",
        "# def softmax(x): return x.exp() / x.exp().sum()"
      ],
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6NnUt5FGHAaE",
        "colab_type": "text"
      },
      "source": [
        "[^2]"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p7RiaE7kHZuo",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "7f09d76f-99cd-4bee-bf38-e38318307619"
      },
      "source": [
        "p = softmax(y_hat); p[0].sum()"
      ],
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(1., grad_fn=<SumBackward0>)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 75
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-BwNmdysHg7A",
        "colab_type": "text"
      },
      "source": [
        "- I need *log* of softmax because we will use cross-entropy which has form of $H(X, q) = H(X) + D(p|q) = - \\sum_{x} p(x)\\  log\\ q(x)$ [^3]"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mr8RQd3ZRppI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def log_softmax(x): return x.exp() / x.exp().sum(-1, keepdim=True)"
      ],
      "execution_count": 85,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0usabhbXdAvn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "a = log_softmax(y_hat)"
      ],
      "execution_count": 87,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VLksrlDQS3ue",
        "colab_type": "text"
      },
      "source": [
        "### Cross Entropy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VNFW_jxyS7Ek",
        "colab_type": "text"
      },
      "source": [
        "- see the footnote 3 regarding further information of cross entropy\n",
        "- One possible reason is because an information theory is based on descrete and determinate value"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H7tREkCuU65w",
        "colab_type": "text"
      },
      "source": [
        "### Negative log likelihood function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "94nPlA0oXJbs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def nll(x, y): return -x[range(x.shape[0]), y].mean() #x: prediction, y:target "
      ],
      "execution_count": 77,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YcKHlflJX7p3",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "63cabb0a-5243-4f0e-f7fe-e7e13dd3a878"
      },
      "source": [
        "nll(y_hat, train_y)"
      ],
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(0.0161, grad_fn=<NegBackward>)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 78
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KFJuF2ibXuIq",
        "colab_type": "text"
      },
      "source": [
        "- Negative value is related to entropy, roughly variable which is rare is meaningful [^4]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v-8dPvdkcKLb",
        "colab_type": "text"
      },
      "source": [
        "### LogSumExp"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZF2GCqf3cLjN",
        "colab_type": "text"
      },
      "source": [
        "- Further tasks (after reading sum refs) [^5]"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "io0jIz5RcQAL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def log_softmax(x): return x - x.logsumexp(-1, keepdim=True)"
      ],
      "execution_count": 82,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vbj1y3lYdKBw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_near(log_softmax(y_hat), a)"
      ],
      "execution_count": 89,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tTbuDvuWdh9g",
        "colab_type": "text"
      },
      "source": [
        "# 2. Basic Training Loop"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tbRnQ8ildlOr",
        "colab_type": "text"
      },
      "source": [
        "~ code from here ~"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DB9w0mADRmwP",
        "colab_type": "text"
      },
      "source": [
        "# Notes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zKh7TcqJRQ0-",
        "colab_type": "text"
      },
      "source": [
        "[^1]: I don't have to define parameters no more, but obviously at first I tried to iniailized Model with `w1, b1, ....`, meaning I didn't practice enough the previous part, part of the torch nn, where I get parameters\n",
        "\n",
        "[^2]: be cautious since `train_x.sum(-1)` will squeeze the rank, so that you need arg `keepdim:bool`, false as default\n",
        "\n",
        "[^3]: see *eq (2.46)*, Foundations of Natural Language Processing, Christopher D. Manning and Hinrich Schütze\n",
        "\n",
        "[^4]: Should check for `cross-entropy $\\approx$ softmax of negative likelihood`\n",
        "\n",
        "[^5]: check later"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vr9R50W2cXUc",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    }
  ]
}