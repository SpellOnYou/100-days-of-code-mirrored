{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "batchnorm2",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JLtRzBDaSPlO"
      },
      "source": [
        "Q1) What are the parameters of batchnorm? What information do you need to train the parameters?  \n",
        "\n",
        "A1) $ \\frac{X - \\mu}{\\sigma} * \\gamma + \\beta $, in which $\\gamma$ and $\\beta$ are optimized using training data (i.e. parameter) and $\\mu$ and $\\sigma$ attained from data.\n",
        "\n",
        "Q2) Why do we use 'exponentially weighted moving average' (chain of linear interpolation) of training data in inference time? In other words, why can't we use one batch of training set's mean and variance in inference time?  \n",
        "A2) When we get a totally different type of image at inference time, we can not fairly access/evaluate the parameters since attained mean/variance of one training data could be irrevant to validation/test data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tmwTnPT0WWMQ",
        "outputId": "ea2bba60-d631-4bf4-9e47-ea0afaac42f0"
      },
      "source": [
        "!git clone https://github.com/fastai/course-v3/"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'course-v3'...\n",
            "remote: Enumerating objects: 5893, done.\u001b[K\n",
            "remote: Counting objects: 100% (3/3), done.\u001b[K\n",
            "remote: Compressing objects: 100% (3/3), done.\u001b[K\n",
            "remote: Total 5893 (delta 0), reused 2 (delta 0), pack-reused 5890\u001b[K\n",
            "Receiving objects: 100% (5893/5893), 263.03 MiB | 33.42 MiB/s, done.\n",
            "Resolving deltas: 100% (3249/3249), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-Ei0MPj5WbTs",
        "outputId": "99c1a537-d958-45c6-da54-11a0c0cc2bf5"
      },
      "source": [
        "%cd /content/course-v3/nbs/dl2/\n",
        "%load_ext autoreload\n",
        "%autoreload 2\n",
        "%matplotlib inline\n",
        "from exp.nb_06 import *"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/course-v3/nbs/dl2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fz2BLhg9W9Uj"
      },
      "source": [
        "def get_data():\n",
        "    # path = datasets.download_data(MNIST_URL, ext='.gz')\n",
        "    path = '/content/mnist.pkl.gz'\n",
        "    with gzip.open(path, 'rb') as f:\n",
        "        ((x_train, y_train), (x_valid, y_valid), _) = pickle.load(f, encoding='latin-1')\n",
        "    return map(tensor, (x_train,y_train,x_valid,y_valid))"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lmhp-b2KXss9"
      },
      "source": [
        "- get data and transform those to regularized size\n",
        "- dataset class, which enables slicing and length information\n",
        "- databunch instance, which gives 1) dataloader 2) number of class\n",
        "    1. dataloader\n",
        "        - generate training data in a random order (when shuffle=True)\n",
        "        - in case tensors don't have same length, pad first or last (e.g. pytorch [pad_packed_sequence](https://pytorch.org/docs/stable/generated/torch.nn.utils.rnn.pad_packed_sequence.html))\n",
        "- callback functions\n",
        "    - recorder which records learning rate and loss\n",
        "    - avgstats which records total loss and batch count\n",
        "    - cuda : convert to gpu before batch\n",
        "    - unflatten image\n",
        "\n",
        "- define convolution channels \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QTy_uFkOZ36r"
      },
      "source": [
        "??AvgStats\n",
        "??AvgStatsCallback"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MmMrB45IXqXA",
        "outputId": "62fb44b8-ab99-4682-d960-12a38df1ce06"
      },
      "source": [
        "x_train, y_train, x_valid, y_valid = get_data()\n",
        "x_train, x_valid = normalize_to(x_train, x_valid)\n",
        "\n",
        "train_ds, valid_ds = Dataset(x_train, y_train), Dataset(x_valid, y_valid)\n",
        "nh, bs = 50, 512\n",
        "c = y_train.max().item()+1\n",
        "loss_func = F.cross_entropy\n",
        "\n",
        "data = DataBunch(*get_dls(train_ds, valid_ds, bs), c)\n",
        "mnist_view = view_tfm(1, 28, 28)\n",
        "cbfs = [Recorder,\n",
        "        partial(AvgStatsCallback, accuracy),\n",
        "        # CudaCallback,\n",
        "        partial(BatchTransformXCallback, mnist_view)]\n",
        "nfs = [8, 16, 32, 64, 64]\n",
        "learn, run = get_learn_run(nfs, data, 0.4, conv_layer, cbs=cbfs)\n",
        "\n",
        "%time run.fit(2, learn)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train: [1.39714296875, tensor(0.5276)]\n",
            "valid: [0.2345330322265625, tensor(0.9307)]\n",
            "train: [0.1934852734375, tensor(0.9416)]\n",
            "valid: [0.1346829345703125, tensor(0.9600)]\n",
            "CPU times: user 22.9 s, sys: 600 ms, total: 23.5 s\n",
            "Wall time: 12.2 s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j69hJcogVhmS"
      },
      "source": [
        "🎮 Q3: Implement customized batch norm, and plot activations’ mean and std.\n",
        "- Caution: in train uses batches mean and variance while inference time it uses exponentially weighted moving average\n",
        "- Caution: batch norm broadcasts tensor thorough batch data. so that dimension size of normalizing parameter differs depending on types of normalization."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pi_SY8JHSGVB"
      },
      "source": [
        "xb, yb = next(iter(data.train_dl))\n",
        "xb = xb.view(-1, 1, 28, 28)"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FRmjPiFWSgnU",
        "outputId": "6f387e9a-dab7-467b-bc83-38fe11c77055",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "xb.mean(dim=(0, 2, 3), keepdim=True).shape"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([1, 1, 1, 1])"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cAK3tKhTT3rZ"
      },
      "source": [
        "xb.lerp()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UcmTT1wGPk33"
      },
      "source": [
        "# customized batchnorm\n",
        "class BatchNorm(nn.Module):\n",
        "    def __init__(self, nf, mom=0.1, eps=1e-5):\n",
        "        super().__init__()\n",
        "        self.mom, self.eps = mom, eps\n",
        "\n",
        "        self.mult = nn.Parameter(torch.ones(nf, 1, 1))\n",
        "        self.add  = nn.Parameter(torch.zeros(nf, 1, 1))\n",
        "        self.register_buffer('vars', torch.ones(1, nf, 1, 1))\n",
        "        self.register_buffer('means', torch.zeros(1, nf, 1, 1))\n",
        "\n",
        "    def update_stats(self, x):\n",
        "        m = x.mean(dim=(0, 2, 3), keepdim=1)\n",
        "        v = x.var(dim=(0, 2, 3), keepdim=1)\n",
        "        self.means.lerp_(m, self.mom)\n",
        "        self.vars.lerp_(v, self.mom)\n",
        "        return m, v\n",
        "    def forward(self, x):\n",
        "        if self.in_train:\n",
        "            with torch.no_grad(): # TODO: where and where not we need this?\n",
        "                m, v = self.updates_stats(x)\n",
        "        else:\n",
        "            m, v = self.means, self.vars\n",
        "        x = (x - m) / (v+self.eps).sqrt()\n",
        "        return x * self.mult + self.add"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SX1b3tb0VR0R"
      },
      "source": [
        "def conv_layer(ni, nf, ks=3, stride=2, bn=True, **kwargs):\n",
        "    layers = [nn.Conv2d(ni, nf, ks, padding=ks//2, stride=stride, bias = not bn), GeneralRelu(**kwargs)]\n",
        "    if bn: layers.append(BatchNorm(nf))\n",
        "    return nn.Sequential(*layers)"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WmQ1DveMV4Jy"
      },
      "source": [
        "def init_cnn(m, uniform=False):\n",
        "    f = init.kaiming_uniform_ if uniform else init.kaiming_normal_\n",
        "    init_cnn_(m, f)\n",
        "def init_cnn_(m, f):\n",
        "    if isinstance(m, nn.Conv2d):\n",
        "        # actualy initialization happens here\n",
        "        f(m.weight, a=0.1)\n",
        "        # when bias exist (i.e. no batchnorm) make sure it's zero\n",
        "        if getattr(m, 'bias', None) is not None: m.bias.data.zero_()\n",
        "    for layer in m.children(): init_cnn_(layer, f)\n",
        "def get_learn_run(nfs, data, lr, layer, cbs=None, opt_func = None, uniform = False, **kwargs):\n",
        "    model = get_cnn_model(data, nfs, layer, **kwargs)\n",
        "    init_cnn(model, uniform = uniform)\n",
        "    return get_runner(model, data, lr=lr, cbs=cbs, opt_func=opt_func)"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dhAQzYidV38p"
      },
      "source": [
        "learn, run = get_learn_run(nfs, data, 0.9, conv_layer, cbs=cbfs)"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vc606REBYPCG"
      },
      "source": [
        "- plot activations"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RDOVHVWiPkPq"
      },
      "source": [
        "\n",
        "\n",
        "🎮 Q4: Use built-in batchnorm of pytorch\n",
        "\n",
        "🎮 Q5: Add scheduler and train\n",
        "\n",
        "📝 Q6: Explain difference between batchnorm and layernorm. Implement Layernorm class.\n",
        "\n",
        "📝 🎮 Q7: Implement InstanceNorm class. Why do you think the model trained on instance norm can not be a classification model?\n",
        "\n",
        "🎮 Q8: GroupNorm: initialize activation with N=20, channel = 6, height, width = 10 and 1) separate 6 channels into 3 groups 2) separate 6 channels into 6 groups (instancenorm) 3) put all 6 channels into as signle group (layernorm)\n",
        "\n",
        "📝 🎮 Q9: Fastai introduces RunningBatchNorm class to be used in small batch cases. Implement it and write your opinion for what reason small batch size makes problem."
      ]
    }
  ]
}