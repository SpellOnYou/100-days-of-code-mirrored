{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "05-copy-annealing",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i-s6YliGodFA"
      },
      "source": [
        "- My own practice to implement(or copy) re-usable/probable codes from scratch. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V0pJJh0qy04k"
      },
      "source": [
        "import gzip, pickle, pathlib, torch"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uDsXtBMI2WGg"
      },
      "source": [
        "data_path = pathlib.Path(\"/content/mnist.pkl.gz\")"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4-vKxvv_2VLk"
      },
      "source": [
        "def get_data(data_path):\n",
        "    with gzip.open(data_path) as f:\n",
        "        (x_train, y_train), (x_valid, y_valid), (x_test, y_test)= pickle.load(f, encoding='latin-1')\n",
        "        return map(torch.tensor, (x_train, y_train, x_valid, y_valid, x_test, y_test))"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3m6bTFdN29ep"
      },
      "source": [
        "x_train, y_train, x_valid, y_valid, x_test, y_test = get_data(data_path)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JdIjP9xd3NXY",
        "outputId": "b326ec04-8a2b-4bfc-b4c3-253acc157162"
      },
      "source": [
        "def print_size(): [print(i) for i in map(lambda x: print(x.shape), (x_train, y_train, x_valid, y_valid, x_test, y_test))]\n",
        "print_size()"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([50000, 784])\n",
            "None\n",
            "torch.Size([50000])\n",
            "None\n",
            "torch.Size([10000, 784])\n",
            "None\n",
            "torch.Size([10000])\n",
            "None\n",
            "torch.Size([10000, 784])\n",
            "None\n",
            "torch.Size([10000])\n",
            "None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        },
        "id": "bTmFBuduYNZT",
        "outputId": "56c12a86-218a-4a10-da49-22f7532ae07f"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.imshow(x_train[-1].view(28, -1))"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7fa3ffce9d90>"
            ]
          },
          "metadata": {},
          "execution_count": 19
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4TsnPk7i3f4J"
      },
      "source": [
        "`Learner`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wr3gebBP3hUE",
        "outputId": "4f6e66b0-b01a-4375-f7a3-28f8b87fc7b4"
      },
      "source": [
        "# !git clone https://github.com/fastai/course-v3\n",
        "# !cat course-v3/nbs/dl2/exp/nb_01.py\n",
        "# !cat course-v3/nbs/dl2/exp/nb_02.py\n",
        "# !cat course-v3/nbs/dl2/exp/nb_03.py\n",
        "# !cat course-v3/nbs/dl2/exp/nb_04.py"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "#################################################\n",
            "### THIS FILE WAS AUTOGENERATED! DO NOT EDIT! ###\n",
            "#################################################\n",
            "# file to edit: dev_nb/01_matmul.ipynb\n",
            "\n",
            "from exp.nb_00 import *\n",
            "import operator\n",
            "\n",
            "def test(a,b,cmp,cname=None):\n",
            "    if cname is None: cname=cmp.__name__\n",
            "    assert cmp(a,b),f\"{cname}:\\n{a}\\n{b}\"\n",
            "\n",
            "def test_eq(a,b): test(a,b,operator.eq,'==')\n",
            "\n",
            "from pathlib import Path\n",
            "from IPython.core.debugger import set_trace\n",
            "from fastai import datasets\n",
            "import pickle, gzip, math, torch, matplotlib as mpl\n",
            "import matplotlib.pyplot as plt\n",
            "from torch import tensor\n",
            "\n",
            "MNIST_URL='http://deeplearning.net/data/mnist/mnist.pkl'\n",
            "\n",
            "def near(a,b): return torch.allclose(a, b, rtol=1e-3, atol=1e-5)\n",
            "def test_near(a,b): test(a,b,near)\n",
            "#################################################\n",
            "### THIS FILE WAS AUTOGENERATED! DO NOT EDIT! ###\n",
            "#################################################\n",
            "# file to edit: dev_nb/02_fully_connected.ipynb\n",
            "\n",
            "from exp.nb_01 import *\n",
            "\n",
            "def get_data():\n",
            "    path = datasets.download_data(MNIST_URL, ext='.gz')\n",
            "    with gzip.open(path, 'rb') as f:\n",
            "        ((x_train, y_train), (x_valid, y_valid), _) = pickle.load(f, encoding='latin-1')\n",
            "    return map(tensor, (x_train,y_train,x_valid,y_valid))\n",
            "\n",
            "def normalize(x, m, s): return (x-m)/s\n",
            "\n",
            "def test_near_zero(a,tol=1e-3): assert a.abs()<tol, f\"Near zero: {a}\"\n",
            "\n",
            "from torch.nn import init\n",
            "\n",
            "def mse(output, targ): return (output.squeeze(-1) - targ).pow(2).mean()\n",
            "\n",
            "from torch import nn\n",
            "#################################################\n",
            "### THIS FILE WAS AUTOGENERATED! DO NOT EDIT! ###\n",
            "#################################################\n",
            "# file to edit: dev_nb/03_minibatch_training.ipynb\n",
            "\n",
            "from exp.nb_02 import *\n",
            "import torch.nn.functional as F\n",
            "\n",
            "def accuracy(out, yb): return (torch.argmax(out, dim=1)==yb).float().mean()\n",
            "\n",
            "from torch import optim\n",
            "\n",
            "class Dataset():\n",
            "    def __init__(self, x, y): self.x,self.y = x,y\n",
            "    def __len__(self): return len(self.x)\n",
            "    def __getitem__(self, i): return self.x[i],self.y[i]\n",
            "\n",
            "from torch.utils.data import DataLoader, SequentialSampler, RandomSampler\n",
            "\n",
            "def get_dls(train_ds, valid_ds, bs, **kwargs):\n",
            "    return (DataLoader(train_ds, batch_size=bs, shuffle=True, **kwargs),\n",
            "            DataLoader(valid_ds, batch_size=bs*2, **kwargs))\n",
            "#################################################\n",
            "### THIS FILE WAS AUTOGENERATED! DO NOT EDIT! ###\n",
            "#################################################\n",
            "# file to edit: dev_nb/04_callbacks.ipynb\n",
            "\n",
            "from exp.nb_03 import *\n",
            "\n",
            "class DataBunch():\n",
            "    def __init__(self, train_dl, valid_dl, c=None):\n",
            "        self.train_dl,self.valid_dl,self.c = train_dl,valid_dl,c\n",
            "\n",
            "    @property\n",
            "    def train_ds(self): return self.train_dl.dataset\n",
            "\n",
            "    @property\n",
            "    def valid_ds(self): return self.valid_dl.dataset\n",
            "\n",
            "def get_model(data, lr=0.5, nh=50):\n",
            "    m = data.train_ds.x.shape[1]\n",
            "    model = nn.Sequential(nn.Linear(m,nh), nn.ReLU(), nn.Linear(nh,data.c))\n",
            "    return model, optim.SGD(model.parameters(), lr=lr)\n",
            "\n",
            "class Learner():\n",
            "    def __init__(self, model, opt, loss_func, data):\n",
            "        self.model,self.opt,self.loss_func,self.data = model,opt,loss_func,data\n",
            "\n",
            "import re\n",
            "\n",
            "_camel_re1 = re.compile('(.)([A-Z][a-z]+)')\n",
            "_camel_re2 = re.compile('([a-z0-9])([A-Z])')\n",
            "def camel2snake(name):\n",
            "    s1 = re.sub(_camel_re1, r'\\1_\\2', name)\n",
            "    return re.sub(_camel_re2, r'\\1_\\2', s1).lower()\n",
            "\n",
            "class Callback():\n",
            "    _order=0\n",
            "    def set_runner(self, run): self.run=run\n",
            "    def __getattr__(self, k): return getattr(self.run, k)\n",
            "    @property\n",
            "    def name(self):\n",
            "        name = re.sub(r'Callback$', '', self.__class__.__name__)\n",
            "        return camel2snake(name or 'callback')\n",
            "\n",
            "class TrainEvalCallback(Callback):\n",
            "    def begin_fit(self):\n",
            "        self.run.n_epochs=0.\n",
            "        self.run.n_iter=0\n",
            "\n",
            "    def after_batch(self):\n",
            "        if not self.in_train: return\n",
            "        self.run.n_epochs += 1./self.iters\n",
            "        self.run.n_iter   += 1\n",
            "\n",
            "    def begin_epoch(self):\n",
            "        self.run.n_epochs=self.epoch\n",
            "        self.model.train()\n",
            "        self.run.in_train=True\n",
            "\n",
            "    def begin_validate(self):\n",
            "        self.model.eval()\n",
            "        self.run.in_train=False\n",
            "\n",
            "from typing import *\n",
            "\n",
            "def listify(o):\n",
            "    if o is None: return []\n",
            "    if isinstance(o, list): return o\n",
            "    if isinstance(o, str): return [o]\n",
            "    if isinstance(o, Iterable): return list(o)\n",
            "    return [o]\n",
            "\n",
            "class Runner():\n",
            "    def __init__(self, cbs=None, cb_funcs=None):\n",
            "        cbs = listify(cbs)\n",
            "        for cbf in listify(cb_funcs):\n",
            "            cb = cbf()\n",
            "            setattr(self, cb.name, cb)\n",
            "            cbs.append(cb)\n",
            "        self.stop,self.cbs = False,[TrainEvalCallback()]+cbs\n",
            "\n",
            "    @property\n",
            "    def opt(self):       return self.learn.opt\n",
            "    @property\n",
            "    def model(self):     return self.learn.model\n",
            "    @property\n",
            "    def loss_func(self): return self.learn.loss_func\n",
            "    @property\n",
            "    def data(self):      return self.learn.data\n",
            "\n",
            "    def one_batch(self, xb, yb):\n",
            "        self.xb,self.yb = xb,yb\n",
            "        if self('begin_batch'): return\n",
            "        self.pred = self.model(self.xb)\n",
            "        if self('after_pred'): return\n",
            "        self.loss = self.loss_func(self.pred, self.yb)\n",
            "        if self('after_loss') or not self.in_train: return\n",
            "        self.loss.backward()\n",
            "        if self('after_backward'): return\n",
            "        self.opt.step()\n",
            "        if self('after_step'): return\n",
            "        self.opt.zero_grad()\n",
            "\n",
            "    def all_batches(self, dl):\n",
            "        self.iters = len(dl)\n",
            "        for xb,yb in dl:\n",
            "            if self.stop: break\n",
            "            self.one_batch(xb, yb)\n",
            "            self('after_batch')\n",
            "        self.stop=False\n",
            "\n",
            "    def fit(self, epochs, learn):\n",
            "        self.epochs,self.learn,self.loss = epochs,learn,tensor(0.)\n",
            "\n",
            "        try:\n",
            "            for cb in self.cbs: cb.set_runner(self)\n",
            "            if self('begin_fit'): return\n",
            "            for epoch in range(epochs):\n",
            "                self.epoch = epoch\n",
            "                if not self('begin_epoch'): self.all_batches(self.data.train_dl)\n",
            "\n",
            "                with torch.no_grad():\n",
            "                    if not self('begin_validate'): self.all_batches(self.data.valid_dl)\n",
            "                if self('after_epoch'): break\n",
            "\n",
            "        finally:\n",
            "            self('after_fit')\n",
            "            self.learn = None\n",
            "\n",
            "    def __call__(self, cb_name):\n",
            "        for cb in sorted(self.cbs, key=lambda x: x._order):\n",
            "            f = getattr(cb, cb_name, None)\n",
            "            if f and f(): return True\n",
            "        return False\n",
            "\n",
            "class AvgStats():\n",
            "    def __init__(self, metrics, in_train): self.metrics,self.in_train = listify(metrics),in_train\n",
            "\n",
            "    def reset(self):\n",
            "        self.tot_loss,self.count = 0.,0\n",
            "        self.tot_mets = [0.] * len(self.metrics)\n",
            "\n",
            "    @property\n",
            "    def all_stats(self): return [self.tot_loss.item()] + self.tot_mets\n",
            "    @property\n",
            "    def avg_stats(self): return [o/self.count for o in self.all_stats]\n",
            "\n",
            "    def __repr__(self):\n",
            "        if not self.count: return \"\"\n",
            "        return f\"{'train' if self.in_train else 'valid'}: {self.avg_stats}\"\n",
            "\n",
            "    def accumulate(self, run):\n",
            "        bn = run.xb.shape[0]\n",
            "        self.tot_loss += run.loss * bn\n",
            "        self.count += bn\n",
            "        for i,m in enumerate(self.metrics):\n",
            "            self.tot_mets[i] += m(run.pred, run.yb) * bn\n",
            "\n",
            "class AvgStatsCallback(Callback):\n",
            "    def __init__(self, metrics):\n",
            "        self.train_stats,self.valid_stats = AvgStats(metrics,True),AvgStats(metrics,False)\n",
            "\n",
            "    def begin_epoch(self):\n",
            "        self.train_stats.reset()\n",
            "        self.valid_stats.reset()\n",
            "\n",
            "    def after_loss(self):\n",
            "        stats = self.train_stats if self.in_train else self.valid_stats\n",
            "        with torch.no_grad(): stats.accumulate(self.run)\n",
            "\n",
            "    def after_epoch(self):\n",
            "        print(self.train_stats)\n",
            "        print(self.valid_stats)\n",
            "\n",
            "from functools import partial"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mJ0HwTM64mbt"
      },
      "source": [
        "def accuracy(out, yb): return (torch.argmax(out, dim=1)==yb).float().mean()\n",
        "\n",
        "class Dataset():\n",
        "    def __init__(self, x, y): self.x,self.y = x,y\n",
        "    def __len__(self): return len(self.x)\n",
        "    def __getitem__(self, i): return self.x[i],self.y[i]\n",
        "\n",
        "from torch.utils.data import DataLoader, SequentialSampler, RandomSampler\n",
        "\n",
        "def get_dls(train_ds, valid_ds, bs, **kwargs):\n",
        "    return (DataLoader(train_ds, batch_size=bs, shuffle=True, **kwargs),\n",
        "            DataLoader(valid_ds, batch_size=bs*2, **kwargs))\n",
        "    \n",
        "class DataBunch():\n",
        "    def __init__(self, train_dl, valid_dl, c=None):\n",
        "        self.train_dl,self.valid_dl,self.c = train_dl,valid_dl,c\n",
        "\n",
        "    @property\n",
        "    def train_ds(self): return self.train_dl.dataset\n",
        "\n",
        "    @property\n",
        "    def valid_ds(self): return self.valid_dl.dataset\n",
        "\n",
        "def get_model(data, lr=0.5, nh=50):\n",
        "    m = data.train_ds.x.shape[1]\n",
        "    model = nn.Sequential(nn.Linear(m,nh), nn.ReLU(), nn.Linear(nh,data.c))\n",
        "    return model, optim.SGD(model.parameters(), lr=lr)\n",
        "\n",
        "class Learner():\n",
        "    def __init__(self, model, opt, loss_func, data):\n",
        "        self.model,self.opt,self.loss_func,self.data = model,opt,loss_func,data\n",
        "\n",
        "import re\n",
        "\n",
        "_camel_re1 = re.compile('(.)([A-Z][a-z]+)')\n",
        "_camel_re2 = re.compile('([a-z0-9])([A-Z])')\n",
        "def camel2snake(name):\n",
        "    s1 = re.sub(_camel_re1, r'\\1_\\2', name)\n",
        "    return re.sub(_camel_re2, r'\\1_\\2', s1).lower()\n",
        "\n",
        "class Callback():\n",
        "    _order=0\n",
        "    def set_runner(self, run): self.run=run\n",
        "    def __getattr__(self, k): return getattr(self.run, k)\n",
        "    @property\n",
        "    def name(self):\n",
        "        name = re.sub(r'Callback$', '', self.__class__.__name__)\n",
        "        return camel2snake(name or 'callback')\n",
        "\n",
        "class TrainEvalCallback(Callback):\n",
        "    def begin_fit(self):\n",
        "        self.run.n_epochs=0.\n",
        "        self.run.n_iter=0\n",
        "\n",
        "    def after_batch(self):\n",
        "        if not self.in_train: return\n",
        "        self.run.n_epochs += 1./self.iters\n",
        "        self.run.n_iter   += 1\n",
        "\n",
        "    def begin_epoch(self):\n",
        "        self.run.n_epochs=self.epoch\n",
        "        self.model.train()\n",
        "        self.run.in_train=True\n",
        "\n",
        "    def begin_validate(self):\n",
        "        self.model.eval()\n",
        "        self.run.in_train=False\n",
        "\n",
        "from typing import *\n",
        "\n",
        "def listify(o):\n",
        "    if o is None: return []\n",
        "    if isinstance(o, list): return o\n",
        "    if isinstance(o, str): return [o]\n",
        "    if isinstance(o, Iterable): return list(o)\n",
        "    return [o]\n",
        "\n",
        "class Runner():\n",
        "    def __init__(self, cbs=None, cb_funcs=None):\n",
        "        cbs = listify(cbs)\n",
        "        for cbf in listify(cb_funcs):\n",
        "            cb = cbf()\n",
        "            setattr(self, cb.name, cb)\n",
        "            cbs.append(cb)\n",
        "        self.stop,self.cbs = False,[TrainEvalCallback()]+cbs\n",
        "\n",
        "    @property\n",
        "    def opt(self):       return self.learn.opt\n",
        "    @property\n",
        "    def model(self):     return self.learn.model\n",
        "    @property\n",
        "    def loss_func(self): return self.learn.loss_func\n",
        "    @property\n",
        "    def data(self):      return self.learn.data\n",
        "\n",
        "    def one_batch(self, xb, yb):\n",
        "        self.xb,self.yb = xb,yb\n",
        "        if self('begin_batch'): return\n",
        "        self.pred = self.model(self.xb)\n",
        "        if self('after_pred'): return\n",
        "        self.loss = self.loss_func(self.pred, self.yb)\n",
        "        if self('after_loss') or not self.in_train: return\n",
        "        self.loss.backward()\n",
        "        if self('after_backward'): return\n",
        "        self.opt.step()\n",
        "        if self('after_step'): return\n",
        "        self.opt.zero_grad()\n",
        "\n",
        "    def all_batches(self, dl):\n",
        "        self.iters = len(dl)\n",
        "        for xb,yb in dl:\n",
        "            if self.stop: break\n",
        "            self.one_batch(xb, yb)\n",
        "            self('after_batch')\n",
        "        self.stop=False\n",
        "\n",
        "    def fit(self, epochs, learn):\n",
        "        self.epochs,self.learn,self.loss = epochs,learn,tensor(0.)\n",
        "\n",
        "        try:\n",
        "            for cb in self.cbs: cb.set_runner(self)\n",
        "            if self('begin_fit'): return\n",
        "            for epoch in range(epochs):\n",
        "                self.epoch = epoch\n",
        "                if not self('begin_epoch'): self.all_batches(self.data.train_dl)\n",
        "\n",
        "                with torch.no_grad():\n",
        "                    if not self('begin_validate'): self.all_batches(self.data.valid_dl)\n",
        "                if self('after_epoch'): break\n",
        "\n",
        "        finally:\n",
        "            self('after_fit')\n",
        "            self.learn = None\n",
        "\n",
        "    def __call__(self, cb_name):\n",
        "        for cb in sorted(self.cbs, key=lambda x: x._order):\n",
        "            f = getattr(cb, cb_name, None)\n",
        "            if f and f(): return True\n",
        "        return False\n",
        "\n",
        "class AvgStats():\n",
        "    def __init__(self, metrics, in_train): self.metrics,self.in_train = listify(metrics),in_train\n",
        "\n",
        "    def reset(self):\n",
        "        self.tot_loss,self.count = 0.,0\n",
        "        self.tot_mets = [0.] * len(self.metrics)\n",
        "\n",
        "    @property\n",
        "    def all_stats(self): return [self.tot_loss.item()] + self.tot_mets\n",
        "    @property\n",
        "    def avg_stats(self): return [o/self.count for o in self.all_stats]\n",
        "\n",
        "    def __repr__(self):\n",
        "        if not self.count: return \"\"\n",
        "        return f\"{'train' if self.in_train else 'valid'}: {self.avg_stats}\"\n",
        "\n",
        "    def accumulate(self, run):\n",
        "        bn = run.xb.shape[0]\n",
        "        self.tot_loss += run.loss * bn\n",
        "        self.count += bn\n",
        "        for i,m in enumerate(self.metrics):\n",
        "            self.tot_mets[i] += m(run.pred, run.yb) * bn\n",
        "\n",
        "class AvgStatsCallback(Callback):\n",
        "    def __init__(self, metrics):\n",
        "        self.train_stats,self.valid_stats = AvgStats(metrics,True),AvgStats(metrics,False)\n",
        "\n",
        "    def begin_epoch(self):\n",
        "        self.train_stats.reset()\n",
        "        self.valid_stats.reset()\n",
        "\n",
        "    def after_loss(self):\n",
        "        stats = self.train_stats if self.in_train else self.valid_stats\n",
        "        with torch.no_grad(): stats.accumulate(self.run)\n",
        "\n",
        "    def after_epoch(self):\n",
        "        print(self.train_stats)\n",
        "        print(self.valid_stats)\n",
        "\n",
        "from functools import partial\n",
        "from torch.functional import F\n",
        "from torch import nn, optim, tensor"
      ],
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VQw7BLyQajp1"
      },
      "source": [
        "#A0\n",
        "x_train, y_train, x_valid, y_valid, x_test, y_test = get_data(data_path)\n",
        "train_ds, valid_ds = Dataset(x_train, y_train), Dataset(x_valid, y_valid)\n",
        "nh, bs = 50, 512\n",
        "c = y_train.max().item()+1\n",
        "loss_func = F.cross_entropy\n",
        "data = DataBunch(*get_dls(train_ds, valid_ds, bs), c=c)"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jcMU-QaccO1p"
      },
      "source": [
        "def create_learner(model_func, loss_func, data):\n",
        "    return Learner(*model_func(data), loss_func, data)"
      ],
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h6mwefXOcwnO"
      },
      "source": [
        "learn = create_learner(get_model, loss_func, data)"
      ],
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OmQ76VkhdXaJ"
      },
      "source": [
        "run = Runner(\n",
        "     [\n",
        "      AvgStatsCallback([accuracy])\n",
        "     ]\n",
        "    )"
      ],
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AeiigvJFeOAC",
        "outputId": "75428474-e959-43f5-bd29-75b74996a284"
      },
      "source": [
        "run.fit(epochs = 3, learn = learn)"
      ],
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train: [0.668972578125, tensor(0.8036)]\n",
            "valid: [0.337932373046875, tensor(0.9007)]\n",
            "train: [0.30240625, tensor(0.9132)]\n",
            "valid: [0.23879384765625, tensor(0.9299)]\n",
            "train: [0.23860115234375, tensor(0.9312)]\n",
            "valid: [0.2073470458984375, tensor(0.9413)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GpM14Zqk43WE"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZC2m2cPXeq-M",
        "outputId": "d14ee537-f3c9-49d8-a83c-55cfb9db960e"
      },
      "source": [
        "learn.opt.param_groups.__len__()"
      ],
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1"
            ]
          },
          "metadata": {},
          "execution_count": 58
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mMj8IzI-fMej",
        "outputId": "69409f75-a736-4460-804d-3f9c8246ab31"
      },
      "source": [
        "get_model"
      ],
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<function __main__.get_model>"
            ]
          },
          "metadata": {},
          "execution_count": 63
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v-PzHLFBeaFe",
        "outputId": "41783312-fdb3-410a-e2d8-1239d5352b5c"
      },
      "source": [
        "for param in learn.opt.param_groups:\n",
        "    print(param['lr'])"
      ],
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "htQkR_NmgEvs",
        "outputId": "08b73ee2-b0aa-4767-aec5-51ae6ecee530"
      },
      "source": [
        "import inspect\n",
        "for k,v in inspect.signature(get_model).parameters.items():\n",
        "    print(k, v)"
      ],
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "data data\n",
            "lr lr=0.5\n",
            "nh nh=50\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HkUR6BN9ZA7L"
      },
      "source": [
        "class Recorder(Callback):\n",
        "    def begin_fit(self): self.lrs, self.losses = [], []\n",
        "\n",
        "    def after_batch(self):\n",
        "        \"\"\"when it's training phase, record learning rate & loss\"\"\"\n",
        "        if not self.in_train: return\n",
        "        self.lrs.append(self.opt.params_group[-1]['lr'])"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}