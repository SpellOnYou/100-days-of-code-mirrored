{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Machine translation.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SpellOnYou/100_Days_of_ML_Code/blob/master/document/Machine_translation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "HBc5AyebHcXF"
      },
      "cell_type": "markdown",
      "source": [
        "### 1. Word-by-word translation\n",
        "  - `내맘 주어도 내 맘 갖고 싶진 않았지`\n",
        "  - `My mind give but my mom have want do not`\n",
        "\n",
        "### 2. Language-specific rule\n",
        "\n",
        "Statistical Machine Translation was a Huge Milestone\n",
        "Statistical machine translation systems perform much better than rule-based systems if you give them enough training data. Franz Josef Och improved on these ideas and used them to build Google Translate in the early 2000s. Machine Translation was finally available to the world.\n",
        "\n",
        "In the early days, it was surprising to everyone that the “dumb” approach to translating based on probability worked better than rule-based systems designed by linguists. This led to a (somewhat mean) saying among researchers in the 80s:\n",
        "\n",
        "“Every time I fire a linguist, my accuracy goes up.”\n",
        "— Frederick Jelinek\n"
      ]
    },
    {
      "metadata": {
        "id": "D2VnWQ0FHfDj",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "\n",
        "### 3.  Statistical Machine Translation Systems\n",
        "\n",
        "- Much better than rule-based systems if you have enough training data.\n",
        "- [Franz Josef Och](https://en.wikipedia.org/wiki/Franz_Josef_Och) used SMTS and used it to develope Google Translate in the early 2000s.\n",
        "- `Machine Translation was finally available to the world`\n",
        "\n",
        "**Threshold** \n",
        "- New pair of language pairs (ex) Telegu-Uganda) does not have parelell corpus/data. \n",
        "- So they need intermediate step. A language that has parallel corpus for almost all languages in the world :: ENGLISH!!!!!!!!!!!!!!!!!!!!!!\n",
        "\n",
        "Language A - english - language B..\n",
        "\n",
        "![from language A - english - language B](/images/SMT.png)\n",
        "\n",
        "### 4. Making Computers Translate Better Using Statistics\n",
        "\n",
        "### Paper : \n",
        "(1)  Sequence to Sequence Learning with Neural Network, 2014.\n",
        "\n",
        "(2) Learning Phrase Representations using RNN Encoder-Decoder for Statistical Machine Translation, 2014.\n",
        "\n",
        "\n",
        "<h4>1) Used RNN</h4> : the previous calculations change the results of future calculations. So it can learn the pattern of the sequence.\n",
        "\n",
        "<h4>2) Encodings</h4> : It let's us represent something very complicated (a picture of a face) with something simple (128 numbers).\n",
        "\n",
        "![original sentence](https://cdn-images-1.medium.com/max/1800/1*B24hDD3nGjfI4y3eNLNOgw.png)\n",
        "\n",
        "What if we could train the second RNN to decode the sentence into Spanish instead of English? (We can use our parallel corpora)\n",
        "\n",
        "![translation](https://cdn-images-1.medium.com/max/1800/1*fGzLsEwnEwFo2Wo9kzIvBw.png)\n",
        "\n",
        "#### Yes, this is neural machine translation and we call this model"
      ]
    },
    {
      "metadata": {
        "id": "7kVkdEU5Km87",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Sequent to sequence model\n",
        ": (SNMT, Statistical Neural Machine Translation)\n",
        "\n",
        "- Why this model ? : RNNs have problems when input and output lengths are different.\n",
        "\n",
        "- Domain : seq2seq - translation, image captioning, interpreting dialects of python, **text summarisation**\n",
        "\n",
        "\n",
        "### Tokens go in, tokens go out. Can’t explain that!\n",
        "![seq2seq](https://www.researchgate.net/profile/Martino_Mensio/publication/324877915/figure/fig8/AS:621610528690180@1525214907191/The-usage-of-RNN-for-a-simple-Encoder-Decoder-approach_W640.jpg)\n",
        "\n",
        "### Model explain : \n",
        "\n",
        "1. The RNN spits out a hidden state **'c'** which represents the vectorised contents of the sentence.\n",
        "\n",
        "2. We can then feed **'c'** to the decoder\n",
        "\n",
        "3. And decoder will generate the translated sentence, word by word. Decoder keeps generating words until a special end of sentence token is produced.\n",
        "\n",
        "sound like this model make sense?\n",
        "\n",
        "\n",
        "## NO"
      ]
    },
    {
      "metadata": {
        "id": "wYXdbI4AKx79",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## This architecture has memory problem"
      ]
    },
    {
      "metadata": {
        "id": "PngGDvSrKyrg",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "'c' means encoded sentence/state\n",
        "\n",
        "The 'c' is usually only **a few hundred floating point numbers** long.\n",
        "\n",
        "The more (length....ㅠㅠ) we try to force our sentence into this fixed dimensionality vector the more lossy our neural net is forced to be.\n",
        "\n",
        "We could increase the hidden size of the LSTM after all they're supposed to remember long term dependencies  but what happens is as we increase the hidden size 'h' of the LSTM the training time increases exponentially.\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "C9FeyluPKy4f",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## To solve the memory problem, 'Attention' into the mix!!!"
      ]
    },
    {
      "metadata": {
        "id": "CjXe_ibPKzDv",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "![image3](http://www.wildml.com/wp-content/uploads/2015/09/Screen-Shot-2015-09-17-at-10.39.06-AM.png)\n",
        "\n",
        "\n",
        "\n",
        "Attention and memory in deep learning and nlp](http://www.wildml.com/2016/01/attention-and-memory-in-deep-learning-and-nlp/)\n",
        "\n",
        "#### 1. Human Visual Attention\n",
        "\n",
        "Image : high resolution, low resolution, focal point over time\n",
        "\n",
        "#### 2. Neural Networks, vision\n",
        "\n",
        "Image recognition ex) [Learning to combine foveal glimpses with a third-order Bolzmann machine](http://papers.nips.cc/paper/4089-learning-to-combine-foveal-glimpses-with-a-third-order-boltzmann-machine), [Learning where to Attend with Deep Architectures for Image Tracking](http://arxiv.org/abs/1109.3737)\n",
        "\n",
        "#### 3. NLP의 역사에 Attention이 개입된 것은 얼마 되지 않았습니다. Attention in the history of the NLP has only recently begun.\n",
        " \n",
        "ex) Neural machine translation\n",
        "\n",
        "\n",
        "\n",
        "## 그렇다면, NLP에서 Attention이란 무엇인가?, If so, what is Attention in the NLP?\n",
        "\n",
        "If I was translating a long sentence, i'd probably glance back at the source sentence a couple times.\n",
        "\n",
        "To make sure i was capturing all the details\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "Km3an1F3L0dj",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Multi-step attention from Convolutional Sequence to Sequence"
      ]
    },
    {
      "metadata": {
        "id": "wcKWTkR6Ltne",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "![Mulgi-step attention from ConvS2S](https://mchromiak.github.io/articles/2017/Sep/12/Transformer-Attention-is-all-you-need/img/MultiStepAttention.gif)"
      ]
    },
    {
      "metadata": {
        "id": "KLikUlecKzNf",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        ""
      ]
    },
    {
      "metadata": {
        "id": "XQvFlPWIKzUS",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### 6 Dec 2017\n",
        "\n",
        "## The Transformer - model architecture"
      ]
    },
    {
      "metadata": {
        "id": "MdNIE8IdLVGS",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#### Transformer is claimed by the authors to be the first to rely entirely on self-attention to compute representation of input and output."
      ]
    },
    {
      "metadata": {
        "id": "mGL3BtbILVfy",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "![bert_image](https://mchromiak.github.io/articles/2017/Sep/12/Transformer-Attention-is-all-you-need/img/encoder.png)\n",
        "\n",
        "\n",
        "### This algorithm achieves\n",
        "\n",
        "- 1) Paralleization\n",
        "\n",
        "CNN/RNN의 공통점은 sequence를 처리 할 때, 단어들을 word-by-word(혹은 char-by-char)로 처리한다는 것이다.\n",
        "\n",
        "*(1, 2, ... , t-1, t, ... ) - i.e. time-series data*\n",
        "\n",
        "하지만 이렇게 되면, 당연히 step이 생기고 병렬 처리가 불가능해져서 메모리 문제 등을 야기시킨다.\n",
        "\n",
        "Transformer는 (후에 살펴볼) Attention과 기존에 word가 갖고 있는 position 정보를 통해서 이 문제를 해결했다.\n",
        "\n",
        "One common feature of CNN / RNN is that when processing a sequence, words are word-by-word (or char-by-char).\n",
        "\n",
        "However, if this happens, multiple steps will occur and parallel processing becomes impossible, causing memory problems.\n",
        "\n",
        "The Transformer solves this problem with the Attention (see below) and the existing position information in the word.\n",
        "\n",
        "- 2) Reduce sequential computation\n",
        "\n",
        "Constant O(1) number of operations to learn dependency between two symbols independently of their position distance in sequence. (문장 내에서 위치만 지정하면 되므로.)\n",
        "\n",
        "\n",
        "\n",
        "### Multi-head attention\n",
        "\n",
        "![](https://mchromiak.github.io/articles/2017/Sep/12/Transformer-Attention-is-all-you-need/img/MultiHead.png)\n",
        "\n",
        "#### Pseudocode or flowchart description of the algorithm ![](https://mchromiak.github.io/articles/2017/Sep/12/Transformer-Attention-is-all-you-need/img/transform20fps.gif)\n",
        "\n",
        "Adopted from [Google Blog](https://research.googleblog.com/2017/08/transformer-novel-neural-network.html)\n",
        "\n",
        "## Q. input과 ontput 사이에 맞물리는 dependency를 아예 없앴습니다. self-attention 혹은 intra-attention. <i can't understand what this means>\n",
        "\n",
        "\n",
        "#### Co-reference resoution. The *it* in both cases relates to diffrent token.\n",
        "\n",
        "\n",
        "![](https://mchromiak.github.io/articles/2017/Sep/12/Transformer-Attention-is-all-you-need/img/CoreferenceResolution.png)\n",
        "\n",
        "\n",
        "Adapted from [Google Blog](https://research.googleblog.com/2017/08/transformer-novel-neural-network.html)\n"
      ]
    },
    {
      "metadata": {
        "id": "53F8KEUJLWQO",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "\n",
        "## ELMo\n",
        "\n",
        "## BERT\n"
      ]
    },
    {
      "metadata": {
        "id": "cLDcbqdBLWZX",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        ""
      ]
    },
    {
      "metadata": {
        "id": "_00nPyUXLWiO",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        ""
      ]
    },
    {
      "metadata": {
        "id": "w3-CrIPdLWs3",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        ""
      ]
    },
    {
      "metadata": {
        "id": "wMDxryY0LW6o",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        ""
      ]
    },
    {
      "metadata": {
        "id": "_w8eFtiQLXCJ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        ""
      ]
    },
    {
      "metadata": {
        "id": "Nd9sdyf2LXJC",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        ""
      ]
    },
    {
      "metadata": {
        "id": "7bSES2B5LXPc",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        ""
      ]
    },
    {
      "metadata": {
        "id": "SZ5MWjpNLXU6",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        ""
      ]
    },
    {
      "metadata": {
        "id": "4YY1EFFALXaE",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        ""
      ]
    },
    {
      "metadata": {
        "id": "swWV-vwmLXfC",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        ""
      ]
    },
    {
      "metadata": {
        "id": "ktxrZALHLXkO",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        ""
      ]
    },
    {
      "metadata": {
        "id": "wn6jDmOKLXpj",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        ""
      ]
    }
  ]
}